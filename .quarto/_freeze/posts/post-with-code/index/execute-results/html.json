{
  "hash": "e2e1b8f785a133a6b7b722ccd46b7fbf",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection on Spacecraft Telemetry\"\nauthor: \"Eric Jackson\"\ndate: \"2023-09-08\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nSpacecraft will generate a massive amount of data the longer they are on orbit, from telemetry data containing voltages, temperatures, etc to raw data from the various types of payloads on orbit\n\n-   Spacecraft have onboard anomaly responses for most known failure cases to safe the vehicle\n\n-   Normally low/high, red/yellow limits set for certain monitors with corresponding response (either automatic or visual alarm)\n\n-   Some anomalies can be hard to predict, multiple components can react slightly out of family to create larger issue\n\n-   Benefits of utilizing machine learning for spacecraft:\n\n-   Prevents loss of mission over potentially high priority targets\n\n-   Automatic response would limit both downtime and human interaction\n\n-   Higher award/incentive fees for lower mission outage percentage\n\n-   Limits time spent by operators and factory investigating and implementing a fix\n\n-   Depending on program and customer, recovery can take anywhere from a few hours to multiple days\n\n-   Predict future anomalous conditions and potentially react before an issue were to occur\n\n-   Some programs have multiple vehicles on orbit meaning there is a plethora of historical training data available\n\n-   Goal: Utilize ARIMA & OCSVM to create a hybrid anomaly detection method and compare results with other common algorithms/methods\n\n![Example of anomaly in telemetry data](anomaly.png){width=\"636\"}\n\n# Data Setup & Preprocessing\n\nUnfold below code to see setup. Basics are generating 28 features\n\nsetting up training data set and validation data set\n\nA visualization of the separation between training and test data can be seen in in @fig-arima2, with the first 75% used for training and the remaining 25% used for test data.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n       \n       \n#exogenous_features=['High_mean_lag3','week']\n\n```\n:::\n\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Training data is first 75% of wheel temperature data, with the remaining 25% used as the test data for verification](index_files/figure-html/fig-arima2-output-1.png){#fig-arima2 width=808 height=485}\n:::\n:::\n\n\n# ARIMA Model\n\nThe first step in accurately identifying anomalies with telemetry data is to forecast future data. The ARIMA model is a statistical based model that is specifically made and used for forecasting time series data. Many machine learning algorithms struggle with time series data and are prone to overfitting, so ARIMA is a great option for analyzing spacecraft telemetry data.\n\nARIMA works best with data that is stationary, meaning that most input data will need to be differenced before running the model. This was accomplished in the preprocessing stage above, but the auto_arima function will still perform an Augmented Dickey-Fuller (ADF) test to check for stationary data by looking if a unit root is present in the data.\n\nThere are 3 hyperparameters that are used as input for the auto_arima function (with 3 more parameters if seasonal modeling is desired). These parameters are used to optimize the below equation. Auto_arima will iterate through multiple variations of each parameter to find the best combination with the lowest prediction error (Akaike Information Criteria or AIC).\n\n[![Generalized ARIMA model function](https://wikimedia.org/api/rest_v1/media/math/render/svg/b6ebbe31d07e994b209c391e3d6f8f5d88e267c3){alt=\"{\\\\displaystyle \\\\left(1-\\\\sum _{i\"}](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)\n\nWhere:\n\n-   p = order of auto-regressive model\n\n-   d = order of first-differencing\n\n-   q = order of moving-average model\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = auto_arima(\n\tdf_train[\"High\"],\n\texogenous=df_train[exogenous_features],\n\ttrace=True,\n\terror_action=\"ignore\",\n\tsuppress_warnings=True,\n    seasonal=True,\n    m=1)\nmodel.fit(df_train.High, exogenous=df_train[exogenous_features])\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid.insert(len(df_valid.columns),\"Forecast_ARIMAX\",forecast,True)\n\nprint(\"\\nRMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.High, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.High, df_valid.Forecast_ARIMAX))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPerforming stepwise search to minimize aic\n ARIMA(2,0,2)(0,0,0)[0] intercept   : AIC=2960.826, Time=15.54 sec\n ARIMA(0,0,0)(0,0,0)[0] intercept   : AIC=5711.667, Time=1.30 sec\n ARIMA(1,0,0)(0,0,0)[0] intercept   : AIC=3359.247, Time=10.82 sec\n ARIMA(0,0,1)(0,0,0)[0] intercept   : AIC=3726.632, Time=13.09 sec\n ARIMA(0,0,0)(0,0,0)[0]             : AIC=5709.707, Time=14.29 sec\n ARIMA(1,0,2)(0,0,0)[0] intercept   : AIC=2904.191, Time=15.88 sec\n ARIMA(0,0,2)(0,0,0)[0] intercept   : AIC=2912.739, Time=14.28 sec\n ARIMA(1,0,1)(0,0,0)[0] intercept   : AIC=3067.819, Time=13.85 sec\n ARIMA(1,0,3)(0,0,0)[0] intercept   : AIC=2930.144, Time=17.32 sec\n ARIMA(0,0,3)(0,0,0)[0] intercept   : AIC=2936.070, Time=16.18 sec\n ARIMA(2,0,1)(0,0,0)[0] intercept   : AIC=2957.813, Time=14.07 sec\n ARIMA(2,0,3)(0,0,0)[0] intercept   : AIC=2898.815, Time=16.53 sec\n ARIMA(3,0,3)(0,0,0)[0] intercept   : AIC=2805.639, Time=17.64 sec\n ARIMA(3,0,2)(0,0,0)[0] intercept   : AIC=2962.492, Time=17.25 sec\n ARIMA(4,0,3)(0,0,0)[0] intercept   : AIC=2810.038, Time=18.72 sec\n ARIMA(3,0,4)(0,0,0)[0] intercept   : AIC=2785.353, Time=22.07 sec\n ARIMA(2,0,4)(0,0,0)[0] intercept   : AIC=2897.289, Time=20.04 sec\n ARIMA(4,0,4)(0,0,0)[0] intercept   : AIC=2807.535, Time=20.42 sec\n ARIMA(3,0,5)(0,0,0)[0] intercept   : AIC=2789.723, Time=21.74 sec\n ARIMA(2,0,5)(0,0,0)[0] intercept   : AIC=2846.091, Time=21.45 sec\n ARIMA(4,0,5)(0,0,0)[0] intercept   : AIC=2791.733, Time=22.70 sec\n ARIMA(3,0,4)(0,0,0)[0]             : AIC=2809.734, Time=18.33 sec\n\nBest model:  ARIMA(3,0,4)(0,0,0)[0] intercept\nTotal fit time: 363.557 seconds\n\nRMSE of Auto ARIMAX: 0.6141699692631081\n\nMAE of Auto ARIMAX: 0.2635940364491945\n```\n:::\n:::\n\n\nIt can be seen that the auto_arima function performed fairly quickly, finding an optimal model in less than 6 minutes with an RMSE of 0.614 and MAE of 0.263. A visual representation of the ARIMA forecast compared to the actual telemetry data can be seen in @fig-arima\n\n::: {.cell fig-width='30%' execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![Initial ARIMA forecast on reaction wheel temperature data](index_files/figure-html/fig-arima-output-1.png){#fig-arima width=790 height=444}\n:::\n:::\n\n\nIn order to determine the accuracy of the anomaly detection methods used below, a set of truth data points needed to be manually chosen. These points were identified to be points in time where an anomaly took place based on personal experience of operational spacecraft data. 115 out of 1137 total points were marked as anomalous. @fig-truth identifies the anomalies, marked in red.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(10,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Set of anomalous data points to be used as truth](index_files/figure-html/fig-truth-output-1.png){#fig-truth width=808 height=505}\n:::\n:::\n\n\n# OCSVM\n\nOne class support vector machine algorithm\n\n[![OCSVM example visualization ](https://www.researchgate.net/publication/362912442/figure/fig4/AS:11431281080912015@1661430301368/Schematic-of-the-OCSVM_W640.jpg){width=\"408\"}](https://www.researchgate.net/publication/362912442_Anomaly_Detection_in_Satellite_Telemetry_Data_Using_a_Sparse_Feature-Based_Method/figures?lo=1)\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n############# OCSVM ##################\n\nfig, ax = plt.subplots(figsize=(10,6))\ndata2=df_valid[\"Forecast_ARIMAX\"]\nmodel =svm.OneClassSVM(nu=0.05,kernel='poly')\nmodel.fit(data2.values.reshape(-1,1))\nanom=(pd.Series(model.predict(data2.values.reshape(-1,1))))\ndf2=pd.DataFrame()\ndf2['Time']=data2.index\ndf2['data']=data2.values\ndf2['anom']=anom\na=df2.loc[df2['anom']==-1,['Time','data']]\ndf2.set_index(\"Time\", drop=True, inplace=True)\nax.plot(df2.index, df2['data'], color='black', label = 'ARIMA')\nax.scatter(a['Time'].values,a['data'], color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Anomalies detected with OCSVM')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=808 height=505}\n:::\n:::\n\n\n# Isolation Forest\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n######### Isolation Forest ################\nimport sklearn\nfrom sklearn.ensemble import IsolationForest\nisofor=df_valid\noutliers_fraction = float(.01)\nscaler = sklearn.preprocessing.StandardScaler()\nnp_scaled = scaler.fit_transform(isofor['Forecast_ARIMAX'].values.reshape(-1, 1))\ndata = pd.DataFrame(np_scaled)\n# train isolation forest\nmodel =  IsolationForest(contamination=outliers_fraction)\nmodel.fit(data)\n\n\n\nisofor['anomaly'] = model.predict(data)\n# visualization\nfig, ax = plt.subplots(figsize=(10,6))\na = isofor.loc[isofor['anomaly'] == -1, ['Forecast_ARIMAX']] #anomaly\nax.plot(isofor.index, isofor['Forecast_ARIMAX'], color='black', label = 'Normal')\nax.scatter(a.index,a['Forecast_ARIMAX'], color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Anomalies detected with Isolation Forest')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=808 height=505}\n:::\n:::\n\n\n# Final Results\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n######################### calculate statistics ###########################\nfrom sklearn.metrics import f1_score,recall_score,precision_score\nfrom sklearn.metrics import mean_squared_error\nfrom tabulate import tabulate\nfrom collections import OrderedDict\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom numpy import array\n\ndef perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf_truth=pd.read_csv('./truth.csv')\nanom=anom.map(lambda val:1 if val==-1 else 0)\n#calculate F1 score\nf1=f1_score(df_truth['Anom'].values, anom.values)\nrec=recall_score(df_truth['Anom'].values, anom.values)\nprec=precision_score(df_truth['Anom'].values, anom.values)\nTP, FP, TN, FN=perf_measure(df_truth['Anom'].values, anom.values)\nfpr=FP/(TN+FP)\nfinal=OrderedDict()\nfinal['OCSVM']=[f1,rec,prec,fpr]\n\na2=isofor['anomaly']\na2=a2.map(lambda val:1 if val==-1 else 0)\nf1=f1_score(df_truth['Anom'].values, a2.values)\nrec=recall_score(df_truth['Anom'].values, a2.values)\nprec=precision_score(df_truth['Anom'].values, a2.values)\nTP, FP, TN, FN=perf_measure(df_truth['Anom'].values, a2.values)\nfpr=FP/(TN+FP)\nfinal['IsoFor']=[f1,rec,prec,fpr]\ndf=pd.DataFrame(final)\ndf.index=['F1','Recall','Precision','FPR']\nprint(tabulate(df, headers='keys', tablefmt='psql'))\n\ncm=confusion_matrix(df_truth['Anom'].values,anom.values)\ncmd=ConfusionMatrixDisplay(cm)\ncmd.plot()\nplt.show()\n\ncm=confusion_matrix(df_truth['Anom'].values,a2.values)\ncmd=ConfusionMatrixDisplay(cm)\ncmd.plot()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----------+-------------+----------+\n|           |       OCSVM |   IsoFor |\n|-----------+-------------+----------|\n| F1        | 0.718232    | 0.217054 |\n| Recall    | 0.565217    | 0.121739 |\n| Precision | 0.984848    | 1        |\n| FPR       | 0.000818331 | 0        |\n+-----------+-------------+----------+\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=513 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-3.png){width=513 height=429}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}