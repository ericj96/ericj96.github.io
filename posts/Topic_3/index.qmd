---
title: "(Old blog post) Data Preprocessing"
author: "Eric Jackson"
date: "2023-09-01"
categories: [code, analysis,data,preprocessing]
image: "image.jpg"
toc: true
toc-depth: 2
---

# Background

For any machine learning model, there is a set of data that will be input into it. Generally the data will be broken into multiple sets, consisting of training data and test data. The training data will be the portion or set of data that is used to train the model, and the test data is what the trained model is run on to produce results.

Before one can use datasets, it's generally necessary to do some form of preprocessing to the raw data to ensure that the model can run efficiently and accurately. This can be as simple as removing NaN or Null values and as complex as performing statistical analysis to remove outliers and normalizing the data.

# Data Preprocessing

## Dropping Null/NaN values

One of the first steps in importing datasets is to drop any NaN or null values. These values will generally cause issues when running and machine learning model and are best to remove immediately. Luckily, there are several built in functions to perform this.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# import dataset 
df=pd.read_csv('./WheelTemperature.csv')
df.Date = pd.to_datetime(df.Date, format="%m/%d/%Y %H:%M")
df.isna().sum()
```

It can be seen above that there are 17 NaN values in this dataset, and they can easily be removed with one call to dropna(). As seen below there are now 0 rows with NaN values.

```{python}
df=df.dropna()
df.isna().sum()
```

## Missing Data and Resampling

Another common issue with datasets is missing data or data that needs to be sampled up or down. In this example, there are 769,746 rows of data containing temperature data every 10 minutes for 14 years. Because temperature in this scenario doesn't change that rapidly, and to improve the simplicity of this example, the data can be downsampled to a much lower rate, every 12 hours for example. This downsampling will be a mean of all values over that time period and reduces the dataset to just 10,691 rows.

```{python}
print('Size of dataset before resampling: %d rows' % len(df))
df=df.set_index('Date')
df=df.resample('12H').mean()
print('Size of dataset after resampling: %d rows' % len(df))
```

As far as missing data, it is seen below in @fig-missingdata that there is a large chunk of missing data in 2018 from June to July.

```{python}
#| label: fig-missingdata
#| fig-cap: "Example of missing data in raw data"
#| 
# dropping data to pretend there is missing data
df=df.drop(df.loc['6/1/2015':'8/1/2015'].index)
plt.figure(figsize=(9,6))
plt.plot(df.loc['2015'],marker='o')
plt.show()
```

By interpolating with the time option (since this is a time series dataset), it is seen below in @fig-missingdatafixed that a linear fit is used to generate data points for the missing set, thus reducing any issues with the model ingesting the missing data.

```{python}
#| label: fig-missingdatafixed
#| fig-cap: "Example of interpolated data to fill missing raw data"

df=df.resample('12H').mean().interpolate('time')
plt.figure(figsize=(9,6))
plt.plot(df.loc['2015'],marker='o')
plt.plot(df.loc['6/1/2015':'8/1/2015'],marker='o',color='r')
plt.legend(['Raw data','Interpolated Data'])
plt.show()
```

## Normalizing and Statistical Analysis

-   removing values more than 2 std dev away

```{python}

from pmdarima import auto_arima
from sklearn.metrics import mean_absolute_error
df=df.resample('24H').mean().interpolate('time')
df_train=df[:int(0.8*(len(df)))]
df_valid=df[int(0.8*(len(df))):]
exogenous_features=['High']
model = auto_arima(
	df_train["High"],
	exogenous=df_train[exogenous_features],
	trace=True,
	error_action="ignore",
	suppress_warnings=True,
    seasonal=False,
    m=1)
model.fit(df_train.High, exogenous=df_train[exogenous_features])
forecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])
df_valid.insert(len(df_valid.columns),"Forecast_ARIMAX",forecast,True)


from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
print("\nRMSE of Auto ARIMAX:", np.sqrt(mean_squared_error(df_valid.High, df_valid.Forecast_ARIMAX)))
print("\nMAE of Auto ARIMAX:", mean_absolute_error(df_valid.High, df_valid.Forecast_ARIMAX))

#df_valid[["High", "Forecast_ARIMAX"]].plot(figsize=(9, 5))
#plt.legend(['Wheel Temperature (Truth)','Forecast (ARIMA)'])
#plt.show()


plt.figure()
plt.plot((df_valid.High-df_valid.Forecast_ARIMAX)/df_valid.High*100)
plt.show()

```

```{python}
lag_features=["High"]
window1 = 3
window2 = 7
window3 = 30

df_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)
df_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)
df_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)

df_mean_3d = df_rolled_3d.mean().shift(1).reset_index()
df_mean_7d = df_rolled_7d.mean().shift(1).reset_index()
df_mean_30d = df_rolled_30d.mean().shift(1).reset_index()

df_std_3d = df_rolled_3d.std().shift(1).reset_index()
df_std_7d = df_rolled_7d.std().shift(1).reset_index()
df_std_30d = df_rolled_30d.std().shift(1).reset_index()

df_mean_3d.set_index("Date", drop=True, inplace=True)
df_mean_7d.set_index("Date", drop=True, inplace=True)
df_mean_30d.set_index("Date", drop=True, inplace=True)
df_std_3d.set_index("Date", drop=True, inplace=True)
df_std_7d.set_index("Date", drop=True, inplace=True)
df_std_30d.set_index("Date", drop=True, inplace=True)

for feature in lag_features:
    
    df[f"{feature}_mean_lag{window1}"] = df_mean_3d[feature]
    df[f"{feature}_mean_lag{window2}"] = df_mean_7d[feature]
    df[f"{feature}_mean_lag{window3}"] = df_mean_30d[feature]
    
    df[f"{feature}_std_lag{window1}"] = df_std_3d[feature]
    df[f"{feature}_std_lag{window2}"] = df_std_7d[feature]
    df[f"{feature}_std_lag{window3}"] = df_std_30d[feature]



exogenous_features=['High_mean_lag3', 'High_mean_lag7',
       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30']
       

```

```{python}
df=df.dropna()       
df_train=df[:int(0.8*(len(df)))]
df_valid=df[int(0.8*(len(df))):]

model = auto_arima(
	df_train["High_mean_lag3"],
	
	trace=True,
	error_action="ignore",
	suppress_warnings=True,
    seasonal=True,
    m=1)
model.fit(df_train.High_mean_lag3)
forecast = model.predict(n_periods=len(df_valid))
df_valid.insert(len(df_valid.columns),"Forecast_ARIMAX",forecast,True)
```

```{python}
plt.figure()
#plt.plot((df_valid.High-df_valid.Forecast_ARIMAX)/df_valid.High*100)
#df_valid[["High_mean_lag3", "Forecast_ARIMAX"]].plot(figsize=(9, 5))
df.High.plot()
plt.show()
```

## Normalization

```{python}
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(df_train)
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(df_train.High.values.reshape(-1,1))
#scaler = StandardScaler().fit(df_train.High.values.reshape(-1,1))
#rescaledX = scaler.transform(df_train.High.values.reshape(-1,1))

plt.figure()
plt.plot(rescaledX)
plt.show()

```

## One-Hot Encoding

```{python}
plt.figure()
i = 0
plt.boxplot(rescaledX, vert=False)
plt.set_ylabel('High')
plt.show()
plt.figure()
plt.plot(rescaledX)
plt.show()
q1, q3 = np.percentile(rescaledX, [25, 75])
iqr = q3 - q1

lower_bound = q1 - (1.5 * iqr)
upper_bound = q3 + (1.5 * iqr)
# Drop the outliers
clean_data = rescaledX[(rescaledX >= lower_bound) 
                & (rescaledX <= upper_bound)]
                
plt.figure()
plt.plot(clean_data)
plt.ylim(-0.05,1.05)
plt.show()


corr = df.corr()
import seaborn as sns
plt.figure(dpi=130)
sns.heatmap(df.corr(), annot=True, fmt= '.2f')
plt.show()

```
