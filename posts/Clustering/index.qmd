---
title: "Clustering Methods"
author: "Eric Jackson"
date: "2023-10-01"
categories: [clustering]
image: "image.jpg"
toc: true
toc-depth: 2
---

# Background

Clustering is a technique commonly used in statistics and machine learning to group similar groups of data together. This allows for things like data compression, optimization of processes, and identification of oddities in data to be accomplished much easier than normal. Each of the clusters identified will share common traits, decided by the specific algorithm. For the purposes of this blog post, a random set of blob data was generated and used with 5 clusters visually obvious.

@fig-og shows the original data and the clusters identified as truth. In order to keep the same results throughout iterations of making this blog post, a random seed (170) was selected so the same results can be reproduced, while still being randomized. The results from the below algorithms will reference and be compared against this truth data.

```{python}
#| code-fold: true
#| code-summary: Setup and imports
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs,make_circles
from sklearn import cluster 
from sklearn.cluster import KMeans,DBSCAN
from itertools import cycle, islice
from sklearn.metrics import confusion_matrix,accuracy_score
from sklearn.metrics.cluster import adjusted_rand_score
from pandas.plotting import table
```

```{python}
#| fig-cap: Original data with true clusters identified 
#| label: fig-og
noisy_circles = make_circles(n_samples=2500, factor=0.5, noise=0.05)
x=noisy_circles[0]
y=noisy_circles[1]
np.random.seed(170) 
blob_centers=np.random.uniform(0,5,[5,2])
blob_std = np.array([.3, .3, 0.4, .3 ,.1])
x,y=make_blobs(n_samples=2500, cluster_std=blob_std,centers=blob_centers,random_state=170)
# plotting original data
fig, ax = plt.subplots(figsize=(6,4))
plt.scatter(x[:,0],x[:,1],c=y)
plt.title('Original Data')
plt.show()
```

# Clustering Algorithms

For each of the four clustering methods used below, a brief description and the code of training the model will be shown, with the resulting data and plots being shown in @sec-conc.

## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)

As explored in the [blog post on anomaly/outlier detection](https://ericj96.github.io/posts/Anomaly_Outlier_Detection/), the DBSCAN algorithm is a widely used machine learning algorithm for clustering given sets of points by grouping together points that are close together and have multiple nearest neighbors. While creating the clusters of the data, it will naturally identify outliers and thus, is a great algorithm to accomplish both outlier detection and clustering.

**Note:** An epsilon value of 0.2 was used for the DBSCAN algorithm. This parameter "defines the maximum distance between two samples for one to be considered as in the neighborhood of the other". [\[1\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

```{python}
dbscan=DBSCAN(eps=0.2)
dbscan.fit(x)
colors = dbscan.labels_
y_pred_dbscan=colors
```

## Affinity Propagation

Unlike the two algorithms that will be investigated below, Affinity Propagation clustering does not require the number of clusters as an argument prior to running the algorithm. It will iterate through the data and find exemplar points that are representative of each cluster. Two arguments were used for this algorithm, a damping factor value of 0.9 and a preference value of -200. The damping value is the extent that the current value is maintained relative to incoming values to avoid oscillations, and the preference value allows for points preference in the availability matrix to be set and allows the model to more accurately predict the number of clusters. [\[2\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)

```{python}
affinity_propagation = cluster.AffinityPropagation(damping=.9,preference=-200)
affinity_propagation.fit(x)
y_pred_af=affinity_propagation.labels_
```

## Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)

The BIRCH clustering algorithm is often used as an alternative to the K means algorithm, as it is more memory efficient and works exceptionally well with larger datasets. It works by constructing a clustering feature tree with a set of nodes and subclusters for each node. The algorithm will merge together subclusters with the smallest radius, and then see if each subcluster has any child nodes. If so, it will continue the same process until it reaches a leaf of the original tree. [\[3\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)

As mentioned above, the BIRCH algorithm can take the number of clusters as an input, and since this number is known to be 5, n_clusters = 5 will be used as the only argument.

```{python}
birch = cluster.Birch(n_clusters=5)
birch.fit(x)
y_pred_birch=birch.labels_
```

## Spectral Clustering

The Spectral Clustering algorithm works by taking the eigenvalues of the similarity matrix, reducing the dimensions, and then performing clustering but now with fewer dimensions. This method is useful for data that is non-convex or irregularly shaped, as well as high dimensional datasets. [\[4\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)

As mentioned above, the Spectral Clustering algorithm can take the number of clusters as an input, and since this number is known to be 5, n_clusters = 5 will be used as the only argument.

```{python}
spectral = cluster.SpectralClustering(n_clusters=5)
spectral.fit(x)   
y_pred_spectral=spectral.labels_
```

# Conclusion {#sec-conc}

Since most clustering algorithms will apply arbitrary labels to the dataset passed through them, it isn't possible to do a simple accuracy scoring or confusion matrix comparison. Therefore, an Adjusted Rand Index (ARI) comparison will be performed between the original blob labels and the predicted/fitted labels from each algorithm. This will allow all four of the clustering algorithms explored above to be compared and ranked against each other. The ARI is a version of the Rand index, which measures the similarity between two data clusterings, but is corrected for chance. It will look at all pairs of samples and compute how many are in the same or different clusters between the modeled and true data. [\[5\]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)

As seen in @fig-sub, the DBSCAN algorithm was able to identify 4 of the 5 clusters correctly, but combined two of the clusters into one and considered points on the edge of each cluster as a separate cluster. This could be due to the DBSCAN algorithms ability to identify outlier points, as all of the purple cluster points appear to be outliers. The remaining three algorithms all performed similarly, correctly identifying all 5 clusters to a certain degree. @fig-tab shows the results in terms of ARI, showing that DBSCAN obviously had the lowest score and the other methods having very high but similar scores. Spectral Clustering edged out BIRCH and Affinity Propagation slightly, with an ARI score of 0.98.

```{python}
#| code-fold: true
#| code-summary: All 4 subplot code
#| fig-cap: All 4 algorithms and the clusters identified by each
#| label: fig-sub
fig, ax = plt.subplots(2,2,figsize=(9,8))
ax1=plt.subplot(221)
plt.scatter(x[:,0],x[:,1],c=y_pred_dbscan)
plt.title('DBSCAN')
ax1=plt.subplot(222)
plt.scatter(x[:,0],x[:,1],c=y_pred_af)
plt.title('Affinity Propagation')
ax1=plt.subplot(223)
plt.scatter(x[:,0],x[:,1],c=y_pred_birch)
plt.title('BIRCH')
ax1=plt.subplot(224)
plt.scatter(x[:,0],x[:,1],c=y_pred_spectral)
plt.title('Spectral Clustering')
plt.show()
```

```{python}
#| code-fold: true
#| code-summary: Table generation code
#| fig-cap: ARI for each of the 4 clustering algorithms
#| label: fig-tab
scores=[adjusted_rand_score(y, dbscan.labels_),adjusted_rand_score(y, affinity_propagation.labels_),adjusted_rand_score(y, birch.labels_) ,adjusted_rand_score(y, spectral.labels_)]
df=pd.DataFrame([scores,scores,scores,scores],columns=['DBSCAN','Affinity Propagation','BIRCH','Spectral Clustering'],index=['Adjusted Rand Score','R^21','R^22','R'])
fix, ax = plt.subplots(figsize=(8,1))
ax.axis('off')
table(ax,df.transpose()['Adjusted Rand Score'],loc='top',cellLoc='center',colWidths=list([.6, .6]))
plt.show()
```
