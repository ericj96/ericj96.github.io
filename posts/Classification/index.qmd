---
title: "Classification"
author: "Eric Jackson"
date: "2023-10-04"
categories: [classification]
image: "image.jpg"
toc: true
toc-depth: 2
---

# Background and Setup

Classification is similar to clustering ([as mentioned in this blog post](https://ericj96.github.io/posts/Clustering/)) in the sense that they group data together into separate categories. The difference comes from the fact that classification has a predefined set of labels that are attached to each data point, and in clustering the labels are missing and the algorithm will apply those labels/groupings. These two topics are commonly referenced as supervised and unsupervised learning (classification vs clustering).

Some of the downsides of classification are that there is a need to train the model before using it with test data, whereas clustering does not require such training and can group data points together immediately. But, classification can be used for much more intensive scenarios, such as handwriting recognition and spam filtering.

As mentioned before, classification algorithms or models require inputs with labels predefined. For this blog post, a native sklearn dataset containing measurements of a species of flower will be used. As seen below, there are width and length measurements along with the label/target class for the species of flower.

```{python}
#| code-fold: true
#| code-summary: Setup/imports

import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.inspection import DecisionBoundaryDisplay
```

```{python}
iris=load_iris()
df = pd.DataFrame(iris['data'],columns = iris['feature_names'])
target=pd.DataFrame(iris['target'])
target[target==0]=iris.target_names[0]
target[target==1]=iris.target_names[1]
target[target==2]=iris.target_names[2]
df['target']=target

# breaking up data into training and test datasets
x_train=df[['petal length (cm)', 'petal width (cm)']]
y_train=df['target']
x_test=df[['petal length (cm)', 'petal width (cm)']]
y_test=df['target']
```

```{python}
#| echo: false
df
```

Before the data is to be run through any classification algorithms, it is helpful to first plot the data and review what to expect. @fig-class shows the dataset visually with each color representing a different label or species of flower.

```{python}
#| code-fold: true
#| code-summary: Plotting code
#| fig-cap: Dataset values broken into their defined target labels
#| label: fig-class

fig, axs = plt.subplots(figsize =(7, 5))
plt.scatter(df['petal length (cm)'][df['target']==iris.target_names[0]],df['petal width (cm)'][df['target']==iris.target_names[0]])
plt.scatter(df['petal length (cm)'][df['target']==iris.target_names[1]],df['petal width (cm)'][df['target']==iris.target_names[1]])
plt.scatter(df['petal length (cm)'][df['target']==iris.target_names[2]],df['petal width (cm)'][df['target']==iris.target_names[2]])
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
plt.legend(iris.target_names)
plt.show()
```

# Classification Algorithms

## Gaussian Naive Bayes

The Gaussian Naive Bayes algorithm from the sklearn library ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)) utilizes Bayes' theorem assuming the features probability is distributed in a Gaussian or normal fashion, with the variance and mean of each data point calculated for each class. There are a multitude of Naive Bayes classifier algorithms (Bernoulli, Multinomial, etc), but the specific Gaussian probabilistic version used here is especially useful when the values are continuous and expected to follow a Gaussian distribution.

As seen from @fig-nb, the algorithm uses circular decision boundaries to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 6 points leading to a mean accuracy of 96%.

```{python}
nb = GaussianNB()
nb.fit(x_train, y_train)
print('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))
y_pred=nb.predict(x_train)
cm=confusion_matrix(y_train,y_pred,labels=iris.target_names)
```

```{python}
#| code-fold: true
#| code-summary: Plotting code
#| label: fig-nb
#| fig-cap: Gaussian Naive Bayes classification results (with decision boundaries) and confusion matrix 

# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07
fig, axs = plt.subplots(1,2,figsize =(8, 5))
plt.subplot(121)
x1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))
x2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))
xx, yy = np.meshgrid(x1grid, x2grid)
r1, r2 = xx.flatten(), yy.flatten()
r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
grid = np.hstack((r1,r2))
y_pred_grid=nb.predict(grid)
zz = y_pred_grid.reshape(xx.shape)
zz[zz=='setosa']=1
zz[zz=='versicolor']=2
zz[zz=='virginica']=3
xx=np.array(xx,dtype=float)
yy=np.array(yy,dtype=float)
zz=np.array(zz,dtype=float)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
plt.legend(iris.target_names)
plt.contourf(xx, yy, zz,cmap='RdBu_r')
plt.subplot(122)
ConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)
plt.tight_layout()  
plt.show()
```

## Gaussian Process Classification

The Gaussian Process Classification algorithm from the sklearn library ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)) uses a general form of the Gaussian probability distribution model and is based on Laplace approximation. As it uses Gaussian probability, the model can compute confidence intervals and determine if refitting of a certain section is required based on probability alone. The algorithm is kernel based, meaning multiple types of covariance functions can be utilized and used to optimize model fitting based on the input data. One downside of this algorithm is that is loses efficiency when the number of features grows larger than a few dozen.

As seen from @fig-gmm, the algorithm uses more linear decision boundaries compared to the above Gaussian Naive to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 5 points leading to a mean accuracy of 96.667%. Both of these algorithms are Gaussian in nature, so it is expected that they have similar results.

```{python}
nb = GaussianProcessClassifier()
nb.fit(x_train, y_train)
print('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))
y_pred=nb.predict(x_train)
cm=confusion_matrix(y_train,y_pred,labels=iris.target_names)
```

```{python}
#| code-fold: true
#| code-summary: Plotting code
#| label: fig-gmm
#| fig-cap: Gaussian Process Classifier classification results (with decision boundaries) and confusion matrix 

# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07
fig, axs = plt.subplots(1,2,figsize =(8, 5))
plt.subplot(121)
x1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))
x2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))
xx, yy = np.meshgrid(x1grid, x2grid)
r1, r2 = xx.flatten(), yy.flatten()
r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
grid = np.hstack((r1,r2))
y_pred_grid=nb.predict(grid)
zz = y_pred_grid.reshape(xx.shape)
zz[zz=='setosa']=1
zz[zz=='versicolor']=2
zz[zz=='virginica']=3
xx=np.array(xx,dtype=float)
yy=np.array(yy,dtype=float)
zz=np.array(zz,dtype=float)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
plt.legend(iris.target_names)
plt.contourf(xx, yy, zz,cmap='RdBu_r')
plt.subplot(122)
ConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)
plt.tight_layout()  
plt.show()
```

## Random Forest Classifier

The Random Forest Classifier as part of the sklearn library ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) uses decision tree classifiers on multiple parts of the dataset, and then averages scores from each one to maintain a high accuracy and attempt to prevent over fitting. The default arguments were used for this algorithm, but the most important one to call out and be aware of is the number of trees/estimators in the forest is defaulted to 100. Random Forest has the additional benefit of being less computationally expensive as other classification models, such as neural networks.

As seen from @fig-rf, the algorithm uses boxier decision boundaries than the previous two algorithms to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 1 point leading to a mean accuracy of 99.333%. This is by far the best accuracy score of the three algorithms and would be the best model to use for this specific dataset for further work.

```{python}
nb = RandomForestClassifier()
nb.fit(x_train, y_train)
print('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))
y_pred=nb.predict(x_train)
cm=confusion_matrix(y_train,y_pred,labels=iris.target_names)
```

```{python}
#| code-fold: true
#| code-summary: Plotting code
#| label: fig-rf
#| fig-cap: Random Forest Classifier classification results (with decision boundaries) and confusion matrix 

# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07
fig, axs = plt.subplots(1,2,figsize =(8, 5))
plt.subplot(121)
x1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))
x2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))
xx, yy = np.meshgrid(x1grid, x2grid)
r1, r2 = xx.flatten(), yy.flatten()
r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
grid = np.hstack((r1,r2))
y_pred_grid=nb.predict(grid)
zz = y_pred_grid.reshape(xx.shape)
zz[zz=='setosa']=1
zz[zz=='versicolor']=2
zz[zz=='virginica']=3
xx=np.array(xx,dtype=float)
yy=np.array(yy,dtype=float)
zz=np.array(zz,dtype=float)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)
plt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
plt.legend(iris.target_names)
plt.contourf(xx, yy, zz,cmap='RdBu_r')
plt.subplot(122)
ConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)
plt.tight_layout()  
plt.show()
```

# Results and Conclusion

The three classification algorithms used in this blog post used a relatively small dataset to attempt and match the original labels. Both Gaussian classification models (Gaussian Naive Bayes and Gaussian Process Classification) performed similarly and had \~96% accuracy. The Random Forest classification algorithm performed much better than the previous two, with a mean accuracy score \> 99% and only misidentified a single data point.
