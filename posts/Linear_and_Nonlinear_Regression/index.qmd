---
title: "Linear and Nonlinear Regression"
author: "Eric Jackson"
date: "2023-10-03"
categories: [linear,nonlinear,regression]
image: "image.jpg"
toc: true
toc-depth: 2
---

# Background

Regression analysis is a broad and useful field of statistical analysis, and for the purpose of this blog, only linear and nonlinear regression will be discuessed. Both linear and nonlinear regression have many practical uses and are used to estimate the relationships between two or more variables, using either lines or curves. The main use of linear or nonlinear regression are for predicting or forecasting data and thus, is used widely in machine learning situations. Regression analysis is also used to infer casual relationships between variables, otherwise called correlation. \[[1](https://en.wikipedia.org/wiki/Linear_regression)\]

# Regression Techniques

## Linear Regression

Linear regression is a technique that uses a straight line to model the relationship between two or more variables. There are several models that can be used for linear regression, with the most common one being the least squares method. Linear regression can also be performed by minimizing the lack of fit or by minimizing a version of the least squares cost function (ridge regression and lasso) \[[1](https://en.wikipedia.org/wiki/Linear_regression)\]. The LinearRegression model from the sklearn library utilizes the least squares method to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation \[[2](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\].

A dataset native to the sklearn library was used for this blog post (load_wine) and two features were chosen at random based on how linear the trend appeared. As can be seen in @fig-lin, the data appears random with a slight positive trend. The linear regression model was overlayed on top of the original data with a red line, and can be seen to match the positive slope of the data very closely. It also sits roughly in the middle of the data throughout, and leads to an RMSE of 2.403.

```{python}
#| code-fold: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_wine
import math

df=load_wine()
df=pd.DataFrame(df['data'],columns=df['feature_names'])
df=df[['alcohol','proline']]
x=df.alcohol
y=df.proline
x=np.array(x.values).reshape(-1,1)
y=np.array(y.values).reshape(-1,1)/100

model = LinearRegression(fit_intercept=True)
model.fit(x,y)

test_predictions = model.predict(x)
from sklearn.metrics import mean_absolute_error,mean_squared_error
MAE = mean_absolute_error(y,test_predictions)
MSE = mean_squared_error(y,test_predictions)
RMSE = np.sqrt(MSE)
print('RMSE: %3.3f' % RMSE)
```

```{python}
#|code-fold: true
#|code-summary: Plotting code
#| label: fig-lin
#| fig-cap: "Linear regression model on data"
fig, ax = plt.subplots(figsize=(8,6))
plt.scatter(x,y)
plt.plot(x,test_predictions,color="red")
plt.legend(['Data','Linear Prediction'])
plt.xlabel('$x_{1}$')
plt.ylabel('$x_{2}$')
plt.show()
```

## Nonlinear Regression

Nonlinear regression is similar to linear regression, but uses a curve to fit the data, compared to a straight line with linear regression. Most problems or data in the real world are not linear in nature, so nonlinear regression allows for a much greater accuracy when analyzing items.

The dataset used for the above linear regression section was used and slightly manipulated to create a more nonlinear quadratic shape. This was done to more easily show the difference between linear and nonlinear regression.

After fitting lines of best fit for varying degrees of polynomials (see @fig-poly), it can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below (@fig-table) shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.

```{python}
#| code-fold: true
#| code-summary: Imports and dataset setup
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_wine
import math
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

df=load_wine()
df=pd.DataFrame(df['data'],columns=df['feature_names'])
df=df[['alcohol','proline']]
x=df.alcohol
y=df.proline
x=np.array(x.values).reshape(-1,1)
xp=x
y=np.array(y.values).reshape(-1,1)/100
yp=y

polynomial_converter = PolynomialFeatures(degree=2,include_bias=False)
poly_features = polynomial_converter.fit_transform(y)

x=poly_features[:,0].reshape(-1,1)
y=poly_features[:,1].reshape(-1,1)*y/100

x[:,0].sort()
y[:,0].sort()
```

```{python}
# Linear regression
polybig_features = PolynomialFeatures(degree=1, include_bias=False)
std_scaler = StandardScaler()
lin_reg = LinearRegression()
polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
polynomial_regression.fit(x, y)
X_newlin = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)
y_newbiglin = polynomial_regression.predict(X_newlin)

# Polynomial regression with degree = 2
polybig_features = PolynomialFeatures(degree=2, include_bias=False)
polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
polynomial_regression.fit(x, y)
X_new = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)
y_newbig = polynomial_regression.predict(X_new)

# Polynomial regression with degree = 3
polybig_features = PolynomialFeatures(degree=3, include_bias=False)
polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
polynomial_regression.fit(x, y)
X_newlin3 = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)
y_newbiglin3 = polynomial_regression.predict(X_new)
```

```{python}
#| code-fold: true
#|code-summary: Plotting code
#| label: fig-poly
#| fig-cap: "Polynomial/Nonlinear regression model on data"
from pandas.plotting import table
fig, ax = plt.subplots(figsize=(8,6))
plt.scatter(x,y,marker='o')
plt.plot(X_newlin, y_newbiglin,color='green')
plt.plot(X_new, y_newbig,color='red')
plt.plot(X_newlin3, y_newbiglin3,color='black')
plt.legend(['Data','Linear fit','Polynomial fit (deg = 2)','Polynomial fit (deg = 3)'])
plt.show()
```

```{python}
#| code-fold: true
#|code-summary: Table code
#| label: fig-table
#| fig-cap: "R squared values for each regression model"
linscore=polynomial_regression.score(X_newlin,y_newbiglin)
linscore2=polynomial_regression.score(X_new,y_newbig)
linscore3=polynomial_regression.score(X_newlin3,y_newbiglin3)

df=pd.DataFrame([[linscore,linscore2,linscore3]],columns=['Linear Regression','Polynomial Regression (deg = 2)','Polynomial Regression (deg = 3)'],index=['R squared value','R^21','R^22'])
fix, ax = plt.subplots(figsize=(8,1))
ax.axis('off')
table(ax,df.transpose()['R squared value'],loc='top',cellLoc='center',colWidths=list([.6, .6]))
plt.show()
```

# Conclusion

As can be seen from the results of this blog post, both linear and nonlinear regression are useful tools when analyzing datasets. Each type of regression serves its own purpose and should be chosen based on the data being monitored/analyzed. For nonlinear regression, it is important to not overfit the model and apply too high of a degree to the polynomial, as this can lead to decreased accuracy.
