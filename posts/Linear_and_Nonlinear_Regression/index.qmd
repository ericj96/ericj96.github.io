---
title: "Linear and Nonlinear Regression"
author: "Eric Jackson"
date: "2023-10-03"
categories: [linear,nonlinear,regression]
image: "image.jpg"
toc: true
toc-depth: 2
---

# Background

# Regression Techniques

## Linear Regression

```{python}
#| code-fold: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import math

df=load_wine()
df=pd.DataFrame(df['data'],columns=df['feature_names'])
df=df[['alcohol','proline']]
x=df.alcohol
y=df.proline
x=np.array(x.values).reshape(-1,1)
y=np.array(y.values).reshape(-1,1)/100

model = LinearRegression(fit_intercept=True)
model.fit(x,y)

test_predictions = model.predict(x)
from sklearn.metrics import mean_absolute_error,mean_squared_error
MAE = mean_absolute_error(y,test_predictions)
MSE = mean_squared_error(y,test_predictions)
RMSE = np.sqrt(MSE)
print('RMSE: %3.3f' % RMSE)
```

```{python}
#|code-fold: true
#|code-summary: Plotting code
fig, ax = plt.subplots(figsize=(8,6))
plt.scatter(x,y)
plt.plot(x,test_predictions,color="red")
plt.legend(['Data','Linear Prediction'])
plt.xlabel('$x_{1}$')
plt.ylabel('$x_{2}$')
plt.show()
```

## Nonlinear Regression

It can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.

```{python}
#| code-fold: true
#| code-summary: Imports and dataset setup
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import math
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn.preprocessing import PolynomialFeatures

df=load_wine()
df=pd.DataFrame(df['data'],columns=df['feature_names'])
df=df[['alcohol','proline']]
x=df.alcohol
y=df.proline
x=np.array(x.values).reshape(-1,1)
xp=x
y=np.array(y.values).reshape(-1,1)/100
yp=y

polynomial_converter = PolynomialFeatures(degree=2,include_bias=False)
poly_features = polynomial_converter.fit_transform(y)

x=poly_features[:,0].reshape(-1,1)
y=poly_features[:,1].reshape(-1,1)*y/100

x[:,0].sort()
y[:,0].sort()
```

```{python}
# Linear regression
polybig_features = PolynomialFeatures(degree=1, include_bias=False)
std_scaler = StandardScaler()
lin_reg = LinearRegression()
polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
polynomial_regression.fit(x, y)
X_newlin = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)
y_newbiglin = polynomial_regression.predict(X_newlin)

# Polynomial regression with degree = 2
polybig_features = PolynomialFeatures(degree=2, include_bias=False)
polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
polynomial_regression.fit(x, y)
X_new = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)
y_newbig = polynomial_regression.predict(X_new)

# Polynomial regression with degree = 3
polybig_features = PolynomialFeatures(degree=3, include_bias=False)
polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)
polynomial_regression.fit(x, y)
X_newlin3 = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)
y_newbiglin3 = polynomial_regression.predict(X_new)
```

```{python}
#| code-fold: true
#|code-summary: Plotting code
fig, ax = plt.subplots(figsize=(8,6))
plt.scatter(x,y,marker='o')
plt.plot(X_newlin, y_newbiglin,color='green')
plt.plot(X_new, y_newbig,color='red')
plt.plot(X_newlin3, y_newbiglin3,color='black')
plt.legend(['Data','Linear fit','Polynomial fit (deg = 2)','Polynomial fit (deg = 3)'])
plt.show()
from pandas.plotting import table

linscore=polynomial_regression.score(X_newlin,y_newbiglin)
linscore2=polynomial_regression.score(X_new,y_newbig)
linscore3=polynomial_regression.score(X_newlin3,y_newbiglin3)

df=pd.DataFrame([[linscore,linscore2,linscore3]],columns=['Linear Regression','Polynomial Regression (deg = 2)','Polynomial Regression (deg = 3)'],index=['R squared','R^21','R^22'])
fix, ax = plt.subplots()
ax.axis('off')
table(ax,df.transpose()['R squared'],loc='top',cellLoc='center',colWidths=list([.6, .6]))
plt.show()
```
