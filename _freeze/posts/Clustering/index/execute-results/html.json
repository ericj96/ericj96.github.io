{
  "hash": "57820e8b566cae0248b1840a441ab7ff",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-04\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nClustering is an\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\n# plot code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"Mean Radius\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"Mean Smoothness\")\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n    plt.show()\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn import cluster \nfrom sklearn.cluster import KMeans\nfrom itertools import cycle, islice\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\n# setting up blobs code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\n\n#blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8], [-2.8,  2.8], [-2.8,  1.3]])\n                         \n# randomize centers of blobs \nblob_centers= np.random.randint(0,5,size=(5,2))+.116846842*.65487984\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\nx, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n                  random_state=7)\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(x)     \n\ncm = confusion_matrix(y, y_pred)\ncm_argmax = cm.argmax(axis=0)\ny_pred_ = np.array([cm_argmax[i] for i in y_pred])\ncm_ = confusion_matrix(y, y_pred)\naccuracy_score(y,y_pred_)\n\nfrom sklearn.cluster import DBSCAN\ndbscan=DBSCAN(eps=0.5)\ndbscan.fit(x)\ncolors = dbscan.labels_\ny_pred=colors\ncm = confusion_matrix(y, y_pred)\ncm_argmax = cm.argmax(axis=0)\ny_pred_ = np.array([cm_argmax[i] for i in y_pred])\ncm_ = confusion_matrix(y, y_pred)\naccuracy_score(y,y_pred_)\nfig, ax = plt.subplots(figsize=(9,7))\n\nplot_dbscan(dbscan, x, size=9)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=8.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=736 height=597}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncolors = np.array(\n    list(\n        islice(\n            cycle(\n                [\n                    \"#377eb8\",\n                    \"#ff7f00\",\n                    \"#4daf4a\",\n                    \"#f781bf\",\n                    \"#a65628\",\n                    \"#984ea3\",\n                    \"#999999\",\n                    \"#e41a1c\",\n                    \"#dede00\",\n                ]\n            ),\n            int(max(y_pred) + 1),\n        )\n    )\n)\n\n        \ncolors = np.append(colors, [\"#000000\"])\nfig, ax = plt.subplots(figsize=(9,7))\nplt.scatter(x[:,0],x[:,1],color=colors[y_pred])\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\n#plt.show()\n\n\n\ncolors = np.array(\n    list(\n        islice(\n            cycle(\n                [\n                    \"#377eb8\",\n                    \"#ff7f00\",\n                    \"#4daf4a\",\n                    \"#f781bf\",\n                    \"#a65628\",\n                    \"#984ea3\",\n                    \"#999999\",\n                    \"#e41a1c\",\n                    \"#dede00\",\n                ]\n            ),\n            int(max(y) + 1),\n        )\n    )\n)\n\n\ncolors = np.append(colors, [\"#000000\"])\nfig, ax = plt.subplots(figsize=(9,7))\nplt.scatter(x[:,0],x[:,1],color=colors[y])\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\n#plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=730 height=577}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=730 height=577}\n:::\n:::\n\n\n# Clustering Algorithms\n\n## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nAs explored in the [blog post on anomaly/outlier detection](https://ericj96.github.io/posts/Anomaly_Outlier_Detection/), the DBSCAN algorithm is a widely used machine learning algorithm for clustering given sets of points by grouping together points that are close together and have multiple nearest neighbors. While creating the clusters of the data, it will naturally identify outliers and thus, is a great algorithm to accomplish both outlier detection and clustering.\n\nOTHERS\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\nparams = default_base.copy()\naffinity_propagation = cluster.AffinityPropagation(\ndamping=params[\"damping\"],\npreference=params[\"preference\"],\nrandom_state=params[\"random_state\"],\n)\naffinity_propagation.fit(x)\ny_pred = affinity_propagation.labels_.astype(int)\naccuracy_score(y,y_pred)\ncm = confusion_matrix(y, y_pred)\ncm_argmax = cm.argmax(axis=0)\ny_pred_ = np.array([cm_argmax[i] for i in y_pred])\ncm_ = confusion_matrix(y, y_pred)\naccuracy_score(y,y_pred_)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0.7695\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nseed = 30\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nb,truth = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n\nblob_centers= np.random.randint(0,5,size=(5,2))+.116846842*.65487984\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\n#blobs = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n#                  random_state=7)\n#b,truth = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n#                 random_state=7)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 13))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    \n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n   \n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n       \n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n        \n        cm = confusion_matrix(truth, y_pred)\n        cm_argmax = cm.argmax(axis=0)\n        y_pred_ = np.array([cm_argmax[i] for i in y_pred])\n        cm_ = confusion_matrix(y, y_pred)\n        #accuracy_score(y,y_pred_)\n        acc=accuracy_score(truth,y_pred_)\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            #(\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            ('%3.6f' % acc),\n            transform=plt.gca().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.savefig('./pic.png')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning:\n\nMiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=2\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=1955 height=1257}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}