{
  "hash": "67f4e79a91aaa61ab30d03c4ebcc0433",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-01\"\ncategories: [clustering]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nClustering is a technique commonly used in statistics and machine learning to group similar groups of data together. This allows for things like data compression, optimization of processes, and identification of oddities in data to be accomplished much easier than normal. Each of the clusters identified will share common traits, decided by the specific algorithm. For the purposes of this blog post, a random set of blob data was generated and used with 5 clusters visually obvious.\n\n@fig-og shows the original data and the clusters identified as truth. In order to keep the same results throughout iterations of making this blog post, a random seed (170) was selected so the same results can be reproduced, while still being randomized. The results from the below algorithms will reference and be compared against this truth data.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Setup and imports\"}\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs,make_circles\nfrom sklearn import cluster \nfrom sklearn.cluster import KMeans,DBSCAN\nfrom itertools import cycle, islice\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom pandas.plotting import table\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnoisy_circles = make_circles(n_samples=2500, factor=0.5, noise=0.05)\nx=noisy_circles[0]\ny=noisy_circles[1]\nnp.random.seed(170) \nblob_centers=np.random.uniform(0,5,[5,2])\nblob_std = np.array([.3, .3, 0.4, .3 ,.1])\nx,y=make_blobs(n_samples=2500, cluster_std=blob_std,centers=blob_centers,random_state=170)\n# plotting original data\nfig, ax = plt.subplots(figsize=(6,4))\nplt.scatter(x[:,0],x[:,1],c=y)\nplt.title('Original Data')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Original data with true clusters identified](index_files/figure-html/fig-og-output-1.png){#fig-og width=495 height=357}\n:::\n:::\n\n\n# Clustering Algorithms\n\nFor each of the four clustering methods used below, a brief description and the code of training the model will be shown, with the resulting data and plots being shown in @sec-conc.\n\n## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nAs explored in the [blog post on anomaly/outlier detection](https://ericj96.github.io/posts/Anomaly_Outlier_Detection/), the DBSCAN algorithm is a widely used machine learning algorithm for clustering given sets of points by grouping together points that are close together and have multiple nearest neighbors. While creating the clusters of the data, it will naturally identify outliers and thus, is a great algorithm to accomplish both outlier detection and clustering.\n\n**Note:** An epsilon value of 0.2 was used for the DBSCAN algorithm. This parameter \"defines the maximum distance between two samples for one to be considered as in the neighborhood of the other\". [\\[1\\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndbscan=DBSCAN(eps=0.2)\ndbscan.fit(x)\ncolors = dbscan.labels_\ny_pred_dbscan=colors\n```\n:::\n\n\n## Affinity Propagation\n\nUnlike the two algorithms that will be investigated below, Affinity Propagation clustering does not require the number of clusters as an argument prior to running the algorithm. It will iterate through the data and find exemplar points that are representative of each cluster. Two arguments were used for this algorithm, a damping factor value of 0.9 and a preference value of -200. The damping value is the extent that the current value is maintained relative to incoming values to avoid oscillations, and the preference value allows for points preference in the availability matrix to be set and allows the model to more accurately predict the number of clusters. [\\[2\\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\naffinity_propagation = cluster.AffinityPropagation(damping=.9,preference=-200)\naffinity_propagation.fit(x)\ny_pred_af=affinity_propagation.labels_\n```\n:::\n\n\n## Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)\n\nThe BIRCH clustering algorithm is often used as an alternative to the K means algorithm, as it is more memory efficient and works exceptionally well with larger datasets. It works by constructing a clustering feature tree with a set of nodes and subclusters for each node. The algorithm will merge together subclusters with the smallest radius, and then see if each subcluster has any child nodes. If so, it will continue the same process until it reaches a leaf of the original tree. [\\[3\\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)\n\nAs mentioned above, the BIRCH algorithm can take the number of clusters as an input, and since this number is known to be 5, n_clusters = 5 will be used as the only argument.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nbirch = cluster.Birch(n_clusters=5)\nbirch.fit(x)\ny_pred_birch=birch.labels_\n```\n:::\n\n\n## Spectral Clustering\n\nThe Spectral Clustering algorithm works by taking the eigenvalues of the similarity matrix, reducing the dimensions, and then performing clustering but now with fewer dimensions. This method is useful for data that is non-convex or irregularly shaped, as well as high dimensional datasets. [\\[4\\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)\n\nAs mentioned above, the Spectral Clustering algorithm can take the number of clusters as an input, and since this number is known to be 5, n_clusters = 5 will be used as the only argument.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nspectral = cluster.SpectralClustering(n_clusters=5)\nspectral.fit(x)   \ny_pred_spectral=spectral.labels_\n```\n:::\n\n\n# Conclusion  {#sec-conc}\n\nSince most clustering algorithms will apply arbitrary labels to the dataset passed through them, it isn't possible to do a simple accuracy scoring or confusion matrix comparison. Therefore, an Adjusted Rand Index (ARI) comparison will be performed between the original blob labels and the predicted/fitted labels from each algorithm. This will allow all four of the clustering algorithms explored above to be compared and ranked against each other. The ARI is a version of the Rand index, which measures the similarity between two data clusterings, but is corrected for chance. It will look at all pairs of samples and compute how many are in the same or different clusters between the modeled and true data. [\\[5\\]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html)\n\nAs seen in @fig-sub, the DBSCAN algorithm was able to identify 4 of the 5 clusters correctly, but combined two of the clusters into one and considered points on the edge of each cluster as a separate cluster. This could be due to the DBSCAN algorithms ability to identify outlier points, as all of the purple cluster points appear to be outliers. The remaining three algorithms all performed similarly, correctly identifying all 5 clusters to a certain degree. @fig-tab shows the results in terms of ARI, showing that DBSCAN obviously had the lowest score and the other methods having very high but similar scores. Spectral Clustering edged out BIRCH and Affinity Propagation slightly, with an ARI score of 0.98.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"All 4 subplot code\"}\nfig, ax = plt.subplots(2,2,figsize=(9,8))\nax1=plt.subplot(221)\nplt.scatter(x[:,0],x[:,1],c=y_pred_dbscan)\nplt.title('DBSCAN')\nax1=plt.subplot(222)\nplt.scatter(x[:,0],x[:,1],c=y_pred_af)\nplt.title('Affinity Propagation')\nax1=plt.subplot(223)\nplt.scatter(x[:,0],x[:,1],c=y_pred_birch)\nplt.title('BIRCH')\nax1=plt.subplot(224)\nplt.scatter(x[:,0],x[:,1],c=y_pred_spectral)\nplt.title('Spectral Clustering')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![All 4 algorithms and the clusters identified by each](index_files/figure-html/fig-sub-output-1.png){#fig-sub width=718 height=653}\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table generation code\"}\nscores=[adjusted_rand_score(y, dbscan.labels_),adjusted_rand_score(y, affinity_propagation.labels_),adjusted_rand_score(y, birch.labels_) ,adjusted_rand_score(y, spectral.labels_)]\ndf=pd.DataFrame([scores,scores,scores,scores],columns=['DBSCAN','Affinity Propagation','BIRCH','Spectral Clustering'],index=['Adjusted Rand Score','R^21','R^22','R'])\nfix, ax = plt.subplots(figsize=(8,1))\nax.axis('off')\ntable(ax,df.transpose()['Adjusted Rand Score'],loc='top',cellLoc='center',colWidths=list([.6, .6]))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![ARI for each of the 4 clustering algorithms](index_files/figure-html/fig-tab-output-1.png){#fig-tab width=653 height=173}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}