{
  "hash": "400d5c68d9a8adf8a0a269823e05b9b6",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-01\"\ncategories: [classification]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background and Setup\n\nClassification is similar to clustering ([as mentioned in this blog post](https://ericj96.github.io/posts/Clustering/)) in the sense that they group data together into separate categories. The difference comes from the fact that classification has a predefined set of labels that are attached to each data point, and in clustering the labels are missing and the algorithm will apply those labels/groupings. These two topics are commonly referenced as supervised and unsupervised learning (classification vs clustering).\n\nSome of the downsides of classification are that there is a need to train the model before using it with test data, whereas clustering does not require such training and can group data points together immediately. But, classification can be used for much more intensive scenarios, such as handwriting recognition and spam filtering.\n\nAs mentioned before, classification algorithms or models require inputs with labels predefined. For this blog post, a native sklearn dataset containing measurements of a species of flower will be used. As seen below, there are width and length measurements along with the label/target class for the species of flower.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Setup/imports\"}\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.inspection import DecisionBoundaryDisplay\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\niris=load_iris()\ndf = pd.DataFrame(iris['data'],columns = iris['feature_names'])\ntarget=pd.DataFrame(iris['target'])\ntarget[target==0]=iris.target_names[0]\ntarget[target==1]=iris.target_names[1]\ntarget[target==2]=iris.target_names[2]\ndf['target']=target\n\n# breaking up data into training and test datasets\nx_train=df[['petal length (cm)', 'petal width (cm)']]\ny_train=df['target']\nx_test=df[['petal length (cm)', 'petal width (cm)']]\ny_test=df['target']\n```\n:::\n\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows Ã— 5 columns</p>\n</div>\n```\n:::\n:::\n\n\nBefore the data is to be run through any classification algorithms, it is helpful to first plot the data and review what to expect. @fig-class shows the dataset visually with each color representing a different label or species of flower.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\nfig, axs = plt.subplots(figsize =(7, 5))\nplt.scatter(df['petal length (cm)'][df['target']==iris.target_names[0]],df['petal width (cm)'][df['target']==iris.target_names[0]])\nplt.scatter(df['petal length (cm)'][df['target']==iris.target_names[1]],df['petal width (cm)'][df['target']==iris.target_names[1]])\nplt.scatter(df['petal length (cm)'][df['target']==iris.target_names[2]],df['petal width (cm)'][df['target']==iris.target_names[2]])\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Dataset values broken into their defined target labels](index_files/figure-html/fig-class-output-1.png){#fig-class width=589 height=429}\n:::\n:::\n\n\n# Classification Algorithms\n\n## Gaussian Naive Bayes\n\nThe Gaussian Naive Bayes algorithm from the sklearn library ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)) utilizes Bayes' theorem assuming the features probability is distributed in a Gaussian or normal fashion, with the variance and mean of each data point calculated for each class. There are a multitude of Naive Bayes classifier algorithms (Bernoulli, Multinomial, etc), but the specific Gaussian probabilistic version used here is especially useful when the values are continuous and expected to follow a Gaussian distribution.\n\nAs seen from @fig-nb, the algorithm uses circular decision boundaries to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 6 points leading to a mean accuracy of 96%.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean accuracy score: 96.000%\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\n# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nx1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))\nx2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))\nxx, yy = np.meshgrid(x1grid, x2grid)\nr1, r2 = xx.flatten(), yy.flatten()\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\ngrid = np.hstack((r1,r2))\ny_pred_grid=nb.predict(grid)\nzz = y_pred_grid.reshape(xx.shape)\nzz[zz=='setosa']=1\nzz[zz=='versicolor']=2\nzz[zz=='virginica']=3\nxx=np.array(xx,dtype=float)\nyy=np.array(yy,dtype=float)\nzz=np.array(zz,dtype=float)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.contourf(xx, yy, zz,cmap='RdBu_r')\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\nplt.tight_layout()  \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Gaussian Naive Bayes classification results (with decision boundaries) and confusion matrix](index_files/figure-html/fig-nb-output-1.png){#fig-nb width=757 height=468}\n:::\n:::\n\n\n## Gaussian Process Classification\n\nThe Gaussian Process Classification algorithm from the sklearn library ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)) uses a general form of the Gaussian probability distribution model and is based on Laplace approximation. As it uses Gaussian probability, the model can compute confidence intervals and determine if refitting of a certain section is required based on probability alone. The algorithm is kernel based, meaning multiple types of covariance functions can be utilized and used to optimize model fitting based on the input data. One downside of this algorithm is that is loses efficiency when the number of features grows larger than a few dozen.\n\nAs seen from @fig-gmm, the algorithm uses more linear decision boundaries compared to the above Gaussian Naive to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 5 points leading to a mean accuracy of 96.667%. Both of these algorithms are Gaussian in nature, so it is expected that they have similar results.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nnb = GaussianProcessClassifier()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean accuracy score: 96.667%\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\n# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nx1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))\nx2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))\nxx, yy = np.meshgrid(x1grid, x2grid)\nr1, r2 = xx.flatten(), yy.flatten()\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\ngrid = np.hstack((r1,r2))\ny_pred_grid=nb.predict(grid)\nzz = y_pred_grid.reshape(xx.shape)\nzz[zz=='setosa']=1\nzz[zz=='versicolor']=2\nzz[zz=='virginica']=3\nxx=np.array(xx,dtype=float)\nyy=np.array(yy,dtype=float)\nzz=np.array(zz,dtype=float)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.contourf(xx, yy, zz,cmap='RdBu_r')\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\nplt.tight_layout()  \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Gaussian Process Classifier classification results (with decision boundaries) and confusion matrix](index_files/figure-html/fig-gmm-output-1.png){#fig-gmm width=757 height=468}\n:::\n:::\n\n\n## Random Forest Classifier\n\nThe Random Forest Classifier as part of the sklearn library ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) uses decision tree classifiers on multiple parts of the dataset, and then averages scores from each one to maintain a high accuracy and attempt to prevent over fitting. The default arguments were used for this algorithm, but the most important one to call out and be aware of is the number of trees/estimators in the forest is defaulted to 100. Random Forest has the additional benefit of being less computationally expensive as other classification models, such as neural networks.\n\nAs seen from @fig-rf, the algorithm uses boxier decision boundaries than the previous two algorithms to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 1 point leading to a mean accuracy of 99.333%. This is by far the best accuracy score of the three algorithms and would be the best model to use for this specific dataset for further work.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nnb = RandomForestClassifier()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean accuracy score: 99.333%\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\n# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nx1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))\nx2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))\nxx, yy = np.meshgrid(x1grid, x2grid)\nr1, r2 = xx.flatten(), yy.flatten()\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\ngrid = np.hstack((r1,r2))\ny_pred_grid=nb.predict(grid)\nzz = y_pred_grid.reshape(xx.shape)\nzz[zz=='setosa']=1\nzz[zz=='versicolor']=2\nzz[zz=='virginica']=3\nxx=np.array(xx,dtype=float)\nyy=np.array(yy,dtype=float)\nzz=np.array(zz,dtype=float)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.contourf(xx, yy, zz,cmap='RdBu_r')\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\nplt.tight_layout()  \nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Random Forest Classifier classification results (with decision boundaries) and confusion matrix](index_files/figure-html/fig-rf-output-1.png){#fig-rf width=757 height=468}\n:::\n:::\n\n\n# Results and Conclusion\n\nThe three classification algorithms used in this blog post used a relatively small dataset to attempt and match the original labels. Both Gaussian classification models (Gaussian Naive Bayes and Gaussian Process Classification) performed similarly and had \\~96% accuracy. The Random Forest classification algorithm performed much better than the previous two, with a mean accuracy score \\> 99% and only misidentified a single data point.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}