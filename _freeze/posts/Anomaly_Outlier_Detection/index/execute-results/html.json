{
  "hash": "1d64a0977ea88dd7d1342948f608934b",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-05\"\ncategories: [outlier,anomaly,detection]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nIn this blog post, two popular outlier detection algorithms (DBSCAN and Isolation Forest) will be used and compared against each other. Outlier detection is important for preprocessing datasets as there is the potential that outliers can throw off the results of a machine learning model. By removing outliers, it is possible to get a more accurate result. These methods can also be used for anomaly detection in time series data, spacecraft telemetry for example, to identify potential out of ordinary trends in data. This can potentially allow for a preemptive response to an issue (ex: temperature changing, so heaters are powered on/off) and reduces the amount of time that the spacecraft is out of mission.\n\nFor simplicity, a known dataset will be imported and used with two features as the dataset. The breast cancer dataset from sklearn is a commonly used binary classification dataset from UC Irvine showing information of tumor characteristics in Wisconsin.\n\n@fig-whisker shows a box and whisker plot of both sets of data for an initial look at the outliers identified through purely statistical means.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Setup and plotting of initial data\"}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndata=X_train[[\"mean radius\",\"mean smoothness\"]]\ndf=data;\n\nred_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\nfig, axs = plt.subplots(len(df.columns),1, figsize=(8,6))\nfor i, ax in enumerate(axs.flat):\n    ax.boxplot(df.iloc[:,i], flierprops=red_circle,vert=False)\n    ax.set_title(df.columns[i])\n    ax.tick_params(axis='y', labelsize=14)\n  \nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Box/whisker plot of initial data](index_files/figure-html/fig-whisker-output-1.png){#fig-whisker width=758 height=565}\n:::\n:::\n\n\n# Outlier Detection Algorithms\n\n## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nDBSCAN is a popular algorithm used mainly for clustering given sets of points. It works by grouping together points that are close together and have multiple nearest neighbors. It considers outliers to be points that are alone in low density regions. [\\[1\\]](https://en.wikipedia.org/wiki/DBSCAN)\n\nFor the example in this blog, a two-dimensional set of data (mean radius and mean smoothness) is used to run through the DBSCAN algorithm. @fig-scatter shows the result with outlier/anomalous data points shown in orange. Since DBSCAN is most known for being a clustering algorithm, the bottom plot shows the different clusters that the algorithm identified. All points with a color and a star indicate a clustered point, of which there are two separate clusters that the algorithm identified. Any point with a red X indicates it is an anomalous/outlier point, and any point that is not surrounded by color and is a period is a non-core point (of which there were only two single points identified as such).\n\n**Note:** The default epsilon parameter value of 0.5 was used for the DBSCAN algorithm. This parameter \"defines the maximum distance between two samples for one to be considered as in the neighborhood of the other\". [\\[2\\]](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"plot_dbscan function\"}\n# plot code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"Mean Radius\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"Mean Smoothness\")\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n    plt.show()\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \n\n# DBSCAN model training/fitting\ndbscan=DBSCAN()\ndbscan.fit(X_train[[\"mean radius\",\"mean smoothness\"]])\ncolors = dbscan.labels_\noutliers=colors.T<0\nnormal=colors.T>=0\nprint(\"Number of outliers detected: %d\" % sum(i<0 for i in colors.T))\nprint(\"Number of normal samples detected: %d\" % sum(i>=0 for i in colors.T))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of outliers detected: 8\nNumber of normal samples detected: 561\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\n#plotting\nfig, ax = plt.subplots(figsize=(9,14))\nplt.subplot(211)\nplt.plot(X_train[\"mean radius\"][colors==0],X_train[\"mean smoothness\"][colors==0],marker='o',linestyle=\"None\")\nplt.plot(X_train[\"mean radius\"][colors!=0],X_train[\"mean smoothness\"][colors!=0],marker='o',linestyle=\"None\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.title(\"Dataset Outlier Detection via DBSCAN\")\n\nplt.subplot(212)\nplot_dbscan(dbscan, X_train[[\"mean radius\",\"mean smoothness\"]].values, size=100)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Data that the DBSCAN algorithm identifies as an outlier or not](index_files/figure-html/fig-scatter-output-1.png){#fig-scatter width=746 height=1115}\n:::\n:::\n\n\n## Isolation Forest\n\nIsolation Forest is another commonly used outlier detection method which detects outliers utilizing binary trees. This method recursively partitions data points based on randomly selected attribute and then assigned anomaly scores based on number of \"splits\" needed to isolate a data point. The training dataset is used to build the \"trees\" and then the validation data is passed through those trees and assigned an anomaly score. In the case of this example, the training and validation is the same dataset (only one iteration of the call to the model). Based on the anomaly score, it can be determined which points are outliers. One of the inputs to the Isolation Forest algorithm is the contamination parameter, or the expected percentage of data that will be anomalous. For the purposes of the example, the default contamination value of 10% will be used.\n\nAdvantages:\n\n-   Low memory utilization\n\n-   Works best with large datasets\n\nNote: Below contains some modified code from [HERE](https://medium.com/mlearning-ai/unsupervised-outlier-detection-with-isolation-forest-eab398c593b2)\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\n\noutliers_fraction = float(.03)\nmodel =  IsolationForest()\ndata=df.values\nprediction = model.fit_predict(data)\n\nprint(\"Number of outliers detected: {}\".format(prediction[prediction < 0].sum()*-1))\nprint(\"Number of normal samples detected: {}\".format(prediction[prediction > 0].sum()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of outliers detected: 84\nNumber of normal samples detected: 485\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\nfig, ax = plt.subplots(figsize=(9,7))\nnormal_data = data[np.where(prediction > 0)]\noutliers = data[np.where(prediction < 0)]\nplt.scatter(normal_data[:, 0], normal_data[:, 1])\nplt.scatter(outliers[:, 0], outliers[:, 1])\nplt.title(\"Dataset Outlier Detection via Isolation Forest\")\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Data that the Isolation Forest algorithm identifies as an outlier or not](index_files/figure-html/fig-isofor-output-1.png){#fig-isofor width=746 height=597}\n:::\n:::\n\n\n# Results and Conclusion\n\nWhen comparing DBSCAN to Isolation Forest, it is obvious to see that Isolation Forest removed a multitude more outliers than the DBSCAN algorithm. But, this is partially due to the use of default parameters in each algorithm and could be potentially be optimized. If the DBSCAN epsilon parameter is reduced to \\~0.08, then the results more closely match what is seen with the Isolation Forest algorithm's default contamination parameter. Finding optimal hyperparameter values is an extremely important step in any machine learning algorithm, but comes with its own challenges and time consumption. A separate blog post on optimizing particular hyperparameters can be seen [here](https://ericj96.github.io/posts/Topic_2/).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}