{
  "hash": "7edfee5e56fe252b399091609e69e6ef",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-06\"\ncategories: [code, outlier,anomaly,detection]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nIn this blog post, two popular outlier detection algorithms (DBSCAN and Isolation Forest) will be used and compared against each other. Outlier detection is important for preprocessing datasets as there is the potential that outliers can throw off the results of a machine learning model. By removing outliers, it is possible to get a more accurate result. These methods can also be used for anomaly detection in time series data, spacecraft telemetry for example, to identify potential out of ordinary trends in data. This can potentially allow for a preemptive response to an issue (ex: temperature changing, so heaters are powered on/off) and reduces the amount of time that the spacecraft is out of mission.\n\nFor simplicity, a known dataset will be imported and used with two features as the dataset. The breast cancer dataset from sklearn is a commonly used binary classification dataset from UC Irvine showing information of tumor characteristics in Wisconsin.\n\n@fig-whisker shows a box and whisker plot of both sets of data for an initial look at the outliers identified through purely statistical means.\n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nred_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\nfig, axs = plt.subplots(len(df.columns),1, figsize=(8,6))\nfor i, ax in enumerate(axs.flat):\n    ax.boxplot(df.iloc[:,i], flierprops=red_circle,vert=False)\n    ax.set_title(df.columns[i])\n    ax.tick_params(axis='y', labelsize=14)\n  \nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Box/whisker plot of initial data](index_files/figure-html/fig-whisker-output-1.png){#fig-whisker width=758 height=565}\n:::\n:::\n\n\n# Outlier Detection Algorithms\n\n## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nDBSCAN is a popular algorithm used mainly for clustering given sets of points. It works by grouping together points that are close together and have multiple nearest neighbors. It considers outliers to be points that are alone in low density regions. [\\[1\\]](https://en.wikipedia.org/wiki/DBSCAN)\n\nFor the example in this blog, a two-dimensional set of data (mean radius and mean smoothness) is used to run through the DBSCAN algorithm. @fig-scatter shows the result with outlier/anomalous data points shown in orange.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \n\n# DBSCAN model training/fitting\ndbscan=DBSCAN()\ndbscan.fit(X_train[[\"mean radius\",\"mean smoothness\"]])\ncolors = dbscan.labels_\noutliers=colors.T<0\nnormal=colors.T>=0\nprint(\"Number of outliers detected: %d\" % sum(i<0 for i in colors.T))\nprint(\"Number of normal samples detected: %d\" % sum(i>=0 for i in colors.T))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of outliers detected: 8\nNumber of normal samples detected: 561\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n#plotting\nfig, ax = plt.subplots(figsize=(9,7))\nplt.plot(X_train[\"mean radius\"][colors==0],X_train[\"mean smoothness\"][colors==0],marker='o',linestyle=\"None\")\nplt.plot(X_train[\"mean radius\"][colors==-1],X_train[\"mean smoothness\"][colors==-1],marker='o',linestyle=\"None\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.title(\"Dataset Outlier Detection via DBSCAN\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Data that the DBSCAN algorithm identifies as an outlier or not](index_files/figure-html/fig-scatter-output-1.png){#fig-scatter width=746 height=597}\n:::\n:::\n\n\n## Isolation Forest\n\nIsolation Forest is another commonly used outlier detection method which detects outliers utilizing binary trees. This method recursively partitions data points based on randomly selected attribute and then assigned anomaly scores based on number of \"splits\" needed to isolate a data point. The training dataset is used to build the \"trees\" and then the validation data is passed through those trees and assigned an anomaly score. In the case of this example, the training and validation is the same dataset (only one iteration of the call to the model). Based on the anomaly score, it can be determined which points are outliers. One of the inputs to the Isolation Forest algorithm is the contamination parameter, or the expected percentage of data that will be anomalous. For the purposes of the example, a contamination value of 3% will be used.\n\nAdvantages:\n\n-   Low memory utilization\n\n-   Works best with large datasets\n\nNote: Below contains some modified code from [HERE](https://medium.com/mlearning-ai/unsupervised-outlier-detection-with-isolation-forest-eab398c593b2)\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.ensemble import IsolationForest\n\noutliers_fraction = float(.03)\nmodel =  IsolationForest(contamination=outliers_fraction)\ndata=df.values\nprediction= model.fit_predict(data)\n\nprint(\"Number of outliers detected: {}\".format(prediction[prediction < 0].sum()*-1))\nprint(\"Number of normal samples detected: {}\".format(prediction[prediction > 0].sum()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of outliers detected: 18\nNumber of normal samples detected: 551\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nfig, ax = plt.subplots(figsize=(9,7))\nnormal_data = data[np.where(prediction > 0)]\noutliers = data[np.where(prediction < 0)]\nplt.scatter(normal_data[:, 0], normal_data[:, 1])\nplt.scatter(outliers[:, 0], outliers[:, 1])\nplt.title(\"Dataset Outlier Detection via Isolation Forest\")\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Data that the Isolation Forest algorithm identifies as an outlier or not](index_files/figure-html/fig-isofor-output-1.png){#fig-isofor width=746 height=597}\n:::\n:::\n\n\n# Conclusion\n\nIt can be seen that both DBSCAN and Isolation Forest identified most of the same outlier points, but Isolation Forest identified 10 more total outlier data points, mainly due to the contamination parameter being large (3%). Both methods are valid options, with Isolation Forest being more accurate depending on how accurate the contamination parameter is.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}