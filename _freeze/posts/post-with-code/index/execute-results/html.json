{
  "hash": "420c08433f58d56b573c74d3564586f6",
  "result": {
    "markdown": "---\ntitle: \"Anomaly Detection on Spacecraft Telemetry\"\nauthor: \"Eric Jackson\"\ndate: \"2023-09-08\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nSpacecraft will generate a massive amount of data the longer they are on orbit, from telemetry data containing voltages, temperatures, etc to raw data from the various types of payloads on orbit\n\n-   Spacecraft have onboard anomaly responses for most known failure cases to safe the vehicle\n\n-   Normally low/high, red/yellow limits set for certain monitors with corresponding response (either automatic or visual alarm)\n\n-   Some anomalies can be hard to predict, multiple components can react slightly out of family to create larger issue\n\n-   Benefits of utilizing machine learning for spacecraft:\n\n-   Prevents loss of mission over potentially high priority targets\n\n-   Automatic response would limit both downtime and human interaction\n\n-   Higher award/incentive fees for lower mission outage percentage\n\n-   Limits time spent by operators and factory investigating and implementing a fix\n\n-   Depending on program and customer, recovery can take anywhere from a few hours to multiple days\n\n-   Predict future anomalous conditions and potentially react before an issue were to occur\n\n-   Some programs have multiple vehicles on orbit meaning there is a plethora of historical training data available\n\n-   Goal: Utilize ARIMA & OCSVM to create a hybrid anomaly detection method and compare results with other common algorithms/methods\n\n![Example of anomaly in telemetry](anomaly.png){width=\"636\"}\n\n# Data Setup & Preprocessing\n\nUnfold below code to see setup. Basics are generating 28 features\n\nsetting up training data set and validation data set\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n       \n       \n#exogenous_features=['High_mean_lag3','month', 'week', 'day','day_of_week']\n\n```\n:::\n\n\n# ARIMA Model\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = auto_arima(\n\tdf_train[\"High\"],\n\texogenous=df_train[exogenous_features],\n\ttrace=True,\n\terror_action=\"ignore\",\n\tsuppress_warnings=True,\n    seasonal=True,\n    m=1)\nmodel.fit(df_train.High, exogenous=df_train[exogenous_features])\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid.insert(len(df_valid.columns),\"Forecast_ARIMAX\",forecast,True)\n\nprint(\"\\nRMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.High, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.High, df_valid.Forecast_ARIMAX))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPerforming stepwise search to minimize aic\n ARIMA(2,0,2)(0,0,0)[0] intercept   : AIC=2960.826, Time=14.92 sec\n ARIMA(0,0,0)(0,0,0)[0] intercept   : AIC=5711.667, Time=1.33 sec\n ARIMA(1,0,0)(0,0,0)[0] intercept   : AIC=3359.247, Time=10.78 sec\n ARIMA(0,0,1)(0,0,0)[0] intercept   : AIC=3726.632, Time=12.81 sec\n ARIMA(0,0,0)(0,0,0)[0]             : AIC=5709.707, Time=13.51 sec\n ARIMA(1,0,2)(0,0,0)[0] intercept   : AIC=2904.191, Time=15.36 sec\n ARIMA(0,0,2)(0,0,0)[0] intercept   : AIC=2912.739, Time=14.14 sec\n ARIMA(1,0,1)(0,0,0)[0] intercept   : AIC=3067.819, Time=13.69 sec\n ARIMA(1,0,3)(0,0,0)[0] intercept   : AIC=2930.144, Time=15.71 sec\n ARIMA(0,0,3)(0,0,0)[0] intercept   : AIC=2936.070, Time=15.05 sec\n ARIMA(2,0,1)(0,0,0)[0] intercept   : AIC=2957.813, Time=13.08 sec\n ARIMA(2,0,3)(0,0,0)[0] intercept   : AIC=2898.815, Time=16.29 sec\n ARIMA(3,0,3)(0,0,0)[0] intercept   : AIC=2805.639, Time=17.38 sec\n ARIMA(3,0,2)(0,0,0)[0] intercept   : AIC=2962.492, Time=15.72 sec\n ARIMA(4,0,3)(0,0,0)[0] intercept   : AIC=2810.038, Time=17.40 sec\n ARIMA(3,0,4)(0,0,0)[0] intercept   : AIC=2785.353, Time=21.18 sec\n ARIMA(2,0,4)(0,0,0)[0] intercept   : AIC=2897.289, Time=18.48 sec\n ARIMA(4,0,4)(0,0,0)[0] intercept   : AIC=2807.535, Time=19.43 sec\n ARIMA(3,0,5)(0,0,0)[0] intercept   : AIC=2789.723, Time=20.86 sec\n ARIMA(2,0,5)(0,0,0)[0] intercept   : AIC=2846.091, Time=20.74 sec\n ARIMA(4,0,5)(0,0,0)[0] intercept   : AIC=2791.733, Time=22.14 sec\n ARIMA(3,0,4)(0,0,0)[0]             : AIC=2809.734, Time=18.05 sec\n\nBest model:  ARIMA(3,0,4)(0,0,0)[0] intercept\nTotal fit time: 348.057 seconds\n\nRMSE of Auto ARIMAX: 0.6141699692631081\n\nMAE of Auto ARIMAX: 0.2635940364491945\n```\n:::\n:::\n\n\n@fig-arima\n\n::: {.cell fig-width='30%' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Initial ARIMA forecast on reaction wheel temperature data](index_files/figure-html/fig-arima-output-1.png){#fig-arima width=1087 height=592}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(10,6))\nplt.plot(df_train[\"High\"])\n#plt.plot(df_valid[\"High\"])\nplt.plot(df_valid[\"Forecast_ARIMAX\"])\nplt.legend(['Training Data','Test Data'],loc='best')\nplt.ylabel('Temperature (C)')\n#plt.savefig('model.png',dpi=1200)\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,6))\nplt.plot(df_valid[\"High\"])\n#plt.plot(df_valid[\"High\"])\nplt.plot(df_valid[\"Forecast_ARIMAX\"])\nplt.legend(['Truth','ARIMA Model'],loc='lower left')\nplt.ylabel('Temperature (C)')\n#plt.savefig('model2.png',dpi=1200)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=808 height=485}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=808 height=485}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n############ Truth ################\n\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(10,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=808 height=505}\n:::\n:::\n\n\n# OCSVM\n\nOne class support vector machine algorithm\n\n![OCSVM](https://www.researchgate.net/publication/362912442/figure/fig4/AS:11431281080912015@1661430301368/Schematic-of-the-OCSVM_W640.jpg){width=\"408\"}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n############# OCSVM ##################\n\nfig, ax = plt.subplots(figsize=(10,6))\ndata2=df_valid[\"Forecast_ARIMAX\"]\nmodel =svm.OneClassSVM(nu=0.05,kernel='poly')\nmodel.fit(data2.values.reshape(-1,1))\nanom=(pd.Series(model.predict(data2.values.reshape(-1,1))))\ndf2=pd.DataFrame()\ndf2['Time']=data2.index\ndf2['data']=data2.values\ndf2['anom']=anom\na=df2.loc[df2['anom']==-1,['Time','data']]\ndf2.set_index(\"Time\", drop=True, inplace=True)\nax.plot(df2.index, df2['data'], color='black', label = 'ARIMA')\nax.scatter(a['Time'].values,a['data'], color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Anomalies detected with OCSVM')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=808 height=505}\n:::\n:::\n\n\n# Isolation Forest\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n######### Isolation Forest ################\nimport sklearn\nfrom sklearn.ensemble import IsolationForest\ncatfish_sales=df_valid\noutliers_fraction = float(.01)\nscaler = sklearn.preprocessing.StandardScaler()\nnp_scaled = scaler.fit_transform(catfish_sales['Forecast_ARIMAX'].values.reshape(-1, 1))\ndata = pd.DataFrame(np_scaled)\n# train isolation forest\nmodel =  IsolationForest(contamination=outliers_fraction)\nmodel.fit(data)\n\n\n\ncatfish_sales['anomaly'] = model.predict(data)\n# visualization\nfig, ax = plt.subplots(figsize=(10,6))\na = catfish_sales.loc[catfish_sales['anomaly'] == -1, ['Forecast_ARIMAX']] #anomaly\nax.plot(catfish_sales.index, catfish_sales['Forecast_ARIMAX'], color='black', label = 'Normal')\nax.scatter(a.index,a['Forecast_ARIMAX'], color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Anomalies detected with Isolation Forest')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=808 height=505}\n:::\n:::\n\n\n# Final Results\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n######################### calculate statistics ###########################\nfrom sklearn.metrics import f1_score,recall_score,precision_score\nfrom sklearn.metrics import mean_squared_error\nfrom tabulate import tabulate\nfrom collections import OrderedDict\n\n\n\ndef perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)\n\n\n\n\ndf_truth=pd.read_csv('./truth.csv')\nanom=anom.map(lambda val:1 if val==-1 else 0)\n#calculate F1 score\nf1=f1_score(df_truth['Anom'].values, anom.values)\nrec=recall_score(df_truth['Anom'].values, anom.values)\nprec=precision_score(df_truth['Anom'].values, anom.values)\nTP, FP, TN, FN=perf_measure(df_truth['Anom'].values, anom.values)\nfpr=FP/(TN+FP)\nfinal=OrderedDict()\nfinal['OCSVM']=[f1,rec,prec,fpr]\n\n\na2=catfish_sales['anomaly']\na2=a2.map(lambda val:1 if val==-1 else 0)\nf1=f1_score(df_truth['Anom'].values, a2.values)\nrec=recall_score(df_truth['Anom'].values, a2.values)\nprec=precision_score(df_truth['Anom'].values, a2.values)\nTP, FP, TN, FN=perf_measure(df_truth['Anom'].values, a2.values)\nfpr=FP/(TN+FP)\nfinal['IsoFor']=[f1,rec,prec,fpr]\ndf=pd.DataFrame(final)\ndf.index=['F1','Recall','Precision','FPR']\nprint(tabulate(df, headers='keys', tablefmt='psql'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----------+-------------+----------+\n|           |       OCSVM |   IsoFor |\n|-----------+-------------+----------|\n| F1        | 0.718232    | 0.217054 |\n| Recall    | 0.565217    | 0.121739 |\n| Precision | 0.984848    | 1        |\n| FPR       | 0.000818331 | 0        |\n+-----------+-------------+----------+\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}