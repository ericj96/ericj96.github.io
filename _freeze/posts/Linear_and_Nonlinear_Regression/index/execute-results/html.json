{
  "hash": "1c54e3928296e0d7c83552047ec24faf",
  "result": {
    "markdown": "---\ntitle: \"Linear and Nonlinear Regression\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-03\"\ncategories: [linear,nonlinear,regression]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nRegression analysis is a broad and useful field of statistical analysis, and for the purpose of this blog, only linear and nonlinear regression will be discuessed. Both linear and nonlinear regression have many practical uses and are used to estimate the relationships between two or more variables, using either lines or curves. The main use of linear or nonlinear regression are for predicting or forecasting data and thus, is used widely in machine learning situations. Regression analysis is also used to infer casual relationships between variables, otherwise called correlation. \\[[1](https://en.wikipedia.org/wiki/Linear_regression)\\]\n\n# Regression Techniques\n\n## Linear Regression\n\nLinear regression is a technique that uses a straight line to model the relationship between two or more variables. There are several models that can be used for linear regression, with the most common one being the least squares method. Linear regression can also be performed by minimizing the lack of fit or by minimizing a version of the least squares cost function (ridge regression and lasso) \\[[1](https://en.wikipedia.org/wiki/Linear_regression)\\]. The LinearRegression model from the sklearn library utilizes the least squares method to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation \\[[2](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\\].\n\nA dataset native to the sklearn library was used for this blog post (load_wine) and two features were chosen at random based on how linear the trend appeared. As can be seen in @fig-lin, the data appears random with a slight positive trend. The linear regression model was overlayed on top of the original data with a red line, and can be seen to match the positive slope of the data very closely. It also sits roughly in the middle of the data throughout, and leads to an RMSE of 2.403.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nimport math\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\ny=np.array(y.values).reshape(-1,1)/100\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(x,y)\n\ntest_predictions = model.predict(x)\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nMAE = mean_absolute_error(y,test_predictions)\nMSE = mean_squared_error(y,test_predictions)\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: 2.403\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y)\nplt.plot(x,test_predictions,color=\"red\")\nplt.legend(['Data','Linear Prediction'])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Linear regression model on data](index_files/figure-html/fig-lin-output-1.png){#fig-lin width=671 height=503}\n:::\n:::\n\n\n## Nonlinear Regression\n\nNonlinear regression is similar to linear regression, but uses a curve to fit the data, compared to a straight line with linear regression. Most problems or data in the real world are not linear in nature, so nonlinear regression allows for a much greater accuracy when analyzing items.\n\nThe dataset used for the above linear regression section was used and slightly manipulated to create a more nonlinear quadratic shape. This was done to more easily show the difference between linear and nonlinear regression.\n\nAfter fitting lines of best fit for varying degrees of polynomials (see @fig-poly), it can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below (@fig-table) shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Imports and dataset setup\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\nxp=x\ny=np.array(y.values).reshape(-1,1)/100\nyp=y\n\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\npoly_features = polynomial_converter.fit_transform(y)\n\nx=poly_features[:,0].reshape(-1,1)\ny=poly_features[:,1].reshape(-1,1)*y/100\n\nx[:,0].sort()\ny[:,0].sort()\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Linear regression\npolybig_features = PolynomialFeatures(degree=1, include_bias=False)\nstd_scaler = StandardScaler()\nlin_reg = LinearRegression()\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin = polynomial_regression.predict(X_newlin)\n\n# Polynomial regression with degree = 2\npolybig_features = PolynomialFeatures(degree=2, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_new = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbig = polynomial_regression.predict(X_new)\n\n# Polynomial regression with degree = 3\npolybig_features = PolynomialFeatures(degree=3, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin3 = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin3 = polynomial_regression.predict(X_new)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting code\"}\nfrom pandas.plotting import table\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y,marker='o')\nplt.plot(X_newlin, y_newbiglin,color='green')\nplt.plot(X_new, y_newbig,color='red')\nplt.plot(X_newlin3, y_newbiglin3,color='black')\nplt.legend(['Data','Linear fit','Polynomial fit (deg = 2)','Polynomial fit (deg = 3)'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Polynomial/Nonlinear regression model on data](index_files/figure-html/fig-poly-output-1.png){#fig-poly width=641 height=490}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table code\"}\nlinscore=polynomial_regression.score(X_newlin,y_newbiglin)\nlinscore2=polynomial_regression.score(X_new,y_newbig)\nlinscore3=polynomial_regression.score(X_newlin3,y_newbiglin3)\n\ndf=pd.DataFrame([[linscore,linscore2,linscore3]],columns=['Linear Regression','Polynomial Regression (deg = 2)','Polynomial Regression (deg = 3)'],index=['R squared value','R^21','R^22'])\nfix, ax = plt.subplots(figsize=(8,1))\nax.axis('off')\ntable(ax,df.transpose()['R squared value'],loc='top',cellLoc='center',colWidths=list([.6, .6]))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![R squared values for each regression model](index_files/figure-html/fig-table-output-1.png){#fig-table width=757 height=157}\n:::\n:::\n\n\n# Conclusion\n\nAs can be seen from the results of this blog post, both linear and nonlinear regression are useful tools when analyzing datasets. Each type of regression serves its own purpose and should be chosen based on the data being monitored/analyzed. For nonlinear regression, it is important to not overfit the model and apply too high of a degree to the polynomial, as this can lead to decreased accuracy.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}