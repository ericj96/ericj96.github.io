{
  "hash": "d0c8e873c8edb59ce92d6f959282016a",
  "result": {
    "markdown": "---\ntitle: \"Linear and Nonlinear Regression\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-03\"\ncategories: [linear,nonlinear,regression]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\n# Regression Techniques\n\n## Linear Regression\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nimport math\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\ny=np.array(y.values).reshape(-1,1)/100\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(x,y)\n\ntest_predictions = model.predict(x)\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nMAE = mean_absolute_error(y,test_predictions)\nMSE = mean_squared_error(y,test_predictions)\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y)\nplt.plot(x,test_predictions,color=\"red\")\nplt.legend(['Data','Linear Prediction'])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: 2.403\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=671 height=503}\n:::\n:::\n\n\n## Nonlinear Regression\n\nIt can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\nxp=x\ny=np.array(y.values).reshape(-1,1)/100\nyp=y\n\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\npoly_features = polynomial_converter.fit_transform(y)\n\nx=poly_features[:,0].reshape(-1,1)\ny=poly_features[:,1].reshape(-1,1)*y/100\n\nx[:,0].sort()\ny[:,0].sort()\n\n# Linear regression\npolybig_features = PolynomialFeatures(degree=1, include_bias=False)\nstd_scaler = StandardScaler()\nlin_reg = LinearRegression()\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin = polynomial_regression.predict(X_newlin)\n\nnum=-1\nMAE = mean_absolute_error(y[:num],y_newbiglin[:num])\nMSE = mean_squared_error(y[:num],y_newbiglin[:num])\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n\n# Polynomial regression with degree = 2\npolybig_features = PolynomialFeatures(degree=2, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_new = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbig = polynomial_regression.predict(X_new)\n\nMAE = mean_absolute_error(y[:num],y_newbig[:num])\nMSE = mean_squared_error(y[:num],y_newbig[:num])\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n\n# Polynomial regression with degree = 3\npolybig_features = PolynomialFeatures(degree=3, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin3 = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin3 = polynomial_regression.predict(X_new)\n\nMAE = mean_absolute_error(y[:num],y_newbiglin3[:num])\nMSE = mean_squared_error(y[:num],y_newbiglin3[:num])\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y,marker='o')\nplt.plot(X_newlin, y_newbiglin,color='green')\nplt.plot(X_new, y_newbig,color='red')\nplt.plot(X_newlin3, y_newbiglin3,color='black')\nplt.legend(['Data','Linear fit','Polynomial fit (deg = 2)','Polynomial fit (deg = 3)'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSE: 8.118\nRMSE: 9.905\nRMSE: 10.098\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=641 height=490}\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nfrom pandas.plotting import table\n\nlinscore=polynomial_regression.score(X_newlin,y_newbiglin)\nlinscore2=polynomial_regression.score(X_new,y_newbig)\nlinscore3=polynomial_regression.score(X_newlin3,y_newbiglin3)\n\ndf=pd.DataFrame([[linscore,linscore2,linscore3]],columns=['Linear Regression','Polynomial Regression (deg = 2)','Polynomial Regression (deg = 3)'],index=['R squared','R^21','R^22'])\nfix, ax = plt.subplots()\nax.axis('off')\ntable(ax,df.transpose()['R squared'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=801 height=453}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}