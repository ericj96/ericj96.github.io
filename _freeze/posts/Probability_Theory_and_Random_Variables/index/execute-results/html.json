{
  "hash": "7fa4ae9f6c299f4d7b7d69653c2567b3",
  "result": {
    "markdown": "---\ntitle: \"Probability theory and random variables\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-02\"\ncategories: [probability,random variables]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Probability Theory & Random Variables\n\nMathematics has many broad topics, but one of the most prevalent topics in machine learning is probability. Probability theory contains topics such as discrete and continuous random variables, probability distributions, and statistics.\n\nOne of the more realistic machine learning based scenarios to utilize probability theory methods on is random data. As seen in @fig-og, a set of 5 \"blobs\" has been generated with random centers and varying degrees of standard deviations from said center. This was the same approach taken for my [blog post on Clustering](https://ericj96.github.io/posts/Clustering/).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Setup code\"}\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nimport scipy.stats\nfrom scipy.stats import norm\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nblob_centers=np.random.uniform(0,5,[5,2])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nx, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,random_state=7)\nn_bins=75\ndata=x[:,1]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting original data code\"}\nfig, axs = plt.subplots(figsize =(9, 6))\nplt.scatter(x[:,0],x[:,1])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Plot of random data contained to five blobs](index_files/figure-html/fig-og-output-1.png){#fig-og width=725 height=503}\n:::\n:::\n\n\nWhen working with non-normal or non-uniform data, it is hard to fit a normal distribution curve to it. Utilizing a kernel density estimation (KDE) is one way to smooth as well as estimate the probability density function (PDF) of a random variable based on kernels as weights. The kernel density estimator is seen in the below equation:\n\n![](Capture.JPG){width=\"376\"}\n\nAs seen in @fig-hist, the plot of the kde function on the above data closely follows the trend of the histogram for both x1 and x2 variables, which were generated randomly.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting kde and histogram code\"}\nfig, axs = plt.subplots(2,1,figsize =(9, 7))\nax1=plt.subplot(211)\nax1.hist(data, n_bins, alpha=0.5,density=1,label='x1 data',edgecolor='black');\nax1.plot(t_range,kde(t_range),lw=2, label='x1 kde')\nplt.xlim(x.min()-.5,x.max()+.5)\nax1.legend(loc='best')\nax2=plt.subplot(212)\ndata=x[:,0]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\nax2.hist(data, n_bins, alpha=0.5,density=1,label='x2 data',edgecolor='black');\nax2.plot(t_range,kde(t_range),lw=2, label='x2 kde')\n\nax2.legend(loc='best')\nplt.xlim(x.min()-.5,x.max()+.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Histogram and KDE of data](index_files/figure-html/fig-hist-output-1.png){#fig-hist width=728 height=559}\n:::\n:::\n\n\nMachine learning techniques commonly will use clustering to group data points together and allows the user to see the similarity of their data. One algorithm that is used for clustering is Gaussian Mixtures Model (GMM) which uses probability for clustering and density estimation, and is based on Gaussian distribution curves. Since this blog post is about probability theory, we will utilize this specific method.\n\nSince Gaussian distributions heavily depend on mean and variance of each point, GMM utilizes a statistical algorithm called Expectation-Maximization for calculating the mean and variance value of each Gaussian or cluster. The algorithm first calculates the probability that a point belongs to each cluster, then iterates the mean and covariance matrix to maximize the log likelihood value.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\" code-summary=\"Gaussian Mixture setup and plot code\"}\ngm = GaussianMixture(n_components=5, n_init=10, random_state=42)\ngm.fit(x)\nprint('Gaussian Mixture model converged in %d iterations with a lower bound\\non the log likelihood of the best fit of EM of %3.3f' % (gm.n_iter_,gm.lower_bound_))\nlabels=gm.predict(x)\n\nfig, axs = plt.subplots(figsize =(9, 6))\nplt.scatter(x[:,0][labels==0],x[:,1][labels==0])\nplt.scatter(x[:,0][labels==1],x[:,1][labels==1])\nplt.scatter(x[:,0][labels==2],x[:,1][labels==2])\nplt.scatter(x[:,0][labels==3],x[:,1][labels==3])\nplt.scatter(x[:,0][labels==4],x[:,1][labels==4])\nplt.legend(['Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5'],loc='best')\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGaussian Mixture model converged in 4 iterations with a lower bound\non the log likelihood of the best fit of EM of -0.804\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Gaussian Mixture clustering results](index_files/figure-html/fig-clust-output-2.png){#fig-clust width=725 height=503}\n:::\n:::\n\n\nThe Gaussian Mixture function from sklearn allows one to see the probabilities that a certain point is in each of the 5 clusters (in this example). Below, you can see a table with the probabilities of several datapoints and their respective clusters. It's interesting to note that not every point is 100% certain with this algorithm, there are several points that have \\< 1.0 probabilities, meaning that it might have a 95% probability of it being in one cluster, and a 5% probability of being in another cluster. Since 95% \\> 5%, it assumes it is in the higher probability cluster.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\npd.DataFrame(gm.predict_proba(x).round(3),columns=['Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5'])\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cluster 1</th>\n      <th>Cluster 2</th>\n      <th>Cluster 3</th>\n      <th>Cluster 4</th>\n      <th>Cluster 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.001</td>\n      <td>0.0</td>\n      <td>0.999</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>0.0</td>\n      <td>0.006</td>\n      <td>0.0</td>\n      <td>0.994</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>1.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>0.0</td>\n      <td>1.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows Ã— 5 columns</p>\n</div>\n```\n:::\n:::\n\n\nSources used for code and/or text:\n\n\\[1\\] <https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/#:~:text=A.-,The%20Gaussian%20Mixture%20Model%20(GMM)%20is%20a%20probabilistic%20model%20used,distributions%2C%20each%20representing%20a%20cluster.>\n\n\\[2\\] <https://github.com/maptv/handson-ml3/blob/b8f4fd1e85247096109b175d3289b558cedc74b4/09_unsupervised_learning.ipynb>\n\n\\[3\\] <https://en.wikipedia.org/wiki/Probability_theory>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}