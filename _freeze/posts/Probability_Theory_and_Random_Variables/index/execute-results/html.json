{
  "hash": "58a7a18d1cb157d6f3e7cf5c913fc7be",
  "result": {
    "markdown": "---\ntitle: \"Probability theory and random variables\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-02\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nhistogram of pdf\n\n## Probability Theory\n\nOne of the more realistic machine learning based scenarios to utilize probability theory methods on is random data. As seen in @fig-og, a set of 5 \"blobs\" are generated with random centers and varying degrees of standard deviations from said center. This was the same approach taken for my [blog post on Clustering](https://ericj96.github.io/posts/Clustering/).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Setup code\"}\nimport numpy as np\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nimport scipy.stats\nfrom scipy.stats import norm\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nblob_centers=np.random.uniform(0,5,[5,2])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nx, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,random_state=7)\nn_bins=75\ndata=x[:,1]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting original data code\"}\nfig, axs = plt.subplots(figsize =(9, 6))\nplt.scatter(x[:,0],x[:,1])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Plot of random data contained to five blobs](index_files/figure-html/fig-og-output-1.png){#fig-og width=725 height=503}\n:::\n:::\n\n\nWhen working with non-normal or non-uniform data, it is hard to fit a normal distribution curve to it. Utilizing a kernel density estimation (KDE) is one way to smooth as well as estimate the probability density function (PDF) of a random variable based on kernels as weights. The kernel density estimator is seen in the below equation:\n\n![](Capture.JPG){width=\"376\"}\n\nAs seen in @fig-hist, the plot of the kde function on the data closely follows the trend of the histogram for both x1 and x2 variables, which were generated randomly.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plotting kde and histogram code\"}\nfig, axs = plt.subplots(2,1,figsize =(10, 7))\nax1=plt.subplot(211)\nax1.hist(data, n_bins, alpha=0.5,density=1,label='x1 data',edgecolor='black');\nax1.plot(t_range,kde(t_range),lw=2, label='x1 kde')\nplt.xlim(x.min()-.5,x.max()+.5)\nax1.legend(loc='best')\nax2=plt.subplot(212)\ndata=x[:,0]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\nax2.hist(data, n_bins, alpha=0.5,density=1,label='x2 data',edgecolor='black');\nax2.plot(t_range,kde(t_range),lw=2, label='x2 kde')\n\nax2.legend(loc='best')\nplt.xlim(x.min()-.5,x.max()+.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Histogram and KDE of data](index_files/figure-html/fig-hist-output-1.png){#fig-hist width=802 height=559}\n:::\n:::\n\n\n## Random Variables\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}