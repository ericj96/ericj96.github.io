{
  "hash": "50b44d87c2e9d402b56163999581263b",
  "result": {
    "markdown": "---\ntitle: \"Hyperparameter Tuning\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-06\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\n\nExamples of common hyperparameters include:\n\n-   Learning rate and network size in Long short-term memory (LSTM)\n\n-   k in k-nearest neighbors\n\n-   The penalty in most classifier methods\n\n# Tuning Techniques\n\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically.\n\n## Baseline Data\n\nFor the purposes of this blog, a basic Support Vector Machine (SVM) classification model will be trained with a built in dataset from the sklearn library. This will give a baseline accuracy and results to compare when attempting to optimize the hyperparameters. The hyperparameters that are used by default with this algorithm are:\n\n-   *C* - Regularization parameter. The penalty is a squared l2 penalty \\[default = 1.0\\]\n\n-   *Kernel* - Kernel type used in algorithm \\[default = 'rbf'\\]\n\n    -   Note: Linear and RBF kernels were used as the possible options in this example\n\n-   *Gamma* - Kernel coefficient \\[default = 'scale', or 1 / (n_features \\* X)\\]\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\n# Setting up basic SVM to compare with optimized hyperparameters. Using built in dataset of data\nimport pandas as pd \nimport numpy as np \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split \n\ncancer = load_breast_cancer() \ncancer=load_digits()\ndf_feat = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndf_target = pd.DataFrame(cancer['target'], columns =['Cancer']) \nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target),test_size = 0.30, random_state = 101) \nmodel = SVC() \nmodel.fit(X_train, y_train) \n\n# print prediction results \npredictions = model.predict(X_test) \nprint(classification_report(y_test, predictions)) \nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,predictions))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.98      0.97      0.98        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       0.98      0.97      0.97        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nAccuracy = 0.988889\n\n```\n:::\n:::\n\n\nIt is seen that the baseline model performs with an accuracy of 98.89% using C = 1, kernel = rbf, gamma = scale (or around 0.015). This will be compared against with several methods of hyperparameter optimization but is already extremely high for accuracy.\n\n## Grid Search\n\nThe grid search method of optimizing hyperparameters works by running through each combination of parameters and choosing the combination with the highest score at the end.\n\nThe drawback of using grid search is that it is very time intensive due to the large number of combinations that it iterates through\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n\t\t\t'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n\t\t\t'kernel': ['rbf','linear']} \n\ngrid = GridSearchCV(SVC(), param_grid,refit = True, verbose = 1) \ngrid.fit(X_train, y_train) \ngrid_predictions = grid.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, grid_predictions)) \nprint('%s' % (grid.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,grid_predictions))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nAccuracy = 0.992593\n\n```\n:::\n:::\n\n\nAs seen in the table above, an accuracy of 99.259% was achieved with the hyperparameter values of c=1, gamma = 0.001, and kernel = rbf. This is a slight improvement from the standard parameters, around 0.5% increase in accuracy. As seen in the results section and in @fig-time, the shape of the grid search can be seen in the time it takes for each iteration, as it follows the same pattern for every set of combinations.\n\n## Random Search\n\nRandom search works similarly to grid search, but moves through it in a random fashion and only uses a fixed number of combinations. This allows for similar optimization results as the grid search in a fraction of the time. As seen in the results section and in @fig-time, the random search is \\~6 times faster.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomizedSearchCV(estimator=SVC(),param_distributions=param_grid,verbose=1)\nclf.fit(X_train, y_train) \nclf_predictions = clf.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, clf_predictions)) \nprint('%s' % (clf.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,clf_predictions))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.95      0.96      0.95        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.95      0.97      0.96        61\n           5       0.98      0.98      0.98        59\n           6       1.00      0.96      0.98        46\n           7       1.00      0.96      0.98        56\n           8       0.93      0.97      0.95        59\n           9       0.98      0.98      0.98        48\n\n    accuracy                           0.98       540\n   macro avg       0.98      0.98      0.98       540\nweighted avg       0.98      0.98      0.98       540\n\n{'kernel': 'linear', 'gamma': 0.01, 'C': 1000}\nAccuracy = 0.977778\n\n```\n:::\n:::\n\n\nBecause of the randomness of the random search, the optimal value(s) change every time this method is ran. For the iteration performed at the time of this blog post, the random search method determined optimal values worse than both the grid and Bayesian search methods.\n\n## Bayesian Optimization\n\nBayesian optimization works differently than the other two commonly used methods mentioned above, it uses Bayes Theorem to find the minimum or maximum of an objective function. Because of this difference, not all parameter values are used and a fixed number of hyperparameter combinations are iterated through (default number of iterations is 50). Bayesian optimization is usually used to optimize expensive to evaluate functions, which is not necessarily the case for this example as the data being used is somewhat basic for the purposes of this blog post.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nfrom skopt import BayesSearchCV\n\nbayes = BayesSearchCV(SVC(), param_grid) \nbayes.fit(X_train, y_train) \nbayes_predictions = bayes.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, bayes_predictions)) \nprint('%s' % (bayes.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,bayes_predictions))\nac3=accuracy_score(y_test,bayes_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nOrderedDict([('C', 1.0), ('gamma', 0.001), ('kernel', 'rbf')])\nAccuracy = 0.992593\n\n```\n:::\n:::\n\n\nIt is seen that utilizing a Bayesian search optimization identifies the same optimal values as the grid search, c = 1, gamma = 0.001, and kernel = rbf, with an accuracy of 99.259%. This is once again slightly higher than the baseline method by around 0.5%\n\n# Results\n\nAfter optimizing the hyperparameters for this dataset of 8x8 pixel data, both the grid search and Bayesian search identified the same optimal hyperparameters. But, as seen below, the Bayesian search performed this \\~30% faster. Since the random search method only sometimes performed better than the baseline method in terms of accuracy, this makes the Bayesian search method the optimal choice between these three methods for its accuracy and speed.\n\n::: {.cell fig-width='2' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nimport matplotlib.pyplot as plt\nfrom skopt.plots import plot_convergence\n\nres=pd.DataFrame(bayes.cv_results_)\nres2=pd.DataFrame(grid.cv_results_)\nres3=pd.DataFrame(clf.cv_results_)\nprint('Bayes time: %3.3f sec\\nGrid time: %3.3f sec\\nRandom time: %3.3f sec' % (res.mean_fit_time.sum(),res2.mean_fit_time.sum(),res3.mean_fit_time.sum()))\nplt.figure(figsize=(9,6))\nres.mean_fit_time.plot(figsize=(9,6))\nres2.mean_fit_time.plot(figsize=(9,6))\nres3.mean_fit_time.plot(figsize=(9,6))\nplt.legend(['Bayes','Grid','Random'])\nplt.title('Time taken per iteration')\nplt.ylabel('Time (sec)')\nplt.xlabel('Iteration #')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBayes time: 1.602 sec\nGrid time: 2.496 sec\nRandom time: 0.269 sec\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![Time taken per iteration for each of the 3 optimization methods](index_files/figure-html/fig-time-output-2.png){#fig-time width=746 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}