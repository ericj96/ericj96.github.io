{
  "hash": "b15068c2591df8a8064116e2e08b447c",
  "result": {
    "markdown": "---\ntitle: \"Hyperparameter Tuning\"\nauthor: \"Eric Jackson\"\ndate: \"2023-10-06\"\ncategories: [code, analysis]\nimage: \"image.jpg\"\ntoc: true\ntoc-depth: 2\n---\n\n# Background\n\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\n\nExamples of common hyperparameters include:\n\n-   Learning rate and network size in Long short-term memory (LSTM)\n\n-   k in k-nearest neighbors\n\n-   The penalty in most classifier methods\n\n# Tuning Techniques\n\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\n# Setting up basic SVM to compare with optimized hyperparameters\nimport pandas as pd \nimport numpy as np \nfrom sklearn.metrics import classification_report, confusion_matrix \nfrom sklearn.datasets import load_breast_cancer \nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split \n\ncancer = load_breast_cancer() \n\ndf_feat = pd.DataFrame(cancer['data'], \n\t\t\t\t\tcolumns = cancer['feature_names']) \n\ndf_target = pd.DataFrame(cancer['target'], columns =['Cancer']) \n\t\t\t\t\t\nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target),test_size = 0.30, random_state = 101) \n\nmodel = SVC() \nmodel.fit(X_train, y_train) \n\n# print prediction results \npredictions = model.predict(X_test) \nprint(classification_report(y_test, predictions)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.95      0.85      0.90        66\n           1       0.91      0.97      0.94       105\n\n    accuracy                           0.92       171\n   macro avg       0.93      0.91      0.92       171\nweighted avg       0.93      0.92      0.92       171\n\n```\n:::\n:::\n\n\n## Grid Search\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000,10000], \n\t\t\t'gamma': [1, 0.1, 0.01, 0.001, 0.0001,0.00001], \n\t\t\t'kernel': ['rbf']} \n\ngrid = GridSearchCV(SVC(), param_grid,refit = True, verbose = 1) \n\n# fitting the model for grid search \ngrid.fit(X_train, y_train) \n\ngrid_predictions = grid.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, grid_predictions)) \nprint('%f\\n%s' % (grid.best_score_,grid.best_params_))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 36 candidates, totalling 180 fits\n              precision    recall  f1-score   support\n\n           0       0.94      0.89      0.91        66\n           1       0.94      0.96      0.95       105\n\n    accuracy                           0.94       171\n   macro avg       0.94      0.93      0.93       171\nweighted avg       0.94      0.94      0.94       171\n\n0.959747\n{'C': 100, 'gamma': 1e-05, 'kernel': 'rbf'}\n```\n:::\n:::\n\n\n## Random Search\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import RandomizedSearchCV\nclf = RandomizedSearchCV(estimator=SVC(),\n                   param_distributions=param_grid,\n                   verbose=1)\n                   \nclf.fit(X_train, y_train) \nclf_predictions = clf.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, clf_predictions)) \nprint('%f\\n%s' % (clf.best_score_,clf.best_params_))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n              precision    recall  f1-score   support\n\n           0       0.95      0.91      0.93        66\n           1       0.94      0.97      0.96       105\n\n    accuracy                           0.95       171\n   macro avg       0.95      0.94      0.94       171\nweighted avg       0.95      0.95      0.95       171\n\n0.952278\n{'kernel': 'rbf', 'gamma': 1e-05, 'C': 1000}\n```\n:::\n:::\n\n\n## Bayesian Optimization\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nfrom pandas import read_csv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom skopt import BayesSearchCV\n\nbayes = BayesSearchCV(SVC(), param_grid) \n\n# fitting the model for grid search \nbayes.fit(X_train, y_train) \n\nbayes_predictions = bayes.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, bayes_predictions)) \nprint('%f\\n%s' % (bayes.best_score_,bayes.best_params_))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.94      0.89      0.91        66\n           1       0.94      0.96      0.95       105\n\n    accuracy                           0.94       171\n   macro avg       0.94      0.93      0.93       171\nweighted avg       0.94      0.94      0.94       171\n\n0.959747\nOrderedDict([('C', 100.0), ('gamma', 1e-05), ('kernel', 'rbf')])\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom skopt.plots import plot_convergence\nres=pd.DataFrame(bayes.cv_results_)\nres2=pd.DataFrame(grid.cv_results_)\nres3=pd.DataFrame(clf.cv_results_)\nres.mean_test_score.plot(figsize=(10,10))\nres2.mean_test_score.plot(figsize=(10,10))\nres3.mean_test_score.plot(figsize=(10,10))\nplt.legend(['Bayes','Grid','Random'])\nplt.show()\nplt.clf()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=802 height=781}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 672x480 with 0 Axes>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}