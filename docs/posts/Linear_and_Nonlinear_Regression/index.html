<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Eric Jackson">
<meta name="dcterms.date" content="2023-10-03">

<title>Fall 2023 CS 5805 Blog - Linear and Nonlinear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Fall 2023 CS 5805 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About: Eric Jackson</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ericj96" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Linear and Nonlinear Regression</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">linear</div>
                <div class="quarto-category">nonlinear</div>
                <div class="quarto-category">regression</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Eric Jackson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 3, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#regression-techniques" id="toc-regression-techniques" class="nav-link" data-scroll-target="#regression-techniques">Regression Techniques</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#nonlinear-regression" id="toc-nonlinear-regression" class="nav-link" data-scroll-target="#nonlinear-regression">Nonlinear Regression</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ericj96/ericj96.github.io/blob/main/posts/Linear_and_Nonlinear_Regression/index.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="background" class="level1">
<h1>Background</h1>
<p>Regression analysis is a broad and useful field of statistical analysis, and for the purpose of this blog, only linear and nonlinear regression will be discuessed. Both linear and nonlinear regression have many practical uses and are used to estimate the relationships between two or more variables, using either lines or curves. The main use of linear or nonlinear regression are for predicting or forecasting data and thus, is used widely in machine learning situations. Regression analysis is also used to infer casual relationships between variables, otherwise called correlation. [<a href="https://en.wikipedia.org/wiki/Linear_regression">1</a>]</p>
</section>
<section id="regression-techniques" class="level1">
<h1>Regression Techniques</h1>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear Regression</h2>
<p>Linear regression is a technique that uses a straight line to model the relationship between two or more variables. There are several models that can be used for linear regression, with the most common one being the least squares method. Linear regression can also be performed by minimizing the lack of fit or by minimizing a version of the least squares cost function (ridge regression and lasso) [<a href="https://en.wikipedia.org/wiki/Linear_regression">1</a>]. The LinearRegression model from the sklearn library utilizes the least squares method to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation [<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">2</a>].</p>
<p>A dataset native to the sklearn library was used for this blog post (load_wine) and two features were chosen at random based on how linear the trend appeared. As can be seen in <a href="#fig-lin">Figure&nbsp;1</a>, the data appears random with a slight positive trend. The linear regression model was overlayed on top of the original data with a red line, and can be seen to match the positive slope of the data very closely. It also sits roughly in the middle of the data throughout, and leads to an RMSE of 2.403.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>load_wine()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame(df[<span class="st">'data'</span>],columns<span class="op">=</span>df[<span class="st">'feature_names'</span>])</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>df[[<span class="st">'alcohol'</span>,<span class="st">'proline'</span>]]</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>df.alcohol</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df.proline</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.array(x.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>np.array(y.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="dv">100</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>model.fit(x,y)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>test_predictions <span class="op">=</span> model.predict(x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error,mean_squared_error</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>MAE <span class="op">=</span> mean_absolute_error(y,test_predictions)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> mean_squared_error(y,test_predictions)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>RMSE <span class="op">=</span> np.sqrt(MSE)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'RMSE: </span><span class="sc">%3.3f</span><span class="st">'</span> <span class="op">%</span> RMSE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>RMSE: 2.403</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(x,y)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x,test_predictions,color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Data'</span>,<span class="st">'Linear Prediction'</span>])</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x_</span><span class="sc">{1}</span><span class="st">$'</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$x_</span><span class="sc">{2}</span><span class="st">$'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-lin" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-lin-output-1.png" width="671" height="503" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Linear regression model on data</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="nonlinear-regression" class="level2">
<h2 class="anchored" data-anchor-id="nonlinear-regression">Nonlinear Regression</h2>
<p>Nonlinear regression is similar to linear regression, but uses a curve to fit the data, compared to a straight line with linear regression. Most problems or data in the real world are not linear in nature, so nonlinear regression allows for a much greater accuracy when analyzing items.</p>
<p>The dataset used for the above linear regression section was used and slightly manipulated to create a more nonlinear quadratic shape. This was done to more easily show the difference between linear and nonlinear regression.</p>
<p>After fitting lines of best fit for varying degrees of polynomials (see <a href="#fig-poly">Figure&nbsp;2</a>), it can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below (<a href="#fig-table">Figure&nbsp;3</a>) shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Imports and dataset setup</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error,mean_squared_error</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>load_wine()</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame(df[<span class="st">'data'</span>],columns<span class="op">=</span>df[<span class="st">'feature_names'</span>])</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>df[[<span class="st">'alcohol'</span>,<span class="st">'proline'</span>]]</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>df.alcohol</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df.proline</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.array(x.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>xp<span class="op">=</span>x</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>np.array(y.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="dv">100</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>yp<span class="op">=</span>y</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>polynomial_converter <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>,include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>poly_features <span class="op">=</span> polynomial_converter.fit_transform(y)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>poly_features[:,<span class="dv">0</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>poly_features[:,<span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>y<span class="op">/</span><span class="dv">100</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>x[:,<span class="dv">0</span>].sort()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>y[:,<span class="dv">0</span>].sort()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear regression</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">1</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>std_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>polynomial_regression.fit(x, y)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>X_newlin <span class="op">=</span> np.linspace(<span class="bu">min</span>(x), <span class="bu">max</span>(x), <span class="bu">len</span>(x)).reshape(<span class="bu">len</span>(x), <span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>y_newbiglin <span class="op">=</span> polynomial_regression.predict(X_newlin)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial regression with degree = 2</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>polynomial_regression.fit(x, y)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="bu">min</span>(x), <span class="bu">max</span>(x), <span class="bu">len</span>(x)).reshape(<span class="bu">len</span>(x), <span class="dv">1</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>y_newbig <span class="op">=</span> polynomial_regression.predict(X_new)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial regression with degree = 3</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">3</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>polynomial_regression.fit(x, y)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>X_newlin3 <span class="op">=</span> np.linspace(<span class="bu">min</span>(x), <span class="bu">max</span>(x), <span class="bu">len</span>(x)).reshape(<span class="bu">len</span>(x), <span class="dv">1</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>y_newbiglin3 <span class="op">=</span> polynomial_regression.predict(X_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Plotting code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas.plotting <span class="im">import</span> table</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(x,y,marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X_newlin, y_newbiglin,color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_newbig,color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.plot(X_newlin3, y_newbiglin3,color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Data'</span>,<span class="st">'Linear fit'</span>,<span class="st">'Polynomial fit (deg = 2)'</span>,<span class="st">'Polynomial fit (deg = 3)'</span>])</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-poly" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-poly-output-1.png" width="641" height="490" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Polynomial/Nonlinear regression model on data</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="6">
<details>
<summary>Table code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>linscore<span class="op">=</span>polynomial_regression.score(X_newlin,y_newbiglin)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>linscore2<span class="op">=</span>polynomial_regression.score(X_new,y_newbig)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>linscore3<span class="op">=</span>polynomial_regression.score(X_newlin3,y_newbiglin3)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame([[linscore,linscore2,linscore3]],columns<span class="op">=</span>[<span class="st">'Linear Regression'</span>,<span class="st">'Polynomial Regression (deg = 2)'</span>,<span class="st">'Polynomial Regression (deg = 3)'</span>],index<span class="op">=</span>[<span class="st">'R squared value'</span>,<span class="st">'R^21'</span>,<span class="st">'R^22'</span>])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>fix, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">1</span>))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>ax.axis(<span class="st">'off'</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>table(ax,df.transpose()[<span class="st">'R squared value'</span>],loc<span class="op">=</span><span class="st">'top'</span>,cellLoc<span class="op">=</span><span class="st">'center'</span>,colWidths<span class="op">=</span><span class="bu">list</span>([<span class="fl">.6</span>, <span class="fl">.6</span>]))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-table" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-table-output-1.png" width="757" height="157" class="figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: R squared values for each regression model</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>As can be seen from the results of this blog post, both linear and nonlinear regression are useful tools when analyzing datasets. Each type of regression serves its own purpose and should be chosen based on the data being monitored/analyzed. For nonlinear regression, it is important to not overfit the model and apply too high of a degree to the polynomial, as this can lead to decreased accuracy.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Linear and Nonlinear Regression"</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Eric Jackson"</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-10-03"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [linear,nonlinear,regression]</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "image.jpg"</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="an">toc-depth:</span><span class="co"> 2</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="fu"># Background</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>Regression analysis is a broad and useful field of statistical analysis, and for the purpose of this blog, only linear and nonlinear regression will be discuessed. Both linear and nonlinear regression have many practical uses and are used to estimate the relationships between two or more variables, using either lines or curves. The main use of linear or nonlinear regression are for predicting or forecasting data and thus, is used widely in machine learning situations. Regression analysis is also used to infer casual relationships between variables, otherwise called correlation. <span class="sc">\[</span><span class="co">[</span><span class="ot">1</span><span class="co">](https://en.wikipedia.org/wiki/Linear_regression)</span><span class="sc">\]</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="fu"># Regression Techniques</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linear Regression</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>Linear regression is a technique that uses a straight line to model the relationship between two or more variables. There are several models that can be used for linear regression, with the most common one being the least squares method. Linear regression can also be performed by minimizing the lack of fit or by minimizing a version of the least squares cost function (ridge regression and lasso) <span class="sc">\[</span><span class="co">[</span><span class="ot">1</span><span class="co">](https://en.wikipedia.org/wiki/Linear_regression)</span><span class="sc">\]</span>. The LinearRegression model from the sklearn library utilizes the least squares method to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation <span class="sc">\[</span><span class="co">[</span><span class="ot">2</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)</span><span class="sc">\]</span>.</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>A dataset native to the sklearn library was used for this blog post (load_wine) and two features were chosen at random based on how linear the trend appeared. As can be seen in @fig-lin, the data appears random with a slight positive trend. The linear regression model was overlayed on top of the original data with a red line, and can be seen to match the positive slope of the data very closely. It also sits roughly in the middle of the data throughout, and leads to an RMSE of 2.403.</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>load_wine()</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame(df[<span class="st">'data'</span>],columns<span class="op">=</span>df[<span class="st">'feature_names'</span>])</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>df[[<span class="st">'alcohol'</span>,<span class="st">'proline'</span>]]</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>df.alcohol</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df.proline</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.array(x.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>np.array(y.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="dv">100</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>model.fit(x,y)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>test_predictions <span class="op">=</span> model.predict(x)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error,mean_squared_error</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>MAE <span class="op">=</span> mean_absolute_error(y,test_predictions)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> mean_squared_error(y,test_predictions)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>RMSE <span class="op">=</span> np.sqrt(MSE)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'RMSE: </span><span class="sc">%3.3f</span><span class="st">'</span> <span class="op">%</span> RMSE)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="co">#|code-fold: true</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="co">#|code-summary: Plotting code</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lin</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Linear regression model on data"</span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>plt.scatter(x,y)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>plt.plot(x,test_predictions,color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Data'</span>,<span class="st">'Linear Prediction'</span>])</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x_</span><span class="sc">{1}</span><span class="st">$'</span>)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$x_</span><span class="sc">{2}</span><span class="st">$'</span>)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## Nonlinear Regression</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>Nonlinear regression is similar to linear regression, but uses a curve to fit the data, compared to a straight line with linear regression. Most problems or data in the real world are not linear in nature, so nonlinear regression allows for a much greater accuracy when analyzing items.</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>The dataset used for the above linear regression section was used and slightly manipulated to create a more nonlinear quadratic shape. This was done to more easily show the difference between linear and nonlinear regression.</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>After fitting lines of best fit for varying degrees of polynomials (see @fig-poly), it can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below (@fig-table) shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-summary: Imports and dataset setup</span></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error,mean_squared_error</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>load_wine()</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame(df[<span class="st">'data'</span>],columns<span class="op">=</span>df[<span class="st">'feature_names'</span>])</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>df[[<span class="st">'alcohol'</span>,<span class="st">'proline'</span>]]</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>df.alcohol</span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>df.proline</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.array(x.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>xp<span class="op">=</span>x</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>np.array(y.values).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="dv">100</span></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>yp<span class="op">=</span>y</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>polynomial_converter <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>,include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>poly_features <span class="op">=</span> polynomial_converter.fit_transform(y)</span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>poly_features[:,<span class="dv">0</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span>poly_features[:,<span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>y<span class="op">/</span><span class="dv">100</span></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>x[:,<span class="dv">0</span>].sort()</span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>y[:,<span class="dv">0</span>].sort()</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear regression</span></span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">1</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a>std_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>polynomial_regression.fit(x, y)</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a>X_newlin <span class="op">=</span> np.linspace(<span class="bu">min</span>(x), <span class="bu">max</span>(x), <span class="bu">len</span>(x)).reshape(<span class="bu">len</span>(x), <span class="dv">1</span>)</span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a>y_newbiglin <span class="op">=</span> polynomial_regression.predict(X_newlin)</span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial regression with degree = 2</span></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a>polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a>polynomial_regression.fit(x, y)</span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="bu">min</span>(x), <span class="bu">max</span>(x), <span class="bu">len</span>(x)).reshape(<span class="bu">len</span>(x), <span class="dv">1</span>)</span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a>y_newbig <span class="op">=</span> polynomial_regression.predict(X_new)</span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial regression with degree = 3</span></span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a>polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">3</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a>polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a>polynomial_regression.fit(x, y)</span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>X_newlin3 <span class="op">=</span> np.linspace(<span class="bu">min</span>(x), <span class="bu">max</span>(x), <span class="bu">len</span>(x)).reshape(<span class="bu">len</span>(x), <span class="dv">1</span>)</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a>y_newbiglin3 <span class="op">=</span> polynomial_regression.predict(X_new)</span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a><span class="co">#|code-summary: Plotting code</span></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-poly</span></span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Polynomial/Nonlinear regression model on data"</span></span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas.plotting <span class="im">import</span> table</span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a>plt.scatter(x,y,marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a>plt.plot(X_newlin, y_newbiglin,color<span class="op">=</span><span class="st">'green'</span>)</span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_newbig,color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>plt.plot(X_newlin3, y_newbiglin3,color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Data'</span>,<span class="st">'Linear fit'</span>,<span class="st">'Polynomial fit (deg = 2)'</span>,<span class="st">'Polynomial fit (deg = 3)'</span>])</span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a><span class="co">#|code-summary: Table code</span></span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-table</span></span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "R squared values for each regression model"</span></span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a>linscore<span class="op">=</span>polynomial_regression.score(X_newlin,y_newbiglin)</span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a>linscore2<span class="op">=</span>polynomial_regression.score(X_new,y_newbig)</span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a>linscore3<span class="op">=</span>polynomial_regression.score(X_newlin3,y_newbiglin3)</span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame([[linscore,linscore2,linscore3]],columns<span class="op">=</span>[<span class="st">'Linear Regression'</span>,<span class="st">'Polynomial Regression (deg = 2)'</span>,<span class="st">'Polynomial Regression (deg = 3)'</span>],index<span class="op">=</span>[<span class="st">'R squared value'</span>,<span class="st">'R^21'</span>,<span class="st">'R^22'</span>])</span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a>fix, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">1</span>))</span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a>ax.axis(<span class="st">'off'</span>)</span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a>table(ax,df.transpose()[<span class="st">'R squared value'</span>],loc<span class="op">=</span><span class="st">'top'</span>,cellLoc<span class="op">=</span><span class="st">'center'</span>,colWidths<span class="op">=</span><span class="bu">list</span>([<span class="fl">.6</span>, <span class="fl">.6</span>]))</span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a>As can be seen from the results of this blog post, both linear and nonlinear regression are useful tools when analyzing datasets. Each type of regression serves its own purpose and should be chosen based on the data being monitored/analyzed. For nonlinear regression, it is important to not overfit the model and apply too high of a degree to the polynomial, as this can lead to decreased accuracy.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>