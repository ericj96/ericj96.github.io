[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hyperparameter Tuning\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection on Spacecraft Telemetry\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nData Preprocessing\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "",
    "text": "Every spacecraft that is launched has some form of onboard anomaly responses for most known failure cases in order to safe the vehicle. Normally these are simple low/high limits set for certain monitors (temperature, voltage, etc) with a corresponding response, whether that be a simple visual alarm or powering off certain equipment.\nThe problem with anomalies in space is that they can be incredibly hard to predict, as multiple components can react slightly out of family to create a larger issue. Spacecraft will also generate a massive amount of data the longer they are on orbit, and manually looking and trending this data from a human standpoint can miss certain anomalies. But, this large amount of data makes them perfect for utilizing machine learning. Machine learning would allow for these anomalies to not only be identified, but also potentially predicted and prevented, allowing the spacecraft to stay in mission over potentially high priority targets (depending on the payload/mission). An automatic response to a predicted anomaly would limit both downtime and human interaction, as the investigation and implementation can take hours to days before returning to mission.\n\n\n\nExample of anomaly in telemetry data"
  },
  {
    "objectID": "posts/Topic_2/index.html",
    "href": "posts/Topic_2/index.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "When a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Topic_3/index.html",
    "href": "posts/Topic_3/index.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "Background"
  },
  {
    "objectID": "posts/post-with-code/index.html#preprocessing",
    "href": "posts/post-with-code/index.html#preprocessing",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "Preprocessing",
    "text": "Preprocessing\nMany spacecraft constellations have decades of on orbit telemetry available, but is mostly proprietary and not available for public use. LASP and NASA releases subsets of data to the public, and this is what was used for this blog post. Reaction wheel temperatures, battery temperatures and voltages, and total bus current datasets were used, each having 750,000+ samples of data over ~14.5 years. Because ARIMA requires the data to be stationary, each of the datasets is first sampled down to a daily mean which brings the size down to only 5346 data points. This will allow for much faster processing. To increase the number of exogenous features, each dataset is also turned into sets of rolling mean and rolling standard deviation in windows of 3, 7, and 30 days.\nUnfold the below code to see the setup of the data and how it is preprocessed as mentioned above.\nA visualization of the separation between training and test data can be seen in in Figure 1, with the first 75% used for training and the remaining 25% used for test data.\n\n\nCode\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n\n\n\n\n\n\n\nFigure 1: Training data is first 75% of wheel temperature data, with the remaining 25% used as the test data for verification"
  },
  {
    "objectID": "posts/post-with-code/index.html#truth-data",
    "href": "posts/post-with-code/index.html#truth-data",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "Truth Data",
    "text": "Truth Data\nIn order to determine the accuracy of the anomaly detection methods used below, a set of truth data points needed to be manually chosen. These points were identified to be points in time where an anomaly took place based on personal experience of operational spacecraft data. 115 out of 1137 total points were marked as anomalous. Figure 2 identifies the anomalies, marked in red.\n\n\nCode\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(9,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n\n\n\n\n\nFigure 2: Set of anomalous data points to be used as truth"
  },
  {
    "objectID": "posts/Topic_2/index.html#grid-search",
    "href": "posts/Topic_2/index.html#grid-search",
    "title": "Hyperparameter Tuning",
    "section": "Grid Search",
    "text": "Grid Search\nThe grid search method of optimizing hyperparameters works by running through each combination of parameters and choosing the combination with the highest score at the end.\nThe drawback of using grid search is that it is very time intensive due to the large number of combinations that it iterates through\n\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n            'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n            'kernel': ['rbf','linear']} \n\ngrid = GridSearchCV(SVC(), param_grid,refit = True, verbose = 1) \ngrid.fit(X_train, y_train) \ngrid_predictions = grid.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, grid_predictions)) \nprint('%s' % (grid.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,grid_predictions))\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nAccuracy = 0.992593\n\n\n\nAs seen in the table above, an accuracy of 99.259% was achieved with the hyperparameter values of c=1, gamma = 0.001, and kernel = rbf. This is a slight improvement from the standard parameters, around 0.5% increase in accuracy. As seen in the results section and in Figure 1, the shape of the grid search can be seen in the time it takes for each iteration, as it follows the same pattern for every set of combinations."
  },
  {
    "objectID": "posts/Topic_2/index.html#random-search",
    "href": "posts/Topic_2/index.html#random-search",
    "title": "Hyperparameter Tuning",
    "section": "Random Search",
    "text": "Random Search\nRandom search works similarly to grid search, but moves through it in a random fashion and only uses a fixed number of combinations. This allows for similar optimization results as the grid search in a fraction of the time. As seen in the results section and in Figure 1, the random search is ~6 times faster.\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomizedSearchCV(estimator=SVC(),param_distributions=param_grid,verbose=1)\nclf.fit(X_train, y_train) \nclf_predictions = clf.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, clf_predictions)) \nprint('%s' % (clf.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,clf_predictions))\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.95      0.96      0.95        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.95      0.97      0.96        61\n           5       0.98      0.98      0.98        59\n           6       1.00      0.96      0.98        46\n           7       1.00      0.96      0.98        56\n           8       0.93      0.97      0.95        59\n           9       0.98      0.98      0.98        48\n\n    accuracy                           0.98       540\n   macro avg       0.98      0.98      0.98       540\nweighted avg       0.98      0.98      0.98       540\n\n{'kernel': 'linear', 'gamma': 0.01, 'C': 1000}\nAccuracy = 0.977778\n\n\n\nBecause of the randomness of the random search, the optimal value(s) change every time this method is ran. For the iteration performed at the time of this blog post, the random search method determined optimal values worse than both the grid and Bayesian search methods."
  },
  {
    "objectID": "posts/Topic_2/index.html#bayesian-optimization",
    "href": "posts/Topic_2/index.html#bayesian-optimization",
    "title": "Hyperparameter Tuning",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nBayesian optimization works differently than the other two commonly used methods mentioned above, it uses Bayes Theorem to find the minimum or maximum of an objective function. Because of this difference, not all parameter values are used and a fixed number of hyperparameter combinations are iterated through (default number of iterations is 50). Bayesian optimization is usually used to optimize expensive to evaluate functions, which is not necessarily the case for this example as the data being used is somewhat basic for the purposes of this blog post.\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nfrom skopt import BayesSearchCV\n\nbayes = BayesSearchCV(SVC(), param_grid) \nbayes.fit(X_train, y_train) \nbayes_predictions = bayes.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, bayes_predictions)) \nprint('%s' % (bayes.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,bayes_predictions))\nac3=accuracy_score(y_test,bayes_predictions)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nOrderedDict([('C', 1.0), ('gamma', 0.001), ('kernel', 'rbf')])\nAccuracy = 0.992593\n\n\n\nIt is seen that utilizing a Bayesian search optimization identifies the same optimal values as the grid search, c = 1, gamma = 0.001, and kernel = rbf, with an accuracy of 99.259%. This is once again slightly higher than the baseline method by around 0.5%"
  },
  {
    "objectID": "posts/Topic_2/index.html#baseline-data",
    "href": "posts/Topic_2/index.html#baseline-data",
    "title": "Hyperparameter Tuning",
    "section": "Baseline Data",
    "text": "Baseline Data\nFor the purposes of this blog, a basic Support Vector Machine (SVM) classification model will be trained with a built in dataset from the sklearn library. This will give a baseline accuracy and results to compare when attempting to optimize the hyperparameters. The hyperparameters that are used by default with this algorithm are:\n\nC - Regularization parameter. The penalty is a squared l2 penalty [default = 1.0]\nKernel - Kernel type used in algorithm [default = ‘rbf’]\n\nNote: Linear and RBF kernels were used as the possible options in this example\n\nGamma - Kernel coefficient [default = ‘scale’, or 1 / (n_features * X)]\n\n\n\nCode\n# Setting up basic SVM to compare with optimized hyperparameters. Using built in dataset of data\nimport pandas as pd \nimport numpy as np \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split \n\ncancer = load_breast_cancer() \ncancer=load_digits()\ndf_feat = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndf_target = pd.DataFrame(cancer['target'], columns =['Cancer']) \nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target),test_size = 0.30, random_state = 101) \nmodel = SVC() \nmodel.fit(X_train, y_train) \n\n# print prediction results \npredictions = model.predict(X_test) \nprint(classification_report(y_test, predictions)) \nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,predictions))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.98      0.97      0.98        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       0.98      0.97      0.97        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nAccuracy = 0.988889\n\n\n\nIt is seen that the baseline model performs with an accuracy of 98.89% using C = 1, kernel = rbf, gamma = scale (or around 0.015). This will be compared against with several methods of hyperparameter optimization but is already extremely high for accuracy."
  }
]