[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\noutlier\n\n\nanomaly\n\n\ndetection\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\nlinear\n\n\nnonlinear\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\n(Old blog post) Anomaly Detection on Spacecraft Telemetry\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\n(Old blog post) Hyperparameter Tuning\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\n(Old blog post) Data Preprocessing\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ndata\n\n\npreprocessing\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nEric Jackson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "",
    "text": "Every spacecraft that is launched has some form of onboard anomaly responses for most known failure cases in order to safe the vehicle. Normally these are simple low/high limits set for certain monitors (temperature, voltage, etc) with a corresponding response, whether that be a simple visual alarm or powering off certain equipment.\nThe problem with anomalies in space is that they can be incredibly hard to predict, as multiple components can react slightly out of family to create a larger issue. Spacecraft will also generate a massive amount of data the longer they are on orbit, and manually looking and trending this data from a human standpoint can miss certain anomalies. But, this large amount of data makes them perfect for utilizing machine learning. Machine learning would allow for these anomalies to not only be identified, but also potentially predicted and prevented, allowing the spacecraft to stay in mission over potentially high priority targets (depending on the payload/mission). An automatic response to a predicted anomaly would limit both downtime and human interaction, as the investigation and implementation can take hours to days before returning to mission.\n\n\n\nExample of anomaly in telemetry data"
  },
  {
    "objectID": "posts/Topic_2/index.html",
    "href": "posts/Topic_2/index.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "When a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Topic_3/index.html",
    "href": "posts/Topic_3/index.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "Background\nFor any machine learning model, there is a set of data that will be input into it. Generally the data will be broken into multiple sets, consisting of training data and test data. The training data will be the portion or set of data that is used to train the model, and the test data is what the trained model is run on to produce results.\nBefore one can use datasets, it’s generally necessary to do some form of preprocessing to the raw data to ensure that the model can run efficiently and accurately. This can be as simple as removing NaN or Null values and as complex as performing statistical analysis to remove outliers and normalizing the data.\n\npopulating missing data\ndropping unnecessary data\nsplitting into training and testing\ndropping nan\nconverting categorical values into numerical\ndownsampling data\n\n\n\nData Preprocessing\nOne of the first steps in importing datasets is to drop any NaN or null values. These values will generally cause issues when running and machine learning model and are best to remove immediately. Luckily, there are several built in functions to perform this.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# import dataset \ndf=pd.read_csv('./WheelTemperature.csv')\ndf.isna().sum()\n\nDate     0\nHigh    17\ndtype: int64\n\ndfdrop=df.dropna()\ndfdrop.isna().sum()\n\nDate    0\nHigh    0\ndtype: int64"
  },
  {
    "objectID": "posts/post-with-code/index.html#preprocessing",
    "href": "posts/post-with-code/index.html#preprocessing",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "Preprocessing",
    "text": "Preprocessing\nMany spacecraft constellations have decades of on orbit telemetry available, but is mostly proprietary and not available for public use. LASP and NASA releases subsets of data to the public, and this is what was used for this blog post. Reaction wheel temperatures, battery temperatures and voltages, and total bus current datasets were used, each having 750,000+ samples of data over ~14.5 years. Because ARIMA requires the data to be stationary, each of the datasets is first sampled down to a daily mean which brings the size down to only 5346 data points. This will allow for much faster processing. To increase the number of exogenous features, each dataset is also turned into sets of rolling mean and rolling standard deviation in windows of 3, 7, and 30 days.\nUnfold the below code to see the setup of the data and how it is preprocessed as mentioned above.\nA visualization of the separation between training and test data can be seen in in Figure 1, with the first 75% used for training and the remaining 25% used for test data.\n\n\nCode\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n\n\n\n\n\n\n\nFigure 1: Training data is first 75% of wheel temperature data, with the remaining 25% used as the test data for verification"
  },
  {
    "objectID": "posts/post-with-code/index.html#truth-data",
    "href": "posts/post-with-code/index.html#truth-data",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "Truth Data",
    "text": "Truth Data\nIn order to determine the accuracy of the anomaly detection methods used below, a set of truth data points needed to be manually chosen. These points were identified to be points in time where an anomaly took place based on personal experience of operational spacecraft data. 115 out of 1137 total points were marked as anomalous. Figure 2 identifies the anomalies, marked in red.\n\n\nCode\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(9,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n\n\n\n\n\nFigure 2: Set of anomalous data points to be used as truth"
  },
  {
    "objectID": "posts/Topic_2/index.html#grid-search",
    "href": "posts/Topic_2/index.html#grid-search",
    "title": "Hyperparameter Tuning",
    "section": "Grid Search",
    "text": "Grid Search\nThe grid search method of optimizing hyperparameters works by running through each combination of parameters and choosing the combination with the highest score at the end.\nThe drawback of using grid search is that it is very time intensive due to the large number of combinations that it iterates through\n\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n            'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n            'kernel': ['rbf','linear']} \n\ngrid = GridSearchCV(SVC(), param_grid,refit = True, verbose = 1) \ngrid.fit(X_train, y_train) \ngrid_predictions = grid.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, grid_predictions)) \nprint('%s' % (grid.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,grid_predictions))\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nAccuracy = 0.992593\n\n\n\nAs seen in the table above, an accuracy of 99.259% was achieved with the hyperparameter values of c=1, gamma = 0.001, and kernel = rbf. This is a slight improvement from the standard parameters, around 0.5% increase in accuracy. As seen in the results section and in Figure 1, the shape of the grid search can be seen in the time it takes for each iteration, as it follows the same pattern for every set of combinations."
  },
  {
    "objectID": "posts/Topic_2/index.html#random-search",
    "href": "posts/Topic_2/index.html#random-search",
    "title": "Hyperparameter Tuning",
    "section": "Random Search",
    "text": "Random Search\nRandom search works similarly to grid search, but moves through it in a random fashion and only uses a fixed number of combinations. This allows for similar optimization results as the grid search in a fraction of the time. As seen in the results section and in Figure 1, the random search is ~6 times faster.\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomizedSearchCV(estimator=SVC(),param_distributions=param_grid,verbose=1)\nclf.fit(X_train, y_train) \nclf_predictions = clf.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, clf_predictions)) \nprint('%s' % (clf.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,clf_predictions))\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.95      0.96      0.95        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.95      0.97      0.96        61\n           5       0.98      0.98      0.98        59\n           6       1.00      0.96      0.98        46\n           7       1.00      0.96      0.98        56\n           8       0.93      0.97      0.95        59\n           9       0.98      0.98      0.98        48\n\n    accuracy                           0.98       540\n   macro avg       0.98      0.98      0.98       540\nweighted avg       0.98      0.98      0.98       540\n\n{'kernel': 'linear', 'gamma': 0.01, 'C': 1000}\nAccuracy = 0.977778\n\n\n\nBecause of the randomness of the random search, the optimal value(s) change every time this method is ran. For the iteration performed at the time of this blog post, the random search method determined optimal values worse than both the grid and Bayesian search methods."
  },
  {
    "objectID": "posts/Topic_2/index.html#bayesian-optimization",
    "href": "posts/Topic_2/index.html#bayesian-optimization",
    "title": "Hyperparameter Tuning",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nBayesian optimization works differently than the other two commonly used methods mentioned above, it uses Bayes Theorem to find the minimum or maximum of an objective function. Because of this difference, not all parameter values are used and a fixed number of hyperparameter combinations are iterated through (default number of iterations is 50). Bayesian optimization is usually used to optimize expensive to evaluate functions, which is not necessarily the case for this example as the data being used is somewhat basic for the purposes of this blog post.\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nfrom skopt import BayesSearchCV\n\nbayes = BayesSearchCV(SVC(), param_grid) \nbayes.fit(X_train, y_train) \nbayes_predictions = bayes.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, bayes_predictions)) \nprint('%s' % (bayes.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,bayes_predictions))\nac3=accuracy_score(y_test,bayes_predictions)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nOrderedDict([('C', 1.0), ('gamma', 0.001), ('kernel', 'rbf')])\nAccuracy = 0.992593\n\n\n\nIt is seen that utilizing a Bayesian search optimization identifies the same optimal values as the grid search, c = 1, gamma = 0.001, and kernel = rbf, with an accuracy of 99.259%. This is once again slightly higher than the baseline method by around 0.5%"
  },
  {
    "objectID": "posts/Topic_2/index.html#baseline-data",
    "href": "posts/Topic_2/index.html#baseline-data",
    "title": "Hyperparameter Tuning",
    "section": "Baseline Data",
    "text": "Baseline Data\nFor the purposes of this blog, a basic Support Vector Machine (SVM) classification model will be trained with a built in dataset from the sklearn library. This will give a baseline accuracy and results to compare when attempting to optimize the hyperparameters. The hyperparameters that are used by default with this algorithm are:\n\nC - Regularization parameter. The penalty is a squared l2 penalty [default = 1.0]\nKernel - Kernel type used in algorithm [default = ‘rbf’]\n\nNote: Linear and RBF kernels were used as the possible options in this example\n\nGamma - Kernel coefficient [default = ‘scale’, or 1 / (n_features * X)]\n\n\n\nCode\n# Setting up basic SVM to compare with optimized hyperparameters. Using built in dataset of data\nimport pandas as pd \nimport numpy as np \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split \n\ncancer = load_breast_cancer() \ncancer=load_digits()\ndf_feat = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndf_target = pd.DataFrame(cancer['target'], columns =['Cancer']) \nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target),test_size = 0.30, random_state = 101) \nmodel = SVC() \nmodel.fit(X_train, y_train) \n\n# print prediction results \npredictions = model.predict(X_test) \nprint(classification_report(y_test, predictions)) \nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,predictions))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.98      0.97      0.98        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       0.98      0.97      0.97        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nAccuracy = 0.988889\n\n\n\nIt is seen that the baseline model performs with an accuracy of 98.89% using C = 1, kernel = rbf, gamma = scale (or around 0.015). This will be compared against with several methods of hyperparameter optimization but is already extremely high for accuracy."
  },
  {
    "objectID": "posts/Probability_Theory_and_Random_Variables/index.html",
    "href": "posts/Probability_Theory_and_Random_Variables/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Background\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods\n\n\n\nTuning Techniques\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically."
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression analysis is a broad and useful field of statistical analysis, and for the purpose of this blog, only linear and nonlinear regression will be discuessed. Both linear and nonlinear regression have many practical uses and are used to estimate the relationships between two or more variables, using either lines or curves. The main use of linear or nonlinear regression are for predicting or forecasting data and thus, is used widely in machine learning situations. Regression analysis is also used to infer casual relationships between variables, otherwise called correlation. [1]"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Background"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html",
    "href": "posts/Anomaly_Outlier_Detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "In this blog post, two popular outlier detection algorithms (DBSCAN and Isolation Forest) will be used and compared against each other. Outlier detection is important for preprocessing datasets as there is the potential that outliers can throw off the results of a machine learning model. By removing outliers, it is possible to get a more accurate result. These methods can also be used for anomaly detection in time series data, spacecraft telemetry for example, to identify potential out of ordinary trends in data. This can potentially allow for a preemptive response to an issue (ex: temperature changing, so heaters are powered on/off) and reduces the amount of time that the spacecraft is out of mission.\nFor simplicity, a known dataset will be imported and used with two features as the dataset. The breast cancer dataset from sklearn is a commonly used binary classification dataset from UC Irvine showing information of tumor characteristics in Wisconsin.\nFigure 1 shows a box and whisker plot of both sets of data for an initial look at the outliers identified through purely statistical means.\n\n\nSetup and plotting of initial data\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndata=X_train[[\"mean radius\",\"mean smoothness\"]]\ndf=data;\n\nred_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\nfig, axs = plt.subplots(len(df.columns),1, figsize=(8,6))\nfor i, ax in enumerate(axs.flat):\n    ax.boxplot(df.iloc[:,i], flierprops=red_circle,vert=False)\n    ax.set_title(df.columns[i])\n    ax.tick_params(axis='y', labelsize=14)\n  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Box/whisker plot of initial data"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an\n\n\nCode\n# plot code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"Mean Radius\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"Mean Smoothness\")\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n    plt.show()\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn import cluster \nfrom sklearn.cluster import KMeans\nfrom itertools import cycle, islice\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\n# setting up blobs code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\n\n#blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8], [-2.8,  2.8], [-2.8,  1.3]])\n                         \n# randomize centers of blobs \nblob_centers= np.random.randint(0,5,size=(5,2))+.116846842*.65487984\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\nx, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n                  random_state=7)\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(x)     \n\ncm = confusion_matrix(y, y_pred)\ncm_argmax = cm.argmax(axis=0)\ny_pred_ = np.array([cm_argmax[i] for i in y_pred])\ncm_ = confusion_matrix(y, y_pred)\naccuracy_score(y,y_pred_)\n\nfrom sklearn.cluster import DBSCAN\ndbscan=DBSCAN(eps=0.5)\ndbscan.fit(x)\ncolors = dbscan.labels_\ny_pred=colors\ncm = confusion_matrix(y, y_pred)\ncm_argmax = cm.argmax(axis=0)\ny_pred_ = np.array([cm_argmax[i] for i in y_pred])\ncm_ = confusion_matrix(y, y_pred)\naccuracy_score(y,y_pred_)\nfig, ax = plt.subplots(figsize=(9,7))\n\nplot_dbscan(dbscan, x, size=9)\n\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=8.\n\n\n\n\n\n\n\ncolors = np.array(\n    list(\n        islice(\n            cycle(\n                [\n                    \"#377eb8\",\n                    \"#ff7f00\",\n                    \"#4daf4a\",\n                    \"#f781bf\",\n                    \"#a65628\",\n                    \"#984ea3\",\n                    \"#999999\",\n                    \"#e41a1c\",\n                    \"#dede00\",\n                ]\n            ),\n            int(max(y_pred) + 1),\n        )\n    )\n)\n\n        \ncolors = np.append(colors, [\"#000000\"])\nfig, ax = plt.subplots(figsize=(9,7))\nplt.scatter(x[:,0],x[:,1],color=colors[y_pred])\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\n#plt.show()\n\n\n\ncolors = np.array(\n    list(\n        islice(\n            cycle(\n                [\n                    \"#377eb8\",\n                    \"#ff7f00\",\n                    \"#4daf4a\",\n                    \"#f781bf\",\n                    \"#a65628\",\n                    \"#984ea3\",\n                    \"#999999\",\n                    \"#e41a1c\",\n                    \"#dede00\",\n                ]\n            ),\n            int(max(y) + 1),\n        )\n    )\n)\n\n\ncolors = np.append(colors, [\"#000000\"])\nfig, ax = plt.subplots(figsize=(9,7))\nplt.scatter(x[:,0],x[:,1],color=colors[y])\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.grid()\n#plt.show()"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "href": "posts/Anomaly_Outlier_Detection/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "title": "Anomaly/Outlier Detection",
    "section": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nDBSCAN is a popular algorithm used mainly for clustering given sets of points. It works by grouping together points that are close together and have multiple nearest neighbors. It considers outliers to be points that are alone in low density regions. [1]\nFor the example in this blog, a two-dimensional set of data (mean radius and mean smoothness) is used to run through the DBSCAN algorithm. Figure 2 shows the result with outlier/anomalous data points shown in orange. Since DBSCAN is most known for being a clustering algorithm, the bottom plot shows the different clusters that the algorithm identified. All points with a color and a star indicate a clustered point, of which there are two separate clusters that the algorithm identified. Any point with a red X indicates it is an anomalous/outlier point, and any point that is not surrounded by color and is a period is a non-core point (of which there were only two single points identified as such).\nNote: The default epsilon parameter value of 0.5 was used for the DBSCAN algorithm. This parameter “defines the maximum distance between two samples for one to be considered as in the neighborhood of the other”. [2]\n\n\nplot_dbscan function\n# plot code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"Mean Radius\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"Mean Smoothness\")\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n    plt.show()\n\n\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \n\n# DBSCAN model training/fitting\ndbscan=DBSCAN()\ndbscan.fit(X_train[[\"mean radius\",\"mean smoothness\"]])\ncolors = dbscan.labels_\noutliers=colors.T&lt;0\nnormal=colors.T&gt;=0\nprint(\"Number of outliers detected: %d\" % sum(i&lt;0 for i in colors.T))\nprint(\"Number of normal samples detected: %d\" % sum(i&gt;=0 for i in colors.T))\n\nNumber of outliers detected: 8\nNumber of normal samples detected: 561\n\n\n\n\nPlotting code\n#plotting\nfig, ax = plt.subplots(figsize=(9,14))\nplt.subplot(211)\nplt.plot(X_train[\"mean radius\"][colors==0],X_train[\"mean smoothness\"][colors==0],marker='o',linestyle=\"None\")\nplt.plot(X_train[\"mean radius\"][colors!=0],X_train[\"mean smoothness\"][colors!=0],marker='o',linestyle=\"None\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.title(\"Dataset Outlier Detection via DBSCAN\")\n\nplt.subplot(212)\nplot_dbscan(dbscan, X_train[[\"mean radius\",\"mean smoothness\"]].values, size=100)\n\nplt.show()\n\n\n\n\n\nFigure 2: Data that the DBSCAN algorithm identifies as an outlier or not"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#isolation-forest",
    "href": "posts/Anomaly_Outlier_Detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier Detection",
    "section": "Isolation Forest",
    "text": "Isolation Forest\nIsolation Forest is another commonly used outlier detection method which detects outliers utilizing binary trees. This method recursively partitions data points based on randomly selected attribute and then assigned anomaly scores based on number of “splits” needed to isolate a data point. The training dataset is used to build the “trees” and then the validation data is passed through those trees and assigned an anomaly score. In the case of this example, the training and validation is the same dataset (only one iteration of the call to the model). Based on the anomaly score, it can be determined which points are outliers. One of the inputs to the Isolation Forest algorithm is the contamination parameter, or the expected percentage of data that will be anomalous. For the purposes of the example, the default contamination value of 10% will be used.\nAdvantages:\n\nLow memory utilization\nWorks best with large datasets\n\nNote: Below contains some modified code from HERE\n\nfrom sklearn.ensemble import IsolationForest\n\noutliers_fraction = float(.03)\nmodel =  IsolationForest()\ndata=df.values\nprediction = model.fit_predict(data)\n\nprint(\"Number of outliers detected: {}\".format(prediction[prediction &lt; 0].sum()*-1))\nprint(\"Number of normal samples detected: {}\".format(prediction[prediction &gt; 0].sum()))\n\nNumber of outliers detected: 94\nNumber of normal samples detected: 475\n\n\n\n\nPlotting code\nfig, ax = plt.subplots(figsize=(9,7))\nnormal_data = data[np.where(prediction &gt; 0)]\noutliers = data[np.where(prediction &lt; 0)]\nplt.scatter(normal_data[:, 0], normal_data[:, 1])\nplt.scatter(outliers[:, 0], outliers[:, 1])\nplt.title(\"Dataset Outlier Detection via Isolation Forest\")\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.show()\n\n\n\n\n\nFigure 3: Data that the Isolation Forest algorithm identifies as an outlier or not"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#conclusion",
    "href": "posts/Anomaly_Outlier_Detection/index.html#conclusion",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "It can be seen that both DBSCAN and Isolation Forest identified most of the same outlier points, but Isolation Forest identified 10 more total outlier data points, mainly due to the contamination parameter being large (3%)."
  },
  {
    "objectID": "posts/Clustering/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "href": "posts/Clustering/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "title": "Clustering",
    "section": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nAs explored in the blog post on anomaly/outlier detection, the DBSCAN algorithm is a widely used machine learning algorithm for clustering given sets of points by grouping together points that are close together and have multiple nearest neighbors. While creating the clusters of the data, it will naturally identify outliers and thus, is a great algorithm to accomplish both outlier detection and clustering.\nOTHERS\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\nparams = default_base.copy()\naffinity_propagation = cluster.AffinityPropagation(\ndamping=params[\"damping\"],\npreference=params[\"preference\"],\nrandom_state=params[\"random_state\"],\n)\naffinity_propagation.fit(x)\ny_pred = affinity_propagation.labels_.astype(int)\naccuracy_score(y,y_pred)\ncm = confusion_matrix(y, y_pred)\ncm_argmax = cm.argmax(axis=0)\ny_pred_ = np.array([cm_argmax[i] for i in y_pred])\ncm_ = confusion_matrix(y, y_pred)\naccuracy_score(y,y_pred_)\n\n0.771\n\n\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nseed = 30\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nb,truth = datasets.make_blobs(n_samples=n_samples, random_state=seed)\n\nblob_centers= np.random.randint(0,5,size=(5,2))+.116846842*.65487984\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n\n#blobs = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n#                  random_state=7)\n#b,truth = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n#                 random_state=7)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 13))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    \n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n   \n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        n_init=\"auto\",\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n       \n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" &gt; 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n        \n        cm = confusion_matrix(truth, y_pred)\n        cm_argmax = cm.argmax(axis=0)\n        y_pred_ = np.array([cm_argmax[i] for i in y_pred])\n        cm_ = confusion_matrix(y, y_pred)\n        #accuracy_score(y,y_pred_)\n        acc=accuracy_score(truth,y_pred_)\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            #(\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            ('%3.6f' % acc),\n            transform=plt.gca().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.savefig('./pic.png')\nplt.show()\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1902: UserWarning:\n\nMiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size &gt;= 3072 or by setting the environment variable OMP_NUM_THREADS=2\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n\nC:\\Users\\ericj\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning:\n\nKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2."
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html#linear-regression",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html#linear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is a technique that uses a straight line to model the relationship between two or more variables. There are several models that can be used for linear regression, with the most common one being the least squares method. Linear regression can also be performed by minimizing the lack of fit or by minimizing a version of the least squares cost function (ridge regression and lasso) [1]. The LinearRegression model from the sklearn library utilizes the least squares method to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation [2].\nA dataset native to the sklearn library was used for this blog post (load_wine) and two features were chosen at random based on how linear the trend appeared. As can be seen in Figure 1, the data appears random with a slight positive trend. The linear regression model was overlayed on top of the original data with a red line, and can be seen to match the positive slope of the data very closely. It also sits roughly in the middle of the data throughout, and leads to an RMSE of 2.403.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nimport math\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\ny=np.array(y.values).reshape(-1,1)/100\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(x,y)\n\ntest_predictions = model.predict(x)\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nMAE = mean_absolute_error(y,test_predictions)\nMSE = mean_squared_error(y,test_predictions)\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n\nRMSE: 2.403\n\n\n\n\nPlotting code\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y)\nplt.plot(x,test_predictions,color=\"red\")\nplt.legend(['Data','Linear Prediction'])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n\n\n\n\n\nFigure 1: Linear regression model on data"
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html#nonlinear-regression",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html#nonlinear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\nNonlinear regression is similar to linear regression, but uses a curve to fit the data, compared to a straight line with linear regression. Most problems or data in the real world are not linear in nature, so nonlinear regression allows for a much greater accuracy when analyzing items.\nThe dataset used for the above linear regression section was used and slightly manipulated to create a more nonlinear quadratic shape. This was done to more easily show the difference between linear and nonlinear regression.\nAfter fitting lines of best fit for varying degrees of polynomials (see Figure 2), it can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below (Figure 3) shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.\n\n\nImports and dataset setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\nxp=x\ny=np.array(y.values).reshape(-1,1)/100\nyp=y\n\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\npoly_features = polynomial_converter.fit_transform(y)\n\nx=poly_features[:,0].reshape(-1,1)\ny=poly_features[:,1].reshape(-1,1)*y/100\n\nx[:,0].sort()\ny[:,0].sort()\n\n\n\n# Linear regression\npolybig_features = PolynomialFeatures(degree=1, include_bias=False)\nstd_scaler = StandardScaler()\nlin_reg = LinearRegression()\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin = polynomial_regression.predict(X_newlin)\n\n# Polynomial regression with degree = 2\npolybig_features = PolynomialFeatures(degree=2, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_new = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbig = polynomial_regression.predict(X_new)\n\n# Polynomial regression with degree = 3\npolybig_features = PolynomialFeatures(degree=3, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin3 = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin3 = polynomial_regression.predict(X_new)\n\n\n\nPlotting code\nfrom pandas.plotting import table\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y,marker='o')\nplt.plot(X_newlin, y_newbiglin,color='green')\nplt.plot(X_new, y_newbig,color='red')\nplt.plot(X_newlin3, y_newbiglin3,color='black')\nplt.legend(['Data','Linear fit','Polynomial fit (deg = 2)','Polynomial fit (deg = 3)'])\nplt.show()\n\n\n\n\n\nFigure 2: Polynomial/Nonlinear regression model on data\n\n\n\n\n\n\nTable code\nlinscore=polynomial_regression.score(X_newlin,y_newbiglin)\nlinscore2=polynomial_regression.score(X_new,y_newbig)\nlinscore3=polynomial_regression.score(X_newlin3,y_newbiglin3)\n\ndf=pd.DataFrame([[linscore,linscore2,linscore3]],columns=['Linear Regression','Polynomial Regression (deg = 2)','Polynomial Regression (deg = 3)'],index=['R squared value','R^21','R^22'])\nfix, ax = plt.subplots(figsize=(8,1))\nax.axis('off')\ntable(ax,df.transpose()['R squared value'],loc='top',cellLoc='center',colWidths=list([.6, .6]))\nplt.show()\n\n\n\n\n\nFigure 3: R squared values for each regression model"
  }
]