[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\noutlier\n\n\nanomaly\n\n\ndetection\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nHyperparameter Tuning\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection on Spacecraft Telemetry\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nData Preprocessing\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ndata\n\n\npreprocessing\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "",
    "text": "Every spacecraft that is launched has some form of onboard anomaly responses for most known failure cases in order to safe the vehicle. Normally these are simple low/high limits set for certain monitors (temperature, voltage, etc) with a corresponding response, whether that be a simple visual alarm or powering off certain equipment.\nThe problem with anomalies in space is that they can be incredibly hard to predict, as multiple components can react slightly out of family to create a larger issue. Spacecraft will also generate a massive amount of data the longer they are on orbit, and manually looking and trending this data from a human standpoint can miss certain anomalies. But, this large amount of data makes them perfect for utilizing machine learning. Machine learning would allow for these anomalies to not only be identified, but also potentially predicted and prevented, allowing the spacecraft to stay in mission over potentially high priority targets (depending on the payload/mission). An automatic response to a predicted anomaly would limit both downtime and human interaction, as the investigation and implementation can take hours to days before returning to mission.\n\n\n\nExample of anomaly in telemetry data"
  },
  {
    "objectID": "posts/Topic_2/index.html",
    "href": "posts/Topic_2/index.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "When a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Topic_3/index.html",
    "href": "posts/Topic_3/index.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "Background\nFor any machine learning model, there is a set of data that will be input into it. Generally the data will be broken into multiple sets, consisting of training data and test data. The training data will be the portion or set of data that is used to train the model, and the test data is what the trained model is run on to produce results.\nBefore one can use datasets, it’s generally necessary to do some form of preprocessing to the raw data to ensure that the model can run efficiently and accurately. This can be as simple as removing NaN or Null values and as complex as performing statistical analysis to remove outliers and normalizing the data.\n\npopulating missing data\ndropping unnecessary data\nsplitting into training and testing\ndropping nan\nconverting categorical values into numerical\ndownsampling data\n\n\n\nData Preprocessing\nOne of the first steps in importing datasets is to drop any NaN or null values. These values will generally cause issues when running and machine learning model and are best to remove immediately. Luckily, there are several built in functions to perform this.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# import dataset \ndf=pd.read_csv('./WheelTemperature.csv')\ndf.isna().sum()\n\nDate     0\nHigh    17\ndtype: int64\n\ndfdrop=df.dropna()\ndfdrop.isna().sum()\n\nDate    0\nHigh    0\ndtype: int64"
  },
  {
    "objectID": "posts/post-with-code/index.html#preprocessing",
    "href": "posts/post-with-code/index.html#preprocessing",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "Preprocessing",
    "text": "Preprocessing\nMany spacecraft constellations have decades of on orbit telemetry available, but is mostly proprietary and not available for public use. LASP and NASA releases subsets of data to the public, and this is what was used for this blog post. Reaction wheel temperatures, battery temperatures and voltages, and total bus current datasets were used, each having 750,000+ samples of data over ~14.5 years. Because ARIMA requires the data to be stationary, each of the datasets is first sampled down to a daily mean which brings the size down to only 5346 data points. This will allow for much faster processing. To increase the number of exogenous features, each dataset is also turned into sets of rolling mean and rolling standard deviation in windows of 3, 7, and 30 days.\nUnfold the below code to see the setup of the data and how it is preprocessed as mentioned above.\nA visualization of the separation between training and test data can be seen in in Figure 1, with the first 75% used for training and the remaining 25% used for test data.\n\n\nCode\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n\n\n\n\n\n\n\nFigure 1: Training data is first 75% of wheel temperature data, with the remaining 25% used as the test data for verification"
  },
  {
    "objectID": "posts/post-with-code/index.html#truth-data",
    "href": "posts/post-with-code/index.html#truth-data",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "Truth Data",
    "text": "Truth Data\nIn order to determine the accuracy of the anomaly detection methods used below, a set of truth data points needed to be manually chosen. These points were identified to be points in time where an anomaly took place based on personal experience of operational spacecraft data. 115 out of 1137 total points were marked as anomalous. Figure 2 identifies the anomalies, marked in red.\n\n\nCode\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(9,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n\n\n\n\n\nFigure 2: Set of anomalous data points to be used as truth"
  },
  {
    "objectID": "posts/Topic_2/index.html#grid-search",
    "href": "posts/Topic_2/index.html#grid-search",
    "title": "Hyperparameter Tuning",
    "section": "Grid Search",
    "text": "Grid Search\nThe grid search method of optimizing hyperparameters works by running through each combination of parameters and choosing the combination with the highest score at the end.\nThe drawback of using grid search is that it is very time intensive due to the large number of combinations that it iterates through\n\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n            'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n            'kernel': ['rbf','linear']} \n\ngrid = GridSearchCV(SVC(), param_grid,refit = True, verbose = 1) \ngrid.fit(X_train, y_train) \ngrid_predictions = grid.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, grid_predictions)) \nprint('%s' % (grid.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,grid_predictions))\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nAccuracy = 0.992593\n\n\n\nAs seen in the table above, an accuracy of 99.259% was achieved with the hyperparameter values of c=1, gamma = 0.001, and kernel = rbf. This is a slight improvement from the standard parameters, around 0.5% increase in accuracy. As seen in the results section and in Figure 1, the shape of the grid search can be seen in the time it takes for each iteration, as it follows the same pattern for every set of combinations."
  },
  {
    "objectID": "posts/Topic_2/index.html#random-search",
    "href": "posts/Topic_2/index.html#random-search",
    "title": "Hyperparameter Tuning",
    "section": "Random Search",
    "text": "Random Search\nRandom search works similarly to grid search, but moves through it in a random fashion and only uses a fixed number of combinations. This allows for similar optimization results as the grid search in a fraction of the time. As seen in the results section and in Figure 1, the random search is ~6 times faster.\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomizedSearchCV(estimator=SVC(),param_distributions=param_grid,verbose=1)\nclf.fit(X_train, y_train) \nclf_predictions = clf.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, clf_predictions)) \nprint('%s' % (clf.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,clf_predictions))\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.95      0.96      0.95        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.95      0.97      0.96        61\n           5       0.98      0.98      0.98        59\n           6       1.00      0.96      0.98        46\n           7       1.00      0.96      0.98        56\n           8       0.93      0.97      0.95        59\n           9       0.98      0.98      0.98        48\n\n    accuracy                           0.98       540\n   macro avg       0.98      0.98      0.98       540\nweighted avg       0.98      0.98      0.98       540\n\n{'kernel': 'linear', 'gamma': 0.01, 'C': 1000}\nAccuracy = 0.977778\n\n\n\nBecause of the randomness of the random search, the optimal value(s) change every time this method is ran. For the iteration performed at the time of this blog post, the random search method determined optimal values worse than both the grid and Bayesian search methods."
  },
  {
    "objectID": "posts/Topic_2/index.html#bayesian-optimization",
    "href": "posts/Topic_2/index.html#bayesian-optimization",
    "title": "Hyperparameter Tuning",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nBayesian optimization works differently than the other two commonly used methods mentioned above, it uses Bayes Theorem to find the minimum or maximum of an objective function. Because of this difference, not all parameter values are used and a fixed number of hyperparameter combinations are iterated through (default number of iterations is 50). Bayesian optimization is usually used to optimize expensive to evaluate functions, which is not necessarily the case for this example as the data being used is somewhat basic for the purposes of this blog post.\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nfrom skopt import BayesSearchCV\n\nbayes = BayesSearchCV(SVC(), param_grid) \nbayes.fit(X_train, y_train) \nbayes_predictions = bayes.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, bayes_predictions)) \nprint('%s' % (bayes.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,bayes_predictions))\nac3=accuracy_score(y_test,bayes_predictions)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nOrderedDict([('C', 1.0), ('gamma', 0.001), ('kernel', 'rbf')])\nAccuracy = 0.992593\n\n\n\nIt is seen that utilizing a Bayesian search optimization identifies the same optimal values as the grid search, c = 1, gamma = 0.001, and kernel = rbf, with an accuracy of 99.259%. This is once again slightly higher than the baseline method by around 0.5%"
  },
  {
    "objectID": "posts/Topic_2/index.html#baseline-data",
    "href": "posts/Topic_2/index.html#baseline-data",
    "title": "Hyperparameter Tuning",
    "section": "Baseline Data",
    "text": "Baseline Data\nFor the purposes of this blog, a basic Support Vector Machine (SVM) classification model will be trained with a built in dataset from the sklearn library. This will give a baseline accuracy and results to compare when attempting to optimize the hyperparameters. The hyperparameters that are used by default with this algorithm are:\n\nC - Regularization parameter. The penalty is a squared l2 penalty [default = 1.0]\nKernel - Kernel type used in algorithm [default = ‘rbf’]\n\nNote: Linear and RBF kernels were used as the possible options in this example\n\nGamma - Kernel coefficient [default = ‘scale’, or 1 / (n_features * X)]\n\n\n\nCode\n# Setting up basic SVM to compare with optimized hyperparameters. Using built in dataset of data\nimport pandas as pd \nimport numpy as np \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split \n\ncancer = load_breast_cancer() \ncancer=load_digits()\ndf_feat = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndf_target = pd.DataFrame(cancer['target'], columns =['Cancer']) \nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target),test_size = 0.30, random_state = 101) \nmodel = SVC() \nmodel.fit(X_train, y_train) \n\n# print prediction results \npredictions = model.predict(X_test) \nprint(classification_report(y_test, predictions)) \nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,predictions))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.98      0.97      0.98        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       0.98      0.97      0.97        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nAccuracy = 0.988889\n\n\n\nIt is seen that the baseline model performs with an accuracy of 98.89% using C = 1, kernel = rbf, gamma = scale (or around 0.015). This will be compared against with several methods of hyperparameter optimization but is already extremely high for accuracy."
  },
  {
    "objectID": "posts/Probability_Theory_and_Random_Variables/index.html",
    "href": "posts/Probability_Theory_and_Random_Variables/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Background\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods\n\n\n\nTuning Techniques\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically."
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Background\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods\n\n\n\nTuning Techniques\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Background\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods\n\n\n\nTuning Techniques\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically."
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html",
    "href": "posts/Anomaly_Outlier_Detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "In this blog post, two popular outlier detection algorithms (DBSCAN and Isolation Forest) will be used and compared against each other. Outlier detection is important for preprocessing datasets as there is the potential that outliers can throw off the results of a machine learning model. By removing outliers, it is possible to get a more accurate result. These methods can also be used for anomaly detection in time series data, spacecraft telemetry for example, to identify potential out of ordinary trends in data. This can potentially allow for a preemptive response to an issue (ex: temperature changing, so heaters are powered on/off) and reduces the amount of time that the spacecraft is out of mission.\nFor simplicity, a known dataset will be imported and used with two features as the dataset. The breast cancer dataset from sklearn is a commonly used binary classification dataset from UC Irvine showing information of tumor characteristics in Wisconsin.\nFigure 1 shows a box and whisker plot of both sets of data for an initial look at the outliers identified through purely statistical means.\n\n\nCode\nred_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\nfig, axs = plt.subplots(len(df.columns),1, figsize=(8,6))\nfor i, ax in enumerate(axs.flat):\n    ax.boxplot(df.iloc[:,i], flierprops=red_circle,vert=False)\n    ax.set_title(df.columns[i])\n    ax.tick_params(axis='y', labelsize=14)\n  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Box/whisker plot of initial data"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Background\nWhen a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods\n\n\n\nTuning Techniques\nThe goal of finding optimal hyperparameters is to determine the best combination of hyperparameters that optimize the model. This can be done manually but is extremely time intensive. Thus, there are several solutions available to perform this tuning automatically."
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "href": "posts/Anomaly_Outlier_Detection/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "title": "Anomaly/Outlier Detection",
    "section": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nDBSCAN is a popular algorithm used mainly for clustering given sets of points. It works by grouping together points that are close together and have multiple nearest neighbors. It considers outliers to be points that are alone in low density regions. [1]\nFor the example in this blog, a two-dimensional set of data (mean radius and mean smoothness) is used to run through the DBSCAN algorithm. Figure 2 shows the result with outlier/anomalous data points shown in orange.\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \n\n# DBSCAN model training/fitting\ndbscan=DBSCAN()\ndbscan.fit(X_train[[\"mean radius\",\"mean smoothness\"]])\ncolors = dbscan.labels_\noutliers=colors.T&lt;0\nnormal=colors.T&gt;=0\nprint(\"Number of outliers detected: %d\" % sum(i&lt;0 for i in colors.T))\nprint(\"Number of normal samples detected: %d\" % sum(i&gt;=0 for i in colors.T))\n\nNumber of outliers detected: 8\nNumber of normal samples detected: 561\n\n\n\n\nCode\n#plotting\nfig, ax = plt.subplots(figsize=(9,7))\nplt.plot(X_train[\"mean radius\"][colors==0],X_train[\"mean smoothness\"][colors==0],marker='o',linestyle=\"None\")\nplt.plot(X_train[\"mean radius\"][colors==-1],X_train[\"mean smoothness\"][colors==-1],marker='o',linestyle=\"None\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.title(\"Dataset Outlier Detection via DBSCAN\")\nplt.show()\n\n\n\n\n\nFigure 2: Data that the DBSCAN algorithm identifies as an outlier or not"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#isolation-forest",
    "href": "posts/Anomaly_Outlier_Detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier Detection",
    "section": "Isolation Forest",
    "text": "Isolation Forest\nIsolation Forest is another commonly used outlier detection method which detects outliers utilizing binary trees. This method recursively partitions data points based on randomly selected attribute and then assigned anomaly scores based on number of “splits” needed to isolate a data point. The training dataset is used to build the “trees” and then the validation data is passed through those trees and assigned an anomaly score. In the case of this example, the training and validation is the same dataset (only one iteration of the call to the model). Based on the anomaly score, it can be determined which points are outliers. One of the inputs to the Isolation Forest algorithm is the contamination parameter, or the expected percentage of data that will be anomalous. For the purposes of the example, a contamination value of 3% will be used.\nAdvantages:\n\nLow memory utilization\nWorks best with large datasets\n\nNote: Below contains some modified code from HERE\n\nfrom sklearn.ensemble import IsolationForest\n\noutliers_fraction = float(.03)\nmodel =  IsolationForest(contamination=outliers_fraction)\ndata=df.values\nprediction= model.fit_predict(data)\n\nprint(\"Number of outliers detected: {}\".format(prediction[prediction &lt; 0].sum()*-1))\nprint(\"Number of normal samples detected: {}\".format(prediction[prediction &gt; 0].sum()))\n\nNumber of outliers detected: 18\nNumber of normal samples detected: 551\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(9,7))\nnormal_data = data[np.where(prediction &gt; 0)]\noutliers = data[np.where(prediction &lt; 0)]\nplt.scatter(normal_data[:, 0], normal_data[:, 1])\nplt.scatter(outliers[:, 0], outliers[:, 1])\nplt.title(\"Dataset Outlier Detection via Isolation Forest\")\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.show()\n\n\n\n\n\nFigure 3: Data that the Isolation Forest algorithm identifies as an outlier or not"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#conclusion",
    "href": "posts/Anomaly_Outlier_Detection/index.html#conclusion",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "It can be seen that both DBSCAN and Isolation Forest identified most of the same outlier points, but Isolation Forest identified 10 more total outlier data points, mainly due to the contamination parameter being large (3%)."
  }
]