[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\noutlier\n\n\nanomaly\n\n\ndetection\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\nlinear\n\n\nnonlinear\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nprobability\n\n\nrandom variables\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\n(Old blog post) Anomaly Detection on Spacecraft Telemetry\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\n(Old blog post) Hyperparameter Tuning\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\n(Old blog post) Data Preprocessing\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\ndata\n\n\npreprocessing\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nEric Jackson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "(Old blog post) Anomaly Detection on Spacecraft Telemetry",
    "section": "",
    "text": "Every spacecraft that is launched has some form of onboard anomaly responses for most known failure cases in order to safe the vehicle. Normally these are simple low/high limits set for certain monitors (temperature, voltage, etc) with a corresponding response, whether that be a simple visual alarm or powering off certain equipment.\nThe problem with anomalies in space is that they can be incredibly hard to predict, as multiple components can react slightly out of family to create a larger issue. Spacecraft will also generate a massive amount of data the longer they are on orbit, and manually looking and trending this data from a human standpoint can miss certain anomalies. But, this large amount of data makes them perfect for utilizing machine learning. Machine learning would allow for these anomalies to not only be identified, but also potentially predicted and prevented, allowing the spacecraft to stay in mission over potentially high priority targets (depending on the payload/mission). An automatic response to a predicted anomaly would limit both downtime and human interaction, as the investigation and implementation can take hours to days before returning to mission.\n\n\n\nExample of anomaly in telemetry data"
  },
  {
    "objectID": "posts/Topic_2/index.html",
    "href": "posts/Topic_2/index.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "When a machine learning model is being trained, one of the most important parts of the process is choosing valid hyperparameters. Hyperparameters are parameters whose value can be used to control the learning process, while other parameters are simply derived during the training process. These hyperparameters do not necessarily impact or influence the performance of the model, but impact the speed and quality of the learning process. As such, the hyperparameters are set prior to training the model and not during.\nExamples of common hyperparameters include:\n\nLearning rate and network size in Long short-term memory (LSTM)\nk in k-nearest neighbors\nThe penalty in most classifier methods"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About: Eric Jackson",
    "section": "",
    "text": "I am a current graduate student at Virginia Tech working on my MS in Aerospace Engineering. I also work full time at Lockheed Martin as a space systems engineer."
  },
  {
    "objectID": "posts/Topic_3/index.html",
    "href": "posts/Topic_3/index.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "Background\nFor any machine learning model, there is a set of data that will be input into it. Generally the data will be broken into multiple sets, consisting of training data and test data. The training data will be the portion or set of data that is used to train the model, and the test data is what the trained model is run on to produce results.\nBefore one can use datasets, it’s generally necessary to do some form of preprocessing to the raw data to ensure that the model can run efficiently and accurately. This can be as simple as removing NaN or Null values and as complex as performing statistical analysis to remove outliers and normalizing the data.\n\npopulating missing data\ndropping unnecessary data\nsplitting into training and testing\ndropping nan\nconverting categorical values into numerical\ndownsampling data\n\n\n\nData Preprocessing\nOne of the first steps in importing datasets is to drop any NaN or null values. These values will generally cause issues when running and machine learning model and are best to remove immediately. Luckily, there are several built in functions to perform this.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# import dataset \ndf=pd.read_csv('./WheelTemperature.csv')\ndf.isna().sum()\n\nDate     0\nHigh    17\ndtype: int64\n\ndfdrop=df.dropna()\ndfdrop.isna().sum()\n\nDate    0\nHigh    0\ndtype: int64"
  },
  {
    "objectID": "posts/post-with-code/index.html#preprocessing",
    "href": "posts/post-with-code/index.html#preprocessing",
    "title": "(Old blog post) Anomaly Detection on Spacecraft Telemetry",
    "section": "Preprocessing",
    "text": "Preprocessing\nMany spacecraft constellations have decades of on orbit telemetry available, but is mostly proprietary and not available for public use. LASP and NASA releases subsets of data to the public, and this is what was used for this blog post. Reaction wheel temperatures, battery temperatures and voltages, and total bus current datasets were used, each having 750,000+ samples of data over ~14.5 years. Because ARIMA requires the data to be stationary, each of the datasets is first sampled down to a daily mean which brings the size down to only 5346 data points. This will allow for much faster processing. To increase the number of exogenous features, each dataset is also turned into sets of rolling mean and rolling standard deviation in windows of 3, 7, and 30 days.\nUnfold the below code to see the setup of the data and how it is preprocessed as mentioned above.\nA visualization of the separation between training and test data can be seen in in Figure 1, with the first 75% used for training and the remaining 25% used for test data.\n\n\nCode\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n\n\n\n\n\n\n\nFigure 1: Training data is first 75% of wheel temperature data, with the remaining 25% used as the test data for verification"
  },
  {
    "objectID": "posts/post-with-code/index.html#truth-data",
    "href": "posts/post-with-code/index.html#truth-data",
    "title": "(Old blog post) Anomaly Detection on Spacecraft Telemetry",
    "section": "Truth Data",
    "text": "Truth Data\nIn order to determine the accuracy of the anomaly detection methods used below, a set of truth data points needed to be manually chosen. These points were identified to be points in time where an anomaly took place based on personal experience of operational spacecraft data. 115 out of 1137 total points were marked as anomalous. Figure 2 identifies the anomalies, marked in red.\n\n\nCode\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(9,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n\n\n\n\n\nFigure 2: Set of anomalous data points to be used as truth"
  },
  {
    "objectID": "posts/Topic_2/index.html#grid-search",
    "href": "posts/Topic_2/index.html#grid-search",
    "title": "Hyperparameter Tuning",
    "section": "Grid Search",
    "text": "Grid Search\nThe grid search method of optimizing hyperparameters works by running through each combination of parameters and choosing the combination with the highest score at the end.\nThe drawback of using grid search is that it is very time intensive due to the large number of combinations that it iterates through\n\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n            'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n            'kernel': ['rbf','linear']} \n\ngrid = GridSearchCV(SVC(), param_grid,refit = True, verbose = 1) \ngrid.fit(X_train, y_train) \ngrid_predictions = grid.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, grid_predictions)) \nprint('%s' % (grid.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,grid_predictions))\n\nFitting 5 folds for each of 50 candidates, totalling 250 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\n{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\nAccuracy = 0.992593\n\n\n\nAs seen in the table above, an accuracy of 99.259% was achieved with the hyperparameter values of c=1, gamma = 0.001, and kernel = rbf. This is a slight improvement from the standard parameters, around 0.5% increase in accuracy. As seen in the results section and in Figure 1, the shape of the grid search can be seen in the time it takes for each iteration, as it follows the same pattern for every set of combinations."
  },
  {
    "objectID": "posts/Topic_2/index.html#random-search",
    "href": "posts/Topic_2/index.html#random-search",
    "title": "Hyperparameter Tuning",
    "section": "Random Search",
    "text": "Random Search\nRandom search works similarly to grid search, but moves through it in a random fashion and only uses a fixed number of combinations. This allows for similar optimization results as the grid search in a fraction of the time. As seen in the results section and in Figure 1, the random search is ~6 times faster.\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomizedSearchCV(estimator=SVC(),param_distributions=param_grid,verbose=1)\nclf.fit(X_train, y_train) \nclf_predictions = clf.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, clf_predictions)) \nprint('%s' % (clf.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,clf_predictions))\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.95      0.96      0.95        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.95      0.97      0.96        61\n           5       0.98      0.98      0.98        59\n           6       1.00      0.96      0.98        46\n           7       1.00      0.96      0.98        56\n           8       0.93      0.97      0.95        59\n           9       0.98      0.98      0.98        48\n\n    accuracy                           0.98       540\n   macro avg       0.98      0.98      0.98       540\nweighted avg       0.98      0.98      0.98       540\n\n{'kernel': 'linear', 'gamma': 0.01, 'C': 1000}\nAccuracy = 0.977778\n\n\n\nBecause of the randomness of the random search, the optimal value(s) change every time this method is ran. For the iteration performed at the time of this blog post, the random search method determined optimal values worse than both the grid and Bayesian search methods."
  },
  {
    "objectID": "posts/Topic_2/index.html#bayesian-optimization",
    "href": "posts/Topic_2/index.html#bayesian-optimization",
    "title": "Hyperparameter Tuning",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\nBayesian optimization works differently than the other two commonly used methods mentioned above, it uses Bayes Theorem to find the minimum or maximum of an objective function. Because of this difference, not all parameter values are used and a fixed number of hyperparameter combinations are iterated through (default number of iterations is 50). Bayesian optimization is usually used to optimize expensive to evaluate functions, which is not necessarily the case for this example as the data being used is somewhat basic for the purposes of this blog post.\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\nfrom skopt import BayesSearchCV\n\nbayes = BayesSearchCV(SVC(), param_grid) \nbayes.fit(X_train, y_train) \nbayes_predictions = bayes.predict(X_test) \n\n# print classification report \nprint(classification_report(y_test, bayes_predictions)) \nprint('%s' % (bayes.best_params_))\nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,bayes_predictions))\nac3=accuracy_score(y_test,bayes_predictions)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       1.00      0.98      0.99        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       1.00      0.97      0.98        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nOrderedDict([('C', 1.0), ('gamma', 0.001), ('kernel', 'rbf')])\nAccuracy = 0.992593\n\n\n\nIt is seen that utilizing a Bayesian search optimization identifies the same optimal values as the grid search, c = 1, gamma = 0.001, and kernel = rbf, with an accuracy of 99.259%. This is once again slightly higher than the baseline method by around 0.5%"
  },
  {
    "objectID": "posts/Topic_2/index.html#baseline-data",
    "href": "posts/Topic_2/index.html#baseline-data",
    "title": "Hyperparameter Tuning",
    "section": "Baseline Data",
    "text": "Baseline Data\nFor the purposes of this blog, a basic Support Vector Machine (SVM) classification model will be trained with a built in dataset from the sklearn library. This will give a baseline accuracy and results to compare when attempting to optimize the hyperparameters. The hyperparameters that are used by default with this algorithm are:\n\nC - Regularization parameter. The penalty is a squared l2 penalty [default = 1.0]\nKernel - Kernel type used in algorithm [default = ‘rbf’]\n\nNote: Linear and RBF kernels were used as the possible options in this example\n\nGamma - Kernel coefficient [default = ‘scale’, or 1 / (n_features * X)]\n\n\n\nCode\n# Setting up basic SVM to compare with optimized hyperparameters. Using built in dataset of data\nimport pandas as pd \nimport numpy as np \nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.datasets import load_breast_cancer, load_digits\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split \n\ncancer = load_breast_cancer() \ncancer=load_digits()\ndf_feat = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndf_target = pd.DataFrame(cancer['target'], columns =['Cancer']) \nX_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target),test_size = 0.30, random_state = 101) \nmodel = SVC() \nmodel.fit(X_train, y_train) \n\n# print prediction results \npredictions = model.predict(X_test) \nprint(classification_report(y_test, predictions)) \nprint('Accuracy = %3.6f\\n' % accuracy_score(y_test,predictions))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99        53\n           1       0.96      1.00      0.98        55\n           2       1.00      1.00      1.00        49\n           3       1.00      1.00      1.00        54\n           4       0.98      0.97      0.98        61\n           5       1.00      0.98      0.99        59\n           6       1.00      1.00      1.00        46\n           7       1.00      1.00      1.00        56\n           8       0.98      0.97      0.97        59\n           9       0.96      1.00      0.98        48\n\n    accuracy                           0.99       540\n   macro avg       0.99      0.99      0.99       540\nweighted avg       0.99      0.99      0.99       540\n\nAccuracy = 0.988889\n\n\n\nIt is seen that the baseline model performs with an accuracy of 98.89% using C = 1, kernel = rbf, gamma = scale (or around 0.015). This will be compared against with several methods of hyperparameter optimization but is already extremely high for accuracy."
  },
  {
    "objectID": "posts/Probability_Theory_and_Random_Variables/index.html",
    "href": "posts/Probability_Theory_and_Random_Variables/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Probability Theory & Random Variables\nMathematics has many broad topics, but one of the most prevalent topics in machine learning is probability. Probability theory contains topics such as discrete and continuous random variables, probability distributions, and statistics.\nOne of the more realistic machine learning based scenarios to utilize probability theory methods on is random data. As seen in Figure 1, a set of 5 “blobs” has been generated with random centers and varying degrees of standard deviations from said center. This was the same approach taken for my blog post on Clustering.\n\n\nSetup code\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nimport scipy.stats\nfrom scipy.stats import norm\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n\n\nblob_centers=np.random.uniform(0,5,[5,2])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nx, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,random_state=7)\nn_bins=75\ndata=x[:,1]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\n\n\n\nPlotting original data code\nfig, axs = plt.subplots(figsize =(9, 6))\nplt.scatter(x[:,0],x[:,1])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n\n\n\n\n\nFigure 1: Plot of random data contained to five blobs\n\n\n\n\nWhen working with non-normal or non-uniform data, it is hard to fit a normal distribution curve to it. Utilizing a kernel density estimation (KDE) is one way to smooth as well as estimate the probability density function (PDF) of a random variable based on kernels as weights. The kernel density estimator is seen in the below equation:\n\nAs seen in Figure 2, the plot of the kde function on the above data closely follows the trend of the histogram for both x1 and x2 variables, which were generated randomly.\n\n\nPlotting kde and histogram code\nfig, axs = plt.subplots(2,1,figsize =(9, 7))\nax1=plt.subplot(211)\nax1.hist(data, n_bins, alpha=0.5,density=1,label='x1 data',edgecolor='black');\nax1.plot(t_range,kde(t_range),lw=2, label='x1 kde')\nplt.xlim(x.min()-.5,x.max()+.5)\nax1.legend(loc='best')\nax2=plt.subplot(212)\ndata=x[:,0]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\nax2.hist(data, n_bins, alpha=0.5,density=1,label='x2 data',edgecolor='black');\nax2.plot(t_range,kde(t_range),lw=2, label='x2 kde')\n\nax2.legend(loc='best')\nplt.xlim(x.min()-.5,x.max()+.5)\nplt.show()\n\n\n\n\n\nFigure 2: Histogram and KDE of data\n\n\n\n\nMachine learning techniques commonly will use clustering to group data points together and allows the user to see the similarity of their data. One algorithm that is used for clustering is Gaussian Mixtures Model (GMM) which uses probability for clustering and density estimation, and is based on Gaussian distribution curves. Since this blog post is about probability theory, we will utilize this specific method.\nSince Gaussian distributions heavily depend on mean and variance of each point, GMM utilizes a statistical algorithm called Expectation-Maximization for calculating the mean and variance value of each Gaussian or cluster. The algorithm first calculates the probability that a point belongs to each cluster, then iterates the mean and covariance matrix to maximize the log likelihood value.\n\ngm = GaussianMixture(n_components=5, n_init=10, random_state=42)\ngm.fit(x)\nprint('Gaussian Mixture model converged in %d iterations with a lower bound\\non the log likelihood of the best fit of EM of %3.3f' % (gm.n_iter_,gm.lower_bound_))\nlabels=gm.predict(x)\n\nfig, axs = plt.subplots(figsize =(9, 6))\nplt.scatter(x[:,0][labels==0],x[:,1][labels==0])\nplt.scatter(x[:,0][labels==1],x[:,1][labels==1])\nplt.scatter(x[:,0][labels==2],x[:,1][labels==2])\nplt.scatter(x[:,0][labels==3],x[:,1][labels==3])\nplt.scatter(x[:,0][labels==4],x[:,1][labels==4])\nplt.legend(['Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5'],loc='best')\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n\nGaussian Mixture model converged in 19 iterations with a lower bound\non the log likelihood of the best fit of EM of -0.659\n\n\n\n\n\nFigure 3: Gaussian Mixture clustering results\n\n\n\n\nThe Gaussian Mixture function from sklearn allows one to see the probabilities that a certain point is in each of the 5 clusters (in this example). Below, you can see a table with the probabilities of several datapoints and their respective clusters. It’s interesting to note that not every point is 100% certain with this algorithm, there are several points that have &lt; 1.0 probabilities, meaning that it might have a 95% probability of it being in one cluster, and a 5% probability of being in another cluster. Since 95% &gt; 5%, it assumes it is in the higher probability cluster.\n\npd.DataFrame(gm.predict_proba(x).round(3),columns=['Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5'])\n\n\n\n\n\n\n\n\nCluster 1\nCluster 2\nCluster 3\nCluster 4\nCluster 5\n\n\n\n\n0\n0.980\n0.000\n0.000\n0.000\n0.020\n\n\n1\n0.000\n1.000\n0.000\n0.000\n0.000\n\n\n2\n0.000\n0.000\n0.000\n0.000\n1.000\n\n\n3\n0.000\n1.000\n0.000\n0.000\n0.000\n\n\n4\n0.000\n0.997\n0.003\n0.000\n0.000\n\n\n...\n...\n...\n...\n...\n...\n\n\n1995\n0.934\n0.000\n0.000\n0.000\n0.066\n\n\n1996\n0.000\n1.000\n0.000\n0.000\n0.000\n\n\n1997\n0.000\n0.000\n0.000\n0.927\n0.073\n\n\n1998\n0.000\n0.000\n0.000\n0.286\n0.714\n\n\n1999\n0.000\n0.982\n0.018\n0.000\n0.000\n\n\n\n\n2000 rows × 5 columns\n\n\n\nSources used for code and/or text:\n[1] https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/#:~:text=A.-,The%20Gaussian%20Mixture%20Model%20(GMM)%20is%20a%20probabilistic%20model%20used,distributions%2C%20each%20representing%20a%20cluster.\n[2] https://github.com/maptv/handson-ml3/blob/b8f4fd1e85247096109b175d3289b558cedc74b4/09_unsupervised_learning.ipynb\n[3] https://en.wikipedia.org/wiki/Probability_theory"
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression analysis is a broad and useful field of statistical analysis, and for the purpose of this blog, only linear and nonlinear regression will be discuessed. Both linear and nonlinear regression have many practical uses and are used to estimate the relationships between two or more variables, using either lines or curves. The main use of linear or nonlinear regression are for predicting or forecasting data and thus, is used widely in machine learning situations. Regression analysis is also used to infer casual relationships between variables, otherwise called correlation. [1]"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is similar to clustering (as mentioned in this blog post) in the sense that they group data together into separate categories. The difference comes from the fact that classification has a predefined set of labels that are attached to each data point, and in clustering the labels are missing and the algorithm will apply those labels/groupings. These two topics are commonly referenced as supervised and unsupervised learning (classification vs clustering).\nSome of the downsides of classification are that there is a need to train the model before using it with test data, whereas clustering does not require such training and can group data points together immediately. But, classification can be used for much more intensive scenarios, such as handwriting recognition and spam filtering.\nAs mentioned before, classification algorithms or models require inputs with labels predefined. For this blog post, a native sklearn dataset containing measurements of a species of flower will be used. As seen below, there are width and length measurements along with the label/target class for the species of flower.\n\n\nSetup/imports\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.datasets import load_iris\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n\n\niris=load_iris()\ndf = pd.DataFrame(iris['data'],columns = iris['feature_names'])\ntarget=pd.DataFrame(iris['target'])\ntarget[target==0]=iris.target_names[0]\ntarget[target==1]=iris.target_names[1]\ntarget[target==2]=iris.target_names[2]\ndf['target']=target\n\n# breaking up data into training and test datasets\nx_train=df[['petal length (cm)', 'petal width (cm)']]\ny_train=df['target']\nx_test=df[['petal length (cm)', 'petal width (cm)']]\ny_test=df['target']\n\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\nBefore the data is to be run through any classification algorithms, it is helpful to first plot the data and review what to expect. Figure 1 shows the dataset visually with each color representing a different label or species of flower.\n\n\nPlotting code\nfig, axs = plt.subplots(figsize =(7, 5))\nplt.scatter(df['petal length (cm)'][df['target']==iris.target_names[0]],df['petal width (cm)'][df['target']==iris.target_names[0]])\nplt.scatter(df['petal length (cm)'][df['target']==iris.target_names[1]],df['petal width (cm)'][df['target']==iris.target_names[1]])\nplt.scatter(df['petal length (cm)'][df['target']==iris.target_names[2]],df['petal width (cm)'][df['target']==iris.target_names[2]])\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.show()\n\n\n\n\n\nFigure 1: Dataset values broken into their defined target labels"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html",
    "href": "posts/Anomaly_Outlier_Detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "In this blog post, two popular outlier detection algorithms (DBSCAN and Isolation Forest) will be used and compared against each other. Outlier detection is important for preprocessing datasets as there is the potential that outliers can throw off the results of a machine learning model. By removing outliers, it is possible to get a more accurate result. These methods can also be used for anomaly detection in time series data, spacecraft telemetry for example, to identify potential out of ordinary trends in data. This can potentially allow for a preemptive response to an issue (ex: temperature changing, so heaters are powered on/off) and reduces the amount of time that the spacecraft is out of mission.\nFor simplicity, a known dataset will be imported and used with two features as the dataset. The breast cancer dataset from sklearn is a commonly used binary classification dataset from UC Irvine showing information of tumor characteristics in Wisconsin.\nFigure 1 shows a box and whisker plot of both sets of data for an initial look at the outliers identified through purely statistical means.\n\n\nSetup and plotting of initial data\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \ndata=X_train[[\"mean radius\",\"mean smoothness\"]]\ndf=data;\n\nred_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\nfig, axs = plt.subplots(len(df.columns),1, figsize=(8,6))\nfor i, ax in enumerate(axs.flat):\n    ax.boxplot(df.iloc[:,i], flierprops=red_circle,vert=False)\n    ax.set_title(df.columns[i])\n    ax.tick_params(axis='y', labelsize=14)\n  \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Box/whisker plot of initial data"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a technique commonly used in statistics and machine learning to group similar groups of data together. This allows for things like data compression, optimization of processes, and identification of oddities in data to be accomplished much easier than normal. Each of the clusters identified will share common traits, decided by the specific algorithm. For the purposes of this blog post, a random set of blob data was generated and used with 5 clusters visually obvious.\nFigure 1 shows the original data and the clusters identified as truth. In order to keep the same results throughout iterations of making this blog post, a random seed (170) was selected so the same results can be reproduced, while still being randomized. The results from the below algorithms will reference and be compared against this truth data.\n\n\nSetup and imports\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs,make_circles\nfrom sklearn import cluster \nfrom sklearn.cluster import KMeans,DBSCAN\nfrom itertools import cycle, islice\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom pandas.plotting import table\n\n\n\nnoisy_circles = make_circles(n_samples=2500, factor=0.5, noise=0.05)\nx=noisy_circles[0]\ny=noisy_circles[1]\nnp.random.seed(170) \nblob_centers=np.random.uniform(0,5,[5,2])\nblob_std = np.array([.3, .3, 0.4, .3 ,.1])\nx,y=make_blobs(n_samples=2500, cluster_std=blob_std,centers=blob_centers,random_state=170)\n# plotting original data\nfig, ax = plt.subplots(figsize=(6,4))\nplt.scatter(x[:,0],x[:,1],c=y)\nplt.title('Original Data')\nplt.show()\n\n\n\n\nFigure 1: Original data with true clusters identified"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "href": "posts/Anomaly_Outlier_Detection/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "title": "Anomaly/Outlier Detection",
    "section": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nDBSCAN is a popular algorithm used mainly for clustering given sets of points. It works by grouping together points that are close together and have multiple nearest neighbors. It considers outliers to be points that are alone in low density regions. [1]\nFor the example in this blog, a two-dimensional set of data (mean radius and mean smoothness) is used to run through the DBSCAN algorithm. Figure 2 shows the result with outlier/anomalous data points shown in orange. Since DBSCAN is most known for being a clustering algorithm, the bottom plot shows the different clusters that the algorithm identified. All points with a color and a star indicate a clustered point, of which there are two separate clusters that the algorithm identified. Any point with a red X indicates it is an anomalous/outlier point, and any point that is not surrounded by color and is a period is a non-core point (of which there were only two single points identified as such).\nNote: The default epsilon parameter value of 0.5 was used for the DBSCAN algorithm. This parameter “defines the maximum distance between two samples for one to be considered as in the neighborhood of the other”. [2]\n\n\nplot_dbscan function\n# plot code taken from https://github.com/maptv/handson-ml3/blob/main/09_unsupervised_learning.ipynb\ndef plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n    core_mask[dbscan.core_sample_indices_] = True\n    anomalies_mask = dbscan.labels_ == -1\n    non_core_mask = ~(core_mask | anomalies_mask)\n\n    cores = dbscan.components_\n    anomalies = X[anomalies_mask]\n    non_cores = X[non_core_mask]\n    \n    plt.scatter(cores[:, 0], cores[:, 1],\n                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20,\n                c=dbscan.labels_[core_mask])\n    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n                c=\"r\", marker=\"x\", s=100)\n    plt.scatter(non_cores[:, 0], non_cores[:, 1],\n                c=dbscan.labels_[non_core_mask], marker=\".\")\n    if show_xlabels:\n        plt.xlabel(\"Mean Radius\")\n    else:\n        plt.tick_params(labelbottom=False)\n    if show_ylabels:\n        plt.ylabel(\"Mean Smoothness\")\n    else:\n        plt.tick_params(labelleft=False)\n    plt.title(f\"eps={dbscan.eps:.2f}, min_samples={dbscan.min_samples}\")\n    plt.grid()\n    plt.gca().set_axisbelow(True)\n    plt.show()\n\n\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import load_breast_cancer\n\n# using imported breast cancer dataset from sklearn\ncancer = load_breast_cancer() \nX_train = pd.DataFrame(cancer['data'],columns = cancer['feature_names']) \n\n# DBSCAN model training/fitting\ndbscan=DBSCAN()\ndbscan.fit(X_train[[\"mean radius\",\"mean smoothness\"]])\ncolors = dbscan.labels_\noutliers=colors.T&lt;0\nnormal=colors.T&gt;=0\nprint(\"Number of outliers detected: %d\" % sum(i&lt;0 for i in colors.T))\nprint(\"Number of normal samples detected: %d\" % sum(i&gt;=0 for i in colors.T))\n\nNumber of outliers detected: 8\nNumber of normal samples detected: 561\n\n\n\n\nPlotting code\n#plotting\nfig, ax = plt.subplots(figsize=(9,14))\nplt.subplot(211)\nplt.plot(X_train[\"mean radius\"][colors==0],X_train[\"mean smoothness\"][colors==0],marker='o',linestyle=\"None\")\nplt.plot(X_train[\"mean radius\"][colors!=0],X_train[\"mean smoothness\"][colors!=0],marker='o',linestyle=\"None\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.title(\"Dataset Outlier Detection via DBSCAN\")\n\nplt.subplot(212)\nplot_dbscan(dbscan, X_train[[\"mean radius\",\"mean smoothness\"]].values, size=100)\n\nplt.show()\n\n\n\n\n\nFigure 2: Data that the DBSCAN algorithm identifies as an outlier or not"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#isolation-forest",
    "href": "posts/Anomaly_Outlier_Detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier Detection",
    "section": "Isolation Forest",
    "text": "Isolation Forest\nIsolation Forest is another commonly used outlier detection method which detects outliers utilizing binary trees. This method recursively partitions data points based on randomly selected attribute and then assigned anomaly scores based on number of “splits” needed to isolate a data point. The training dataset is used to build the “trees” and then the validation data is passed through those trees and assigned an anomaly score. In the case of this example, the training and validation is the same dataset (only one iteration of the call to the model). Based on the anomaly score, it can be determined which points are outliers. One of the inputs to the Isolation Forest algorithm is the contamination parameter, or the expected percentage of data that will be anomalous. For the purposes of the example, the default contamination value of 10% will be used.\nAdvantages:\n\nLow memory utilization\nWorks best with large datasets\n\nNote: Below contains some modified code from HERE\n\nfrom sklearn.ensemble import IsolationForest\n\noutliers_fraction = float(.03)\nmodel =  IsolationForest()\ndata=df.values\nprediction = model.fit_predict(data)\n\nprint(\"Number of outliers detected: {}\".format(prediction[prediction &lt; 0].sum()*-1))\nprint(\"Number of normal samples detected: {}\".format(prediction[prediction &gt; 0].sum()))\n\nNumber of outliers detected: 102\nNumber of normal samples detected: 467\n\n\n\n\nPlotting code\nfig, ax = plt.subplots(figsize=(9,7))\nnormal_data = data[np.where(prediction &gt; 0)]\noutliers = data[np.where(prediction &lt; 0)]\nplt.scatter(normal_data[:, 0], normal_data[:, 1])\nplt.scatter(outliers[:, 0], outliers[:, 1])\nplt.title(\"Dataset Outlier Detection via Isolation Forest\")\nplt.ylabel(\"Mean Smoothness\")\nplt.xlabel(\"Mean Radius\")\nplt.legend([\"Valid Data\",\"Data marked as outliers\"])\nplt.show()\n\n\n\n\n\nFigure 3: Data that the Isolation Forest algorithm identifies as an outlier or not"
  },
  {
    "objectID": "posts/Anomaly_Outlier_Detection/index.html#conclusion",
    "href": "posts/Anomaly_Outlier_Detection/index.html#conclusion",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "It can be seen that both DBSCAN and Isolation Forest identified most of the same outlier points, but Isolation Forest identified 10 more total outlier data points, mainly due to the contamination parameter being large (3%)."
  },
  {
    "objectID": "posts/Clustering/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "href": "posts/Clustering/index.html#density-based-spatial-clustering-of-applications-with-noise-dbscan",
    "title": "Clustering",
    "section": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nAs explored in the blog post on anomaly/outlier detection, the DBSCAN algorithm is a widely used machine learning algorithm for clustering given sets of points by grouping together points that are close together and have multiple nearest neighbors. While creating the clusters of the data, it will naturally identify outliers and thus, is a great algorithm to accomplish both outlier detection and clustering.\nNote: An epsilon value of 0.2 was used for the DBSCAN algorithm. This parameter “defines the maximum distance between two samples for one to be considered as in the neighborhood of the other”. [1]\n\ndbscan=DBSCAN(eps=0.2)\ndbscan.fit(x)\ncolors = dbscan.labels_\ny_pred_dbscan=colors"
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html#linear-regression",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html#linear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is a technique that uses a straight line to model the relationship between two or more variables. There are several models that can be used for linear regression, with the most common one being the least squares method. Linear regression can also be performed by minimizing the lack of fit or by minimizing a version of the least squares cost function (ridge regression and lasso) [1]. The LinearRegression model from the sklearn library utilizes the least squares method to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation [2].\nA dataset native to the sklearn library was used for this blog post (load_wine) and two features were chosen at random based on how linear the trend appeared. As can be seen in Figure 1, the data appears random with a slight positive trend. The linear regression model was overlayed on top of the original data with a red line, and can be seen to match the positive slope of the data very closely. It also sits roughly in the middle of the data throughout, and leads to an RMSE of 2.403.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nimport math\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\ny=np.array(y.values).reshape(-1,1)/100\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(x,y)\n\ntest_predictions = model.predict(x)\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nMAE = mean_absolute_error(y,test_predictions)\nMSE = mean_squared_error(y,test_predictions)\nRMSE = np.sqrt(MSE)\nprint('RMSE: %3.3f' % RMSE)\n\nRMSE: 2.403\n\n\n\n\nPlotting code\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y)\nplt.plot(x,test_predictions,color=\"red\")\nplt.legend(['Data','Linear Prediction'])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n\n\n\n\n\nFigure 1: Linear regression model on data"
  },
  {
    "objectID": "posts/Linear_and_Nonlinear_Regression/index.html#nonlinear-regression",
    "href": "posts/Linear_and_Nonlinear_Regression/index.html#nonlinear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\nNonlinear regression is similar to linear regression, but uses a curve to fit the data, compared to a straight line with linear regression. Most problems or data in the real world are not linear in nature, so nonlinear regression allows for a much greater accuracy when analyzing items.\nThe dataset used for the above linear regression section was used and slightly manipulated to create a more nonlinear quadratic shape. This was done to more easily show the difference between linear and nonlinear regression.\nAfter fitting lines of best fit for varying degrees of polynomials (see Figure 2), it can be seen that, for this nonlinear dataset, the linear regression line of best fit is a very poor fit and only intercepts the data twice. The nonlinear regression fits of degrees 2 and 3 fit much closer to the original data, with degree = 3 fitting the best. The table below (Figure 3) shows the R squared scores of each regression fit, with degree 3 having a perfect score of 1 and degree of 2 nearly perfect at 0.997. Both of the polynomial regression lines visually can be seen to fit closely to the data, and the linear regression fit having a much lower score of 0.697 is not surprising.\n\n\nImports and dataset setup\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_wine\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndf=load_wine()\ndf=pd.DataFrame(df['data'],columns=df['feature_names'])\ndf=df[['alcohol','proline']]\nx=df.alcohol\ny=df.proline\nx=np.array(x.values).reshape(-1,1)\nxp=x\ny=np.array(y.values).reshape(-1,1)/100\nyp=y\n\npolynomial_converter = PolynomialFeatures(degree=2,include_bias=False)\npoly_features = polynomial_converter.fit_transform(y)\n\nx=poly_features[:,0].reshape(-1,1)\ny=poly_features[:,1].reshape(-1,1)*y/100\n\nx[:,0].sort()\ny[:,0].sort()\n\n\n\n# Linear regression\npolybig_features = PolynomialFeatures(degree=1, include_bias=False)\nstd_scaler = StandardScaler()\nlin_reg = LinearRegression()\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin = polynomial_regression.predict(X_newlin)\n\n# Polynomial regression with degree = 2\npolybig_features = PolynomialFeatures(degree=2, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_new = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbig = polynomial_regression.predict(X_new)\n\n# Polynomial regression with degree = 3\npolybig_features = PolynomialFeatures(degree=3, include_bias=False)\npolynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\npolynomial_regression.fit(x, y)\nX_newlin3 = np.linspace(min(x), max(x), len(x)).reshape(len(x), 1)\ny_newbiglin3 = polynomial_regression.predict(X_new)\n\n\n\nPlotting code\nfrom pandas.plotting import table\nfig, ax = plt.subplots(figsize=(8,6))\nplt.scatter(x,y,marker='o')\nplt.plot(X_newlin, y_newbiglin,color='green')\nplt.plot(X_new, y_newbig,color='red')\nplt.plot(X_newlin3, y_newbiglin3,color='black')\nplt.legend(['Data','Linear fit','Polynomial fit (deg = 2)','Polynomial fit (deg = 3)'])\nplt.show()\n\n\n\n\n\nFigure 2: Polynomial/Nonlinear regression model on data\n\n\n\n\n\n\nTable code\nlinscore=polynomial_regression.score(X_newlin,y_newbiglin)\nlinscore2=polynomial_regression.score(X_new,y_newbig)\nlinscore3=polynomial_regression.score(X_newlin3,y_newbiglin3)\n\ndf=pd.DataFrame([[linscore,linscore2,linscore3]],columns=['Linear Regression','Polynomial Regression (deg = 2)','Polynomial Regression (deg = 3)'],index=['R squared value','R^21','R^22'])\nfix, ax = plt.subplots(figsize=(8,1))\nax.axis('off')\ntable(ax,df.transpose()['R squared value'],loc='top',cellLoc='center',colWidths=list([.6, .6]))\nplt.show()\n\n\n\n\n\nFigure 3: R squared values for each regression model"
  },
  {
    "objectID": "posts/Probability_Theory_and_Random_Variables/index.html#probability-theory",
    "href": "posts/Probability_Theory_and_Random_Variables/index.html#probability-theory",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "One of the more realistic machine learning based scenarios to utilize probability theory methods on is random data. As seen in Figure 1, a set of 5 “blobs” are generated with random centers and varying degrees of standard deviations from said center. This was the same approach taken for my blog post on Clustering.\n\n\nSetup code\nimport numpy as np\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nimport scipy.stats\nfrom scipy.stats import norm\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n\n\nblob_centers=np.random.uniform(0,5,[5,2])\nblob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\nx, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,random_state=7)\nn_bins=75\ndata=x[:,1]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\n\n\n\nPlotting original data code\nfig, axs = plt.subplots(figsize =(9, 6))\nplt.scatter(x[:,0],x[:,1])\nplt.xlabel('$x_{1}$')\nplt.ylabel('$x_{2}$')\nplt.show()\n\n\n\n\n\nFigure 1: Plot of random data contained to five blobs\n\n\n\n\nWhen working with non-normal or non-uniform data, it is hard to fit a normal distribution curve to it. Utilizing a kernel density estimation (KDE) is one way to smooth as well as estimate the probability density function (PDF) of a random variable based on kernels as weights. The kernel density estimator is seen in the below equation:\n\nAs seen in Figure 2, the plot of the kde function on the above data closely follows the trend of the histogram for both x1 and x2 variables, which were generated randomly.\n\n\nPlotting kde and histogram code\nfig, axs = plt.subplots(2,1,figsize =(9, 7))\nax1=plt.subplot(211)\nax1.hist(data, n_bins, alpha=0.5,density=1,label='x1 data',edgecolor='black');\nax1.plot(t_range,kde(t_range),lw=2, label='x1 kde')\nplt.xlim(x.min()-.5,x.max()+.5)\nax1.legend(loc='best')\nax2=plt.subplot(212)\ndata=x[:,0]\nkde = stats.gaussian_kde(data,bw_method=None)\nt_range=np.linspace(min(data),max(data),len(data))\nax2.hist(data, n_bins, alpha=0.5,density=1,label='x2 data',edgecolor='black');\nax2.plot(t_range,kde(t_range),lw=2, label='x2 kde')\n\nax2.legend(loc='best')\nplt.xlim(x.min()-.5,x.max()+.5)\nplt.show()\n\n\n\n\n\nFigure 2: Histogram and KDE of data"
  },
  {
    "objectID": "posts/Classification/index.html#classification-algorithms",
    "href": "posts/Classification/index.html#classification-algorithms",
    "title": "Classification",
    "section": "",
    "text": "Naive Bayes\nGMM\nsome neural net?\nhttps://www.pycodemates.com/2022/05/iris-dataset-classification-with-python.html"
  },
  {
    "objectID": "posts/Classification/index.html#gaussian-naive-bayes",
    "href": "posts/Classification/index.html#gaussian-naive-bayes",
    "title": "Classification",
    "section": "Gaussian Naive Bayes",
    "text": "Gaussian Naive Bayes\nThe Gaussian Naive Bayes algorithm from the sklearn library (see here) utilizes Bayes’ theorem assuming the features probability is distributed in a Gaussian or normal fashion, with the variance and mean of each data point calculated for each class. There are a multitude of Naive Bayes classifier algorithms (Bernoulli, Multinomial, etc), but the specific Gaussian probabilistic version used here is especially useful when the values are continuous and expected to follow a Gaussian distribution.\nAs seen from Figure 2, the algorithm uses circular decision boundaries to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 6 points leading to a mean accuracy of 96%.\n\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n\nMean accuracy score: 96.000%\n\n\n\n\nPlotting code\n# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nx1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))\nx2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))\nxx, yy = np.meshgrid(x1grid, x2grid)\nr1, r2 = xx.flatten(), yy.flatten()\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\ngrid = np.hstack((r1,r2))\ny_pred_grid=nb.predict(grid)\nzz = y_pred_grid.reshape(xx.shape)\nzz[zz=='setosa']=1\nzz[zz=='versicolor']=2\nzz[zz=='virginica']=3\nxx=np.array(xx,dtype=float)\nyy=np.array(yy,dtype=float)\nzz=np.array(zz,dtype=float)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.contourf(xx, yy, zz,cmap='RdBu_r')\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\nplt.tight_layout()  \nplt.show()\n\n\n\n\n\nFigure 2: Gaussian Naive Bayes classification results (with decision boundaries) and confusion matrix"
  },
  {
    "objectID": "posts/Classification/index.html#gaussian",
    "href": "posts/Classification/index.html#gaussian",
    "title": "Classification",
    "section": "Gaussian",
    "text": "Gaussian\n\n# Gaussian Process\nnb = GaussianProcessClassifier()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n\nMean accuracy score: 96.667%\n\n\n\n\nPlotting code\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]])\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]])\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]])\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\n#cmd.plot()\nplt.tight_layout()  \nplt.show()\n\n\n\n\n\nFigure 3: Gaussian Process Classifier classification results with confusion matrix"
  },
  {
    "objectID": "posts/Classification/index.html#randomforestclassifier",
    "href": "posts/Classification/index.html#randomforestclassifier",
    "title": "Classification",
    "section": "RandomForestClassifier",
    "text": "RandomForestClassifier\n\n# Random Forest\nnb = RandomForestClassifier()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n\nMean accuracy score: 99.333%\n\n\n\n\nPlotting code\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]])\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]])\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]])\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\n#cmd.plot()\nplt.tight_layout()  \nplt.show()\n\n\n\n\n\nFigure 4: Random Forest Classifier classification results with confusion matrix"
  },
  {
    "objectID": "posts/Classification/index.html#gaussian-process-classification",
    "href": "posts/Classification/index.html#gaussian-process-classification",
    "title": "Classification",
    "section": "Gaussian Process Classification",
    "text": "Gaussian Process Classification\nThe Gaussian Process Classification algorithm from the sklearn library (see here) uses a general form of the Gaussian probability distribution model and is based on Laplace approximation. As it uses Gaussian probability, the model can compute confidence intervals and determine if refitting of a certain section is required based on probability alone. The algorithm is kernel based, meaning multiple types of covariance functions can be utilized and used to optimize model fitting based on the input data. One downside of this algorithm is that is loses efficiency when the number of features grows larger than a few dozen.\nAs seen from Figure 3, the algorithm uses more linear decision boundaries compared to the above Gaussian Naive to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 5 points leading to a mean accuracy of 96.667%. Both of these algorithms are Gaussian in nature, so it is expected that they have similar results.\n\nnb = GaussianProcessClassifier()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n\nMean accuracy score: 96.667%\n\n\n\n\nPlotting code\n# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nx1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))\nx2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))\nxx, yy = np.meshgrid(x1grid, x2grid)\nr1, r2 = xx.flatten(), yy.flatten()\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\ngrid = np.hstack((r1,r2))\ny_pred_grid=nb.predict(grid)\nzz = y_pred_grid.reshape(xx.shape)\nzz[zz=='setosa']=1\nzz[zz=='versicolor']=2\nzz[zz=='virginica']=3\nxx=np.array(xx,dtype=float)\nyy=np.array(yy,dtype=float)\nzz=np.array(zz,dtype=float)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.contourf(xx, yy, zz,cmap='RdBu_r')\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\nplt.tight_layout()  \nplt.show()\n\n\n\n\n\nFigure 3: Gaussian Process Classifier classification results (with decision boundaries) and confusion matrix"
  },
  {
    "objectID": "posts/Classification/index.html#random-forest-classifier",
    "href": "posts/Classification/index.html#random-forest-classifier",
    "title": "Classification",
    "section": "Random Forest Classifier",
    "text": "Random Forest Classifier\nThe Random Forest Classifier as part of the sklearn library (see here) uses decision tree classifiers on multiple parts of the dataset, and then averages scores from each one to maintain a high accuracy and attempt to prevent over fitting. The default arguments were used for this algorithm, but the most important one to call out and be aware of is the number of trees/estimators in the forest is defaulted to 100. Random Forest has the additional benefit of being less computationally expensive as other classification models, such as neural networks.\nAs seen from Figure 4, the algorithm uses boxier decision boundaries than the previous two algorithms to classify each set of labels. The confusion matrix on the right shows that out of 150 samples, it only incorrectly labeled 1 point leading to a mean accuracy of 99.333%. This is by far the best accuracy score of the three algorithms and would be the best model to use for this specific dataset for further work.\n\nnb = RandomForestClassifier()\nnb.fit(x_train, y_train)\nprint('Mean accuracy score: %3.3f%%' % (nb.score(x_test,y_test)*100))\ny_pred=nb.predict(x_train)\ncm=confusion_matrix(y_train,y_pred,labels=iris.target_names)\n\nMean accuracy score: 99.333%\n\n\n\n\nPlotting code\n# decision boundary code adapted from https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\nfig, axs = plt.subplots(1,2,figsize =(8, 5))\nplt.subplot(121)\nx1grid = np.linspace(x_train['petal length (cm)'].min()-.2, x_train['petal length (cm)'].max()+.2, len(x_train))\nx2grid = np.linspace(x_train['petal width (cm)'].min()-.2, x_train['petal width (cm)'].max()+.2, len(x_train))\nxx, yy = np.meshgrid(x1grid, x2grid)\nr1, r2 = xx.flatten(), yy.flatten()\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\ngrid = np.hstack((r1,r2))\ny_pred_grid=nb.predict(grid)\nzz = y_pred_grid.reshape(xx.shape)\nzz[zz=='setosa']=1\nzz[zz=='versicolor']=2\nzz[zz=='virginica']=3\nxx=np.array(xx,dtype=float)\nyy=np.array(yy,dtype=float)\nzz=np.array(zz,dtype=float)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[0]],x_train['petal width (cm)'][y_pred==iris.target_names[0]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[1]],x_train['petal width (cm)'][y_pred==iris.target_names[1]],zorder=2)\nplt.scatter(x_train['petal length (cm)'][y_pred==iris.target_names[2]],x_train['petal width (cm)'][y_pred==iris.target_names[2]],zorder=2)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\nplt.legend(iris.target_names)\nplt.contourf(xx, yy, zz,cmap='RdBu_r')\nplt.subplot(122)\nConfusionMatrixDisplay.from_predictions(y_train,y_pred,ax=axs[1],colorbar=False)\nplt.tight_layout()  \nplt.show()\n\n\n\n\n\nFigure 4: Random Forest Classifier classification results (with decision boundaries) and confusion matrix"
  },
  {
    "objectID": "posts/Clustering/index.html#affinity-propagation",
    "href": "posts/Clustering/index.html#affinity-propagation",
    "title": "Clustering",
    "section": "Affinity Propagation",
    "text": "Affinity Propagation\nUnlike the two algorithms that will be investigated below, Affinity Propagation clustering does not require the number of clusters as an argument prior to running the algorithm. It will iterate through the data and find exemplar points that are representative of each cluster. Two arguments were used for this algorithm, a damping factor value of 0.9 and a preference value of -200. The damping value is the extent that the current value is maintained relative to incoming values to avoid oscillations, and the preference value allows for points preference in the availability matrix to be set and allows the model to more accurately predict the number of clusters. [2]\n\naffinity_propagation = cluster.AffinityPropagation(damping=.9,preference=-200)\naffinity_propagation.fit(x)\ny_pred_af=affinity_propagation.labels_"
  },
  {
    "objectID": "posts/Clustering/index.html#birch",
    "href": "posts/Clustering/index.html#birch",
    "title": "Clustering",
    "section": "BIRCH",
    "text": "BIRCH\n\nbirch = cluster.Birch(n_clusters=5)\nbirch.fit(x)\ny_pred_birch=birch.labels_"
  },
  {
    "objectID": "posts/Clustering/index.html#spectral-clustering",
    "href": "posts/Clustering/index.html#spectral-clustering",
    "title": "Clustering",
    "section": "Spectral Clustering",
    "text": "Spectral Clustering\nThe Spectral Clustering algorithm works by taking the eigenvalues of the similarity matrix, reducing the dimensions, and then performing clustering but now with fewer dimensions. This method is useful for data that is non-convex or irregularly shaped, as well as high dimensional datasets. [4]\nAs mentioned above, the Spectral Clustering algorithm can take the number of clusters as an input, and since this number is known to be 5, n_clusters = 5 will be used as the only argument.\n\nspectral = cluster.SpectralClustering(n_clusters=5)\nspectral.fit(x)   \ny_pred_spectral=spectral.labels_"
  },
  {
    "objectID": "posts/Clustering/index.html#balanced-iterative-reducing-and-clustering-using-hierarchies-birch",
    "href": "posts/Clustering/index.html#balanced-iterative-reducing-and-clustering-using-hierarchies-birch",
    "title": "Clustering",
    "section": "Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)",
    "text": "Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH)\nThe BIRCH clustering algorithm is often used as an alternative to the K means algorithm, as it is more memory efficient and works exceptionally well with larger datasets. It works by constructing a clustering feature tree with a set of nodes and subclusters for each node. The algorithm will merge together subclusters with the smallest radius, and then see if each subcluster has any child nodes. If so, it will continue the same process until it reaches a leaf of the original tree. [3]\nAs mentioned above, the BIRCH algorithm can take the number of clusters as an input, and since this number is known to be 5, n_clusters = 5 will be used as the only argument.\n\nbirch = cluster.Birch(n_clusters=5)\nbirch.fit(x)\ny_pred_birch=birch.labels_"
  }
]