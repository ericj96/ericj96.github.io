[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Anomaly Detection on Spacecraft Telemetry\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nTopic 2\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\n  \n\n\n\n\nTopic 3\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nEric Jackson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Anomaly Detection on Spacecraft Telemetry",
    "section": "",
    "text": "Background\nSpacecraft will generate a massive amount of data the longer they are on orbit, from telemetry data containing voltages, temperatures, etc to raw data from the various types of payloads on orbit\n\nSpacecraft have onboard anomaly responses for most known failure cases to safe the vehicle\nNormally low/high, red/yellow limits set for certain monitors with corresponding response (either automatic or visual alarm)\nSome anomalies can be hard to predict, multiple components can react slightly out of family to create larger issue\nBenefits of utilizing machine learning for spacecraft:\nPrevents loss of mission over potentially high priority targets\nAutomatic response would limit both downtime and human interaction\nHigher award/incentive fees for lower mission outage percentage\nLimits time spent by operators and factory investigating and implementing a fix\nDepending on program and customer, recovery can take anywhere from a few hours to multiple days\nPredict future anomalous conditions and potentially react before an issue were to occur\nSome programs have multiple vehicles on orbit meaning there is a plethora of historical training data available\nGoal: Utilize ARIMA & OCSVM to create a hybrid anomaly detection method and compare results with other common algorithms/methods\n\n\n\n\nExample of anomaly in telemetry\n\n\n\n\nData Setup & Preprocessing\nUnfold below code to see setup. Basics are generating 28 features\nsetting up training data set and validation data set\n\n\nCode\nimport os\nimport datetime\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.svm as svm\nimport matplotlib.pyplot as plt\nimport math\nimport warnings\nwarnings.filterwarnings('ignore', '[\\s\\w\\W]*non-unique[\\s\\w\\W]*', DeprecationWarning)\n\ndf=pd.read_csv('./WheelTemperature.csv')\ndf_battemp=pd.read_csv('./BatteryTemperature.csv')\ndf_buscurrent=pd.read_csv('./TotalBusCurrent.csv')\ndf_busvolt=pd.read_csv('./BusVoltage.csv')\n\ndf_battemp.Date = pd.to_datetime(df_battemp.Date, format=\"%m/%d/%Y %H:%M\")\ndf_buscurrent.Date = pd.to_datetime(df_buscurrent.Date, format=\"%m/%d/%Y\")\ndf_busvolt.Date=pd.to_datetime(df_busvolt.Date, format=\"%m/%d/%Y %H:%M\")\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\n\ndf_battemp=df_battemp.resample('1D',on='Date').mean()\ndf_buscurrent=df_buscurrent.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.resample('1D',on='Date').mean()\ndf_busvolt=df_busvolt.loc['2004-02-13':]\ndf=df.resample('1D',on='Date').mean()\n\ndf=pd.concat([df,df_battemp,df_buscurrent,df_busvolt],axis=1)\ndf['Date']=df.index\n\nlag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\", \"NumTrades\"]\nlag_features=[\"High\",\"Temp\",\"Current\",\"Voltage\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\ndf_mean_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_mean_30d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_3d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_7d.set_index(\"Date\", drop=True, inplace=True)\ndf_std_30d.set_index(\"Date\", drop=True, inplace=True)\n\nfor feature in lag_features:\n    \n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.Date = pd.to_datetime(df.Date, format=\"%m/%d/%Y %H:%M\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.isocalendar().week.astype(np.int64)\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\ndf.set_index(\"Date\", drop=True, inplace=True)\ndf.fillna(df.mean(), inplace=True)\n\ndata=df\ndata.index = pd.to_datetime(data.index)\ndata=data.resample('1D').mean()\ndf_train=data.iloc[0:math.floor(len(data)*.75),:]\ndf_valid=data.iloc[math.floor(len(data)*.75):,:]\n\nexogenous_features=['High_mean_lag3', 'High_mean_lag7',\n       'High_mean_lag30', 'High_std_lag3', 'High_std_lag7', 'High_std_lag30',\n       'Temp_mean_lag3', 'Temp_mean_lag7', 'Temp_mean_lag30', 'Temp_std_lag3',\n       'Temp_std_lag7', 'Temp_std_lag30', 'Current_mean_lag3',\n       'Current_mean_lag7', 'Current_mean_lag30', 'Current_std_lag3',\n       'Current_std_lag7', 'Current_std_lag30', 'Voltage_mean_lag3',\n       'Voltage_mean_lag7', 'Voltage_mean_lag30', 'Voltage_std_lag3',\n       'Voltage_std_lag7', 'Voltage_std_lag30', 'month', 'week', 'day',\n       'day_of_week']\n       \n       \nexogenous_features=['High_mean_lag3','week']\n\n\n\n\nARIMA Model\n\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = auto_arima(\n    df_train[\"High\"],\n    exogenous=df_train[exogenous_features],\n    trace=True,\n    error_action=\"ignore\",\n    suppress_warnings=True,\n    seasonal=True,\n    m=1)\nmodel.fit(df_train.High, exogenous=df_train[exogenous_features])\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid.insert(len(df_valid.columns),\"Forecast_ARIMAX\",forecast,True)\n\nprint(\"\\nRMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.High, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.High, df_valid.Forecast_ARIMAX))\n\nPerforming stepwise search to minimize aic\n ARIMA(2,0,2)(0,0,0)[0] intercept   : AIC=3072.751, Time=3.91 sec\n ARIMA(0,0,0)(0,0,0)[0] intercept   : AIC=6637.987, Time=1.16 sec\n ARIMA(1,0,0)(0,0,0)[0] intercept   : AIC=3389.503, Time=3.05 sec\n ARIMA(0,0,1)(0,0,0)[0] intercept   : AIC=4259.244, Time=2.65 sec\n ARIMA(0,0,0)(0,0,0)[0]             : AIC=6688.243, Time=0.64 sec\n ARIMA(1,0,2)(0,0,0)[0] intercept   : AIC=3130.618, Time=3.62 sec\n ARIMA(2,0,1)(0,0,0)[0] intercept   : AIC=3181.655, Time=3.07 sec\n ARIMA(3,0,2)(0,0,0)[0] intercept   : AIC=3097.031, Time=4.54 sec\n ARIMA(2,0,3)(0,0,0)[0] intercept   : AIC=3133.775, Time=4.16 sec\n ARIMA(1,0,1)(0,0,0)[0] intercept   : AIC=3271.556, Time=3.45 sec\n ARIMA(1,0,3)(0,0,0)[0] intercept   : AIC=3127.658, Time=4.59 sec\n ARIMA(3,0,1)(0,0,0)[0] intercept   : AIC=3166.529, Time=4.07 sec\n ARIMA(3,0,3)(0,0,0)[0] intercept   : AIC=3062.532, Time=5.80 sec\n ARIMA(4,0,3)(0,0,0)[0] intercept   : AIC=3077.630, Time=5.97 sec\n ARIMA(3,0,4)(0,0,0)[0] intercept   : AIC=3066.100, Time=6.51 sec\n ARIMA(2,0,4)(0,0,0)[0] intercept   : AIC=3112.572, Time=5.89 sec\n ARIMA(4,0,2)(0,0,0)[0] intercept   : AIC=3172.202, Time=5.10 sec\n ARIMA(4,0,4)(0,0,0)[0] intercept   : AIC=3061.916, Time=6.94 sec\n ARIMA(5,0,4)(0,0,0)[0] intercept   : AIC=3049.563, Time=7.19 sec\n ARIMA(5,0,3)(0,0,0)[0] intercept   : AIC=3074.573, Time=6.54 sec\n ARIMA(5,0,5)(0,0,0)[0] intercept   : AIC=3043.957, Time=8.19 sec\n ARIMA(4,0,5)(0,0,0)[0] intercept   : AIC=3054.015, Time=7.64 sec\n ARIMA(5,0,5)(0,0,0)[0]             : AIC=3096.432, Time=7.64 sec\n\nBest model:  ARIMA(5,0,5)(0,0,0)[0] intercept\nTotal fit time: 112.342 seconds\n\nRMSE of Auto ARIMAX: 0.7153457717072086\n\nMAE of Auto ARIMAX: 0.32255422754379454\n\n\nFigure 1\n\n\n\n\n\nFigure 1: Initial ARIMA forecast on reaction wheel temperature data\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(10,6))\nplt.plot(df_train[\"High\"])\n#plt.plot(df_valid[\"High\"])\nplt.plot(df_valid[\"Forecast_ARIMAX\"])\nplt.legend(['Training Data','Test Data'],loc='best')\nplt.ylabel('Temperature (C)')\n#plt.savefig('model.png',dpi=1200)\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(10,6))\nplt.plot(df_valid[\"High\"])\n#plt.plot(df_valid[\"High\"])\nplt.plot(df_valid[\"Forecast_ARIMAX\"])\nplt.legend(['Truth','ARIMA Model'],loc='lower left')\nplt.ylabel('Temperature (C)')\n#plt.savefig('model2.png',dpi=1200)\nplt.show()\n\n\n\n\n\n\n\n\n############ Truth ################\n\ndf_truth=pd.read_csv('./truth.csv')\ndf_truth.Date = pd.to_datetime(df_truth.Date, format=\"%m/%d/%Y\")\ndf_truth.set_index(\"Date\", drop=True, inplace=True)\nanom=df_truth['Anom']\nanom=anom.map(lambda val:1 if val==-1 else 0)\na=df_truth.loc[df_truth['Anom']==1,['High']]\nfig, ax = plt.subplots(figsize=(10,6))\nax.plot(df_truth.index,df_truth['High'], color='black', label = 'ARIMA')\nax.scatter(a.index,a.values, color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Truth Anomalies')\nplt.show()\n\n\n\n\n\n\nOCSVM\nOne class support vector machine algorithm\n\n\n\nOCSVM\n\n\n\n############# OCSVM ##################\n\nfig, ax = plt.subplots(figsize=(10,6))\ndata2=df_valid[\"Forecast_ARIMAX\"]\nmodel =svm.OneClassSVM(nu=0.05,kernel='poly')\nmodel.fit(data2.values.reshape(-1,1))\nanom=(pd.Series(model.predict(data2.values.reshape(-1,1))))\ndf2=pd.DataFrame()\ndf2['Time']=data2.index\ndf2['data']=data2.values\ndf2['anom']=anom\na=df2.loc[df2['anom']==-1,['Time','data']]\ndf2.set_index(\"Time\", drop=True, inplace=True)\nax.plot(df2.index, df2['data'], color='black', label = 'ARIMA')\nax.scatter(a['Time'].values,a['data'], color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Anomalies detected with OCSVM')\nplt.show()\n\n\n\n\n\n\nIsolation Forest\n\n######### Isolation Forest ################\nimport sklearn\nfrom sklearn.ensemble import IsolationForest\ncatfish_sales=df_valid\noutliers_fraction = float(.01)\nscaler = sklearn.preprocessing.StandardScaler()\nnp_scaled = scaler.fit_transform(catfish_sales['Forecast_ARIMAX'].values.reshape(-1, 1))\ndata = pd.DataFrame(np_scaled)\n# train isolation forest\nmodel =  IsolationForest(contamination=outliers_fraction)\nmodel.fit(data)\n\n\n\ncatfish_sales['anomaly'] = model.predict(data)\n# visualization\nfig, ax = plt.subplots(figsize=(10,6))\na = catfish_sales.loc[catfish_sales['anomaly'] == -1, ['Forecast_ARIMAX']] #anomaly\nax.plot(catfish_sales.index, catfish_sales['Forecast_ARIMAX'], color='black', label = 'Normal')\nax.scatter(a.index,a['Forecast_ARIMAX'], color='red', label = 'Anomaly')\nplt.legend(['Wheel Temperature','Anomaly'])\nplt.ylabel('Temperature (C)')\nplt.title('Anomalies detected with Isolation Forest')\nplt.show()\n\n\n\n\n\n\nFinal Results\n\n######################### calculate statistics ###########################\nfrom sklearn.metrics import f1_score,recall_score,precision_score\nfrom sklearn.metrics import mean_squared_error\nfrom tabulate import tabulate\nfrom collections import OrderedDict\n\n\n\ndef perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)\n\n\n\n\ndf_truth=pd.read_csv('./truth.csv')\nanom=anom.map(lambda val:1 if val==-1 else 0)\n#calculate F1 score\nf1=f1_score(df_truth['Anom'].values, anom.values)\nrec=recall_score(df_truth['Anom'].values, anom.values)\nprec=precision_score(df_truth['Anom'].values, anom.values)\nTP, FP, TN, FN=perf_measure(df_truth['Anom'].values, anom.values)\nfpr=FP/(TN+FP)\nfinal=OrderedDict()\nfinal['OCSVM']=[f1,rec,prec,fpr]\n\n\na2=catfish_sales['anomaly']\na2=a2.map(lambda val:1 if val==-1 else 0)\nf1=f1_score(df_truth['Anom'].values, a2.values)\nrec=recall_score(df_truth['Anom'].values, a2.values)\nprec=precision_score(df_truth['Anom'].values, a2.values)\nTP, FP, TN, FN=perf_measure(df_truth['Anom'].values, a2.values)\nfpr=FP/(TN+FP)\nfinal['IsoFor']=[f1,rec,prec,fpr]\ndf=pd.DataFrame(final)\ndf.index=['F1','Recall','Precision','FPR']\nprint(tabulate(df, headers='keys', tablefmt='psql'))\n\n+-----------+------------+-------------+\n|           |      OCSVM |      IsoFor |\n|-----------+------------+-------------|\n| F1        | 0.714286   | 0.20155     |\n| Recall    | 0.565217   | 0.113043    |\n| Precision | 0.970149   | 0.928571    |\n| FPR       | 0.00163666 | 0.000818331 |\n+-----------+------------+-------------+"
  },
  {
    "objectID": "posts/Topic_2/index.html",
    "href": "posts/Topic_2/index.html",
    "title": "Topic 2",
    "section": "",
    "text": "Background"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Topic_3/index.html",
    "href": "posts/Topic_3/index.html",
    "title": "Topic 3",
    "section": "",
    "text": "Background"
  }
]